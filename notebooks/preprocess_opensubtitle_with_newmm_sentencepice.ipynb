{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.7\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import copy \n",
    "import re\n",
    "import html\n",
    "\n",
    "from time import time \n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import pythainlp\n",
    "from pythainlp.util import *\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.ulmfit import *\n",
    "\n",
    "# subword-nmt\n",
    "from subword_nmt import learn_bpe as learner\n",
    "from subword_nmt import apply_bpe as subword_tokenizer\n",
    "\n",
    "import fairseq \n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pythainlp.tokenize import DEFAULT_DICT_TRIE\n",
    "\n",
    "from pythainlp.corpus import thai_words\n",
    "\n",
    "print(pythainlp.__version__)\n",
    "# assert pythainlp.__version__ == '2.1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install BPEmb (BPE embeddings)\n",
    "\n",
    "!pip install --q bpemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USE\n",
    "\n",
    "# g = tf.Graph()\n",
    "# with g.as_default():\n",
    "#     text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "#     embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/1\")\n",
    "#     embedded_text = embed(text_input)\n",
    "#     init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "# g.finalize()\n",
    "\n",
    "# # def compute_similarity(src, tgt):\n",
    "# #     \"\"\"\n",
    "# #         Calculate sentence similarity based on Google Universal Sentence Encoder (Multilingual Large)\n",
    "# #     \"\"\"\n",
    "\n",
    "# # # Initialize session.\n",
    "# # session = tf.Session(graph=g)\n",
    "# # session.run(init_op)\n",
    "\n",
    "# # # Compute embeddings.\n",
    "# # en_result = session.run(embedded_text, feed_dict={text_input: english_sentences})\n",
    "# # it_result = session.run(embedded_text, feed_dict={text_input: italian_sentences})\n",
    "# # ja_result = session.run(embedded_text, feed_dict={text_input: japanese_sentences})\n",
    "\n",
    "# # # Compute similarity matrix. Higher score indicates greater similarity.\n",
    "# # similarity_matrix_it = np.inner(en_result, it_result)\n",
    "# # similarity_matrix_ja = np.inner(en_result, ja_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "\n",
    "bpemb_pretrained ={\n",
    "    'th': {\n",
    "        '25000': BPEmb(lang=\"th\", vs=25000)\n",
    "    },\n",
    "    'en': {\n",
    "        '25000': BPEmb(lang=\"en\", vs=25000)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3281534,\n",
       " ['Slave in the Magic Mirror, come from the farthest space.',\n",
       "  'Through wind and darkness, I summon thee.',\n",
       "  'Speak!'])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/opensubtitle_v2018/OpenSubtitles.en-th.en','r', encoding='utf-8') as f:\n",
    "    en = f.read().split('\\n')\n",
    "len(en),en[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3281534,\n",
       " ['ทาสในกระจกวิเศษ, มาจากพื้นที่ที่ไกลที่สุด',\n",
       "  'ผ่านลมและความมืดฉันเรียกเจ้า',\n",
       "  'พูด!'])"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/opensubtitle_v2018/OpenSubtitles.en-th.th','r', encoding='utf-8') as f:\n",
    "    th = f.read().split('\\n')\n",
    "    \n",
    "len(th),th[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRule:    \n",
    "    def test(self, sentence, lang):\n",
    "        pass\n",
    "\n",
    "class ReplaceRule(BaseRule):   \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def replace(self, sentence, lang):\n",
    "        pass\n",
    "\n",
    "class UnescapeString(ReplaceRule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        return True\n",
    "\n",
    "    def replace(self, sentence, lang):\n",
    "        sentence = sentence.replace('\\\\\"', '\"')\n",
    "        return unescape_string(sentence)\n",
    "\n",
    "class ReplaceSymbolOccurenceRule(ReplaceRule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_list = ['', '', '​', '', '', '', '",
    "', '', 'โ', '​',\n",
    "                           '♪', '{\\ cHFFFFFF }', '\\cHFFFFFF', '§', 'font color = \"# 808080 \"',\n",
    "                          ' ##', '{\\\\cHFFFFFF}', '## ', ' ## ', '-# ', '# ', ' #', '\\ N', '\\ NI', ',8203;', '8203;', '#8203; ',\n",
    "                          '#8203;', '\\\\i1}', \"\\\\\\\\\\\\\\\\\", \"\\\\\\\\\\\\\", \"\\\\\\\\\", '\\\\ ', '\\\\', '{}']\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        for token in self.token_list:\n",
    "            if token in sentence:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def replace(self, sentence, lang):\n",
    "        for token in self.token_list:\n",
    "            sentence = sentence.replace(token, '')\n",
    "        return sentence\n",
    "\n",
    "class ReplaceHashtagInSentenceRule(ReplaceRule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if re.match(r\"[[^#]|[^##]]\", sentence):\n",
    "            return True\n",
    "        if re.search(r\"#$\", sentence):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def replace(self, sentence, lang):\n",
    "        sentence = re.sub(r\"^#+\", '', sentence).lstrip()\n",
    "        sentence = re.sub(r\"#$\", '', sentence).rstrip()\n",
    "\n",
    "        return sentence\n",
    "    \n",
    "\n",
    "class NormalizeThaiVowel(ReplaceRule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if 'เเ' in sentence and lang == 'th':\n",
    "            return True\n",
    "        if 'ใใ' in sentence and lang == 'th':\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def replace(self, sentence, lang):\n",
    "        sentence = re.sub(r\"เเ\", 'แ', sentence)\n",
    "        sentence = re.sub(r\"ใใ\", 'ใ', sentence)\n",
    "\n",
    "        return sentence\n",
    "    \n",
    "class ReplaceFullStopInThaiSentenceRule(ReplaceRule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if lang == 'th':\n",
    "            if re.search(r'\\.$', sentence):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def replace(self, sentence, lang):\n",
    "        sentence = re.sub(r\"\\.$\", '', sentence)\n",
    "        return sentence\n",
    "    \n",
    "    \n",
    "    \n",
    "class ReplaceDashInSentenceRule(ReplaceRule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if ' - ' in sentence:\n",
    "            return True\n",
    "        if '- ' in sentence:\n",
    "            return True\n",
    "        if ' -' in sentence:\n",
    "            return True\n",
    "        if re.search(r\"^[-]+\", sentence):\n",
    "            return True\n",
    "        if re.search(r\"[-]+$\", sentence):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def replace(self, sentence, lang):\n",
    "        sentence = re.sub(r\" - \", '', sentence)\n",
    "        sentence = re.sub(r\"- \", '', sentence)\n",
    "        sentence = re.sub(r\" -\", '', sentence)\n",
    "        sentence = re.sub(r\"^[-]+\", '', sentence) # start with space + \"-\" \n",
    "        sentence = re.sub(r\"[-]+$\", '', sentence) # end with space + \"-\" \n",
    "\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "กHello\n",
      "Hello\n",
      "Hello\n",
      "a-sad Hello\n",
      "กับใคร\n"
     ]
    }
   ],
   "source": [
    "def testReplaceDashInSentenceRule():\n",
    "    rule = ReplaceDashInSentenceRule()\n",
    "    assert rule.test('- ', lang=\"th\") == True\n",
    "    assert rule.test(' -', lang=\"th\") == True\n",
    "    assert rule.test(' - ', lang=\"th\") == True\n",
    "    assert rule.test('-กับใคร', lang=\"th\") == True\n",
    "\n",
    "    print(rule.replace('- Hello', lang=\"th\"))\n",
    "    print(rule.replace(' -กHello', lang=\"th\"))\n",
    "    print(rule.replace('Hello-', lang=\"th\"))\n",
    "    print(rule.replace('---- Hello- ', lang=\"th\"))\n",
    "    print(rule.replace('a-sad Hello--- ', lang=\"th\"))\n",
    "    print(rule.replace('-กับใคร', lang=\"th\"))\n",
    "\n",
    "testReplaceDashInSentenceRule()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "def testReplaceHashtagInSentenceRule():\n",
    "    rule = ReplaceHashtagInSentenceRule()\n",
    "    assert rule.test('#..', lang=\"th\") == True\n",
    "    assert rule.test('##..', lang=\"th\") == True\n",
    "    assert rule.test('.#', lang=\"th\") == True\n",
    "\n",
    "    print(rule.replace('#..', lang=\"th\"))\n",
    "    print(rule.replace('##..', lang=\"th\"))\n",
    "    print(rule.replace('#### ..', lang=\"th\"))\n",
    "    print(rule.replace('..', lang=\"th\"))\n",
    "    print(rule.replace('.#', lang=\"th\"))\n",
    "\n",
    "testReplaceHashtagInSentenceRule()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ฉัน\n"
     ]
    }
   ],
   "source": [
    "def testReplaceFullStopInThaiSentenceRule():\n",
    "    rule = ReplaceFullStopInThaiSentenceRule()\n",
    "    assert rule.test('ฉัน.', lang=\"th\") == True\n",
    "    print(rule.replace('ฉัน.', lang=\"th\"))\n",
    "\n",
    "        \n",
    "testReplaceFullStopInThaiSentenceRule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "แรกเกิด\n",
      "ให้\n"
     ]
    }
   ],
   "source": [
    "def testNormalizeThaiVowel():\n",
    "    rule = NormalizeThaiVowel()\n",
    "    assert rule.test('เเ', lang='th') == True\n",
    "    assert rule.test('ใใ', lang='th') == True\n",
    "\n",
    "    print(rule.replace('เเรกเกิด', lang='th'))\n",
    "    print(rule.replace('ใให้', lang='th'))\n",
    "\n",
    "testNormalizeThaiVowel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ContainsAdSymbol(BaseRule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if '@' in sentence:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "class NoThaiWordInThaiSentence(BaseRule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if lang == \"th\":\n",
    "            if countthai(sentence, ignore_chars='') == 0.0:\n",
    "                return True\n",
    "            if re.search(r'^โช [A-z]', sentence):\n",
    "                return True\n",
    "            if re.search(r'^โช\\s\\.', sentence):\n",
    "                return True\n",
    "            if re.search(r'^โช\\sโช', sentence):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "class ContainsTokensInThaiSentence(BaseRule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.list_of_tokens = ['โ#65533;', 'N#233;', '#/N#', 'ใใใ', '#']\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if lang == \"th\":\n",
    "            if re.search(r'\\d\\d; \\d\\d', sentence):\n",
    "                return True\n",
    "            for symbol in self.list_of_tokens:\n",
    "                if symbol in sentence:\n",
    "                    return True\n",
    "           \n",
    "        return False\n",
    "    \n",
    "class ContainsUnknownSymbols(BaseRule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.list_unknown_symbols = [ b'\\x98\\xc2', b'\\xae\\xc2', b'\\x99\\xc2',\n",
    "                                      b'\\xb1\\xc2', b'\\xc2\\xb7', b'\\xc2\\x8b',\n",
    "                                      b'\\xc3\\x83']\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        for symbol in self.list_unknown_symbols:\n",
    "            if symbol in sentence.encode('utf-8'):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "class SentenceLengthLessThanOne(BaseRule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if len(sentence) <= 1:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def testContainsAdSymbol():\n",
    "    rule = ContainsAdSymbol()\n",
    "    print(rule.test('@gmail', lang=\"th\"))\n",
    "    print(rule.test('gmasd', lang=\"th\"))\n",
    "testContainsAdSymbol()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc2\\x8b'\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def testContainsUnknownSymbols():\n",
    "    rule = ContainsUnknownSymbols()\n",
    "    print(''.encode('utf-8'))\n",
    "    print(rule.test('', lang=\"th\"))\n",
    "testContainsUnknownSymbols()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def testNoThaiWordInThaiSentence():\n",
    "    rule = NoThaiWordInThaiSentence()\n",
    "    print(rule.test('', lang=\"th\"))\n",
    "    print(rule.test('en', lang=\"th\"))\n",
    "    print(rule.test('โช โช', lang=\"th\"))\n",
    "\n",
    "testNoThaiWordInThaiSentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_REPLACE_RULES = [ReplaceSymbolOccurenceRule,\n",
    "                         ReplaceDashInSentenceRule,\n",
    "                         ReplaceHashtagInSentenceRule,\n",
    "                         UnescapeString,\n",
    "                         NormalizeThaiVowel]\n",
    "\n",
    "DEFAULT_FILTER_OUT_RULES = [ContainsUnknownSymbols,\n",
    "                            ContainsTokensInThaiSentence,\n",
    "                            ContainsAdSymbol,\n",
    "                            NoThaiWordInThaiSentence,\n",
    "                            SentenceLengthLessThanOne]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text\n",
    "\n",
    "# LIST_OF_TOKENS_TO_REPLACE = ['', '', '​', '', '', '', '",
    "', '', 'โ', '​',\n",
    "#                    '♪', '{\\ cHFFFFFF }', '§', 'font color = \"# 808080 \"']\n",
    "def unescape_string(text):\n",
    "    return html.unescape(text)\n",
    "\n",
    "def sentences_filter(sentences, lang, rules=DEFAULT_FILTER_OUT_RULES):\n",
    "    indices = []\n",
    "    for index, sentence in tqdm_notebook(enumerate(sentences), total=len(sentences)):\n",
    "        \n",
    "        for rule in rules:\n",
    "            rule_obj = rule()\n",
    "            if rule_obj.test(sentence, lang=lang):\n",
    "                indices.append(index)\n",
    "    return indices\n",
    "\n",
    "def clean_sentence(sentence, lang, rules=DEFAULT_REPLACE_RULES):\n",
    "    for rule in rules:\n",
    "        \n",
    "        rule_obj = rule()\n",
    "        if rule_obj.test(sentence, lang=lang):\n",
    "            sentence = rule_obj.replace(sentence, lang=lang)\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_worker(sentence, lang, trie):\n",
    "    \n",
    "    _tokenizer_newmm = partial(pythainlp.tokenize.word_tokenize, engine='newmm',\n",
    "                               keep_whitespace=False,\n",
    "                              custom_dict=(trie if trie != None else DEFAULT_DICT_TRIE))\n",
    "    return ' '.join(_tokenizer_newmm(sentence))\n",
    "  \n",
    "def tokenize_handler(sentences, lang, trie=None):\n",
    "    toks = []\n",
    "    p = Pool(12)\n",
    "    t = time()\n",
    "    _tokenize_worker = partial(tokenize_worker, lang=lang, trie=trie)\n",
    "    toks = p.map(_tokenize_worker, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    print('{} s'.format(time() -t))\n",
    "\n",
    "    return toks\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_spaced_tokens_to_file(data, folder_name, filename):\n",
    "    with open('/root/mt-opus/data/{}/{}'.format(folder_name, filename),'w') as f:\n",
    "        for item in data:\n",
    "            f.write(item + '\\n')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence filtering (th)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3310d02bb64fb38839d58936192c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3281534), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102619\n",
      "sentence filtering (en)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0565bb39924b30bf14aa876fba6a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3281534), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4560\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "print('sentence filtering (th)')\n",
    "indices_to_filter_out_th = sentences_filter(th, lang='th')\n",
    "print(len(indices_to_filter_out_th))\n",
    "\n",
    "print('sentence filtering (en)')\n",
    "indices_to_filter_out_en = sentences_filter(en, lang='en')\n",
    "\n",
    "print(len(indices_to_filter_out_en))\n",
    "\n",
    "indices_to_filter_out = indices_to_filter_out_th + indices_to_filter_out_en\n",
    "indices_to_filter_out = set(indices_to_filter_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93708\n"
     ]
    }
   ],
   "source": [
    "print(len(indices_to_filter_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean sentence (th)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdc3e5d7dbc4cc0b9b8ce7f1470b9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3281534), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean sentence (en)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79de7c270e44b8fb90f85baa346c935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3281534), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274.62059903144836 seconds\n"
     ]
    }
   ],
   "source": [
    "# Clean Sentence\n",
    "\n",
    "print('clean sentence (th)')\n",
    "filtered_th = [clean_sentence(x, lang='th') for i, x in tqdm_notebook(enumerate(th), total=len(th)) if i not in indices_to_filter_out]\n",
    "print('clean sentence (en)')\n",
    "filtered_en = [clean_sentence(x, lang='en') for i, x in tqdm_notebook(enumerate(en), total=len(en)) if i not in indices_to_filter_out]\n",
    "\n",
    "print('{} seconds'.format(time() -t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3281534\n",
      "3187826\n"
     ]
    }
   ],
   "source": [
    "print(len(th))\n",
    "print(len(filtered_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = Counter()\n",
    "\n",
    "# for idx, sent in enumerate(filtered_th):\n",
    "    \n",
    "#     if re.search('โ#',sent):\n",
    "#         counter['th found - at the start of sentence'] += 1\n",
    "#         if counter['th found - at the start of sentence'] < 10:\n",
    "#             print(filtered_en[idx])\n",
    "#             print(sent)\n",
    "   \n",
    "# counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I got peace like a river I got peace like a river\n",
      "โช I got peace like a river I got peace like a river\n",
      "\n",
      " I got peace like a river in my soul\n",
      "โช I got peace like a river in my soul\n",
      "\n",
      " I got love like an ocean\n",
      "โช I got love like an ocean\n",
      "\n",
      " I got love like an ocean\n",
      "โช I got love like an ocean\n",
      "\n",
      " I got love like an ocean in my soul\n",
      "โช I got love like an ocean in my soul\n",
      "\n",
      " I got joy like a fountain I got joy like a fountain\n",
      "โช I got joy like a fountain I got joy like a fountain\n",
      "\n",
      " I got joy like a fountain in my soul... \n",
      "โช I got joy like a fountain in my soul... โช\n",
      "\n",
      " I'm broken\n",
      "โช I'm broken\n",
      "\n",
      " Don't break me\n",
      "โช Don't break me\n",
      "\n",
      " When I hit the ground\n",
      "โช When I hit the ground\n",
      "\n",
      " Some devil\n",
      "โช Some devil\n",
      "\n",
      " Some angel\n",
      "โช Some angel\n",
      "\n",
      " Has got me to the bones\n",
      "โช Has got me to the bones\n",
      "\n",
      " You said always and for ever\n",
      "โช You said always and for ever\n",
      "\n",
      " Now I believe you, baby\n",
      "โช Now I believe you, baby\n",
      "\n",
      " You said always and for ever\n",
      "โช You said always and for ever\n",
      "\n",
      " Is such a long, lonely time\n",
      "โช Is such a long, lonely time\n",
      "\n",
      " Too drunk and\n",
      "โช Too drunk and\n",
      "\n",
      " Still drinking\n",
      "โช Still drinking\n",
      "\n",
      " It's just the way I feel\n",
      "โช It's just the way I feel\n",
      "\n",
      " It's all right\n",
      "โช It's all right\n",
      "\n",
      " That's what you told me\n",
      "โช That's what you told me\n",
      "\n",
      " I feel heavy\n",
      "โช I feel heavy\n",
      "\n",
      " Like floating\n",
      "โช Like floating\n",
      "\n",
      " At the bottom of the sea\n",
      "โช At the bottom of the sea\n",
      "\n",
      " You said always and for ever\n",
      "โช You said always and for ever\n",
      "\n",
      " Now I believe you, baby\n",
      "โช Now I believe you, baby\n",
      "\n",
      " You said always and for ever\n",
      "โช You said always and for ever\n",
      "\n",
      " Is such a long, lonely time\n",
      "โช Is such a long, lonely time\n",
      "\n",
      " Some devil is\n",
      "โช Some devil is\n",
      "\n",
      " Stuck inside of me\n",
      "โช Stuck inside of me\n",
      "\n",
      " I cannot set it free\n",
      "โช I cannot set it free\n",
      "\n",
      " I wish, I wish I was dead and you were breathing\n",
      "โช I wish, I wish I was dead and you were breathing\n",
      "\n",
      " Just so that you could know\n",
      "โช Just so that you could know\n",
      "\n",
      " Some angel is\n",
      "โช Some angel is\n",
      "\n",
      " Stuck inside of me\n",
      "โช Stuck inside of me\n",
      "\n",
      " I cannot set you free\n",
      "โช I cannot set you free\n",
      "\n",
      " You said always and for ever\n",
      "โช You said always and for ever\n",
      "\n",
      " Now I believe you, baby\n",
      "โช Now I believe you, baby\n",
      "\n",
      " You said always and for ever\n",
      "โช You said always and for ever\n",
      "\n",
      " Such a long, lonely time\n",
      "โช Such a long, lonely time\n",
      "\n",
      " Stuck inside of me \n",
      "โช Stuck inside of me โช\n",
      "\n",
      "Cho\n",
      "โช ...\n",
      "\n",
      " Just shades of gray \n",
      "โช Just shades of gray โช\n",
      "\n",
      " \n",
      "โช โช\n",
      "\n",
      " \n",
      "โช โช\n",
      "\n",
      " \n",
      "โช โช\n",
      "\n",
      " \n",
      "โช โช\n",
      "\n",
      " I thought this could be smooth sailing \n",
      "โช I thought this could be smooth sailing โช\n",
      "\n",
      " I don't wanna cry every time we try... \n",
      "โช I don't wanna cry every time we try... โช\n",
      "\n",
      " Change the direction \n",
      "โช change the direction โช\n",
      "\n",
      " And everything's coming okay \n",
      "โช and everything's coming okay โช\n",
      "\n",
      " let you go \n",
      "โช let you go โช\n",
      "\n",
      " oh, though we're caught in the undertow \n",
      "โช oh, though we're caught in the undertow โช\n",
      "\n",
      " don't trip \n",
      "โช don't trip โช\n",
      "\n",
      " matter of fact, I got something you can trip on \n",
      "โช matter of fact, I got something you can trip on โช\n",
      "\n",
      " I might be with my girl, girl \n",
      "โช I might be with my girl, girl โช\n",
      "\n",
      " so you can bring your girls, too \n",
      "โช so you can bring your girls, too โช โช I can teach you to fly โช\n",
      "\n",
      " I can teach you to fly \n",
      "โช I can teach you to fly โช\n",
      "\n",
      " I can teach you to fly, I can teach you to fly \n",
      "โช I can teach you to fly, I can teach you to fly โช\n",
      "\n",
      " Spare me your judgments \n",
      "โช spare me your judgments โช\n",
      "\n",
      " and spare me your dreams \n",
      "โช and spare me your dreams โช\n",
      "\n",
      " I sit alone in this winter clarity \n",
      "โช I sit alone in this winter clarity โช\n",
      "\n",
      " which clouds my mind \n",
      "โช which clouds my mind โช\n",
      "\n",
      " Corrupted by \n",
      "โช corrupted by โช\n",
      "\n",
      " the simple sniff of riches blown \n",
      "โช the simple sniff of riches blown โช\n",
      "\n",
      " Look over your hills \n",
      "โช look over your hills โช\n",
      "\n",
      " Rain down, rain down \n",
      "โช rain down, rain down โช\n",
      "\n",
      " nothing plays out \n",
      "โช nothing plays out โช\n",
      "\n",
      " Like you saw in your mind \n",
      "โช like you saw in your mind โช\n",
      "\n",
      " you keep finding out \n",
      "โช you keep finding out โช\n",
      "\n",
      " into the corner \n",
      "โช into the corner โช\n",
      "\n",
      " Hands are tied down \n",
      "โช hands are tied down โช\n",
      "\n",
      " Mm, mm \n",
      "โช mm, mm โช\n",
      "\n",
      " you're backed \n",
      "โช you're backed โช\n",
      "\n",
      " into the corner \n",
      "โช into the corner โช\n",
      "\n",
      " your... \n",
      "โช your... โช\n",
      "\n",
      " You're backed \n",
      "โช you're backed โช\n",
      "\n",
      " by yourself now \n",
      "โช by yourself now โช\n",
      "\n",
      " you're standing \n",
      "โช you're standing โช\n",
      "\n",
      " you see it all now \n",
      "โช you see it all now โช\n",
      "\n",
      " Blue, babe, blue, babe \n",
      "โช blue, babe, blue, babe โช\n",
      "\n",
      " Blue, baby \n",
      "โช blue, baby โช\n",
      "\n",
      " I wanna disappear \n",
      "โช I wanna disappear โช\n",
      "\n",
      " far from the folks... \n",
      "โช far from the folks... โช\n",
      "\n",
      " No one here can tell me \n",
      "โช no one here can tell me โช\n",
      "\n",
      " well, this rat race \n",
      "โช well, this rat race โช\n",
      "\n",
      " has left me limping \n",
      "โช has left me limping โช\n",
      "\n",
      " well, I'll just stay in bed \n",
      "โช well, I'll just stay in bed โช\n",
      "\n",
      " Ahh \n",
      "โช ahh โช\n",
      "\n",
      " Ahh \n",
      "โช ahh โช\n",
      "\n",
      " Ahh \n",
      "โช ahh โช\n",
      "\n",
      " Ahh \n",
      "โช ahh โช\n",
      "\n",
      "Your knowledge of racial slurs is quite extensive\n",
      "โช Your knowledge of racial slurs โช โช Is quite extensive โช\n",
      "\n",
      "And saying certain words can be offensive\n",
      "โช And saying certain words can be offensive โช\n",
      "\n",
      "We weren't so equal many years ago\n",
      "โช We weren't so equal many years ago โช\n",
      "\n",
      "Back then prejudice was status quo\n",
      "โช Back then prejudice was status quo โช\n",
      "\n",
      "People's brains were in their behinds and yours is stuck in 1949, hey\n",
      "โช People's brains were in their behind โช โช And yours is stuck in 1949, hey! โช\n",
      "\n",
      "Season 6 Episode 12 \"Unforgiven\"\n",
      "โช Supernatural 6x13 โช Unforgiven Original Air Date on February 11, 2011\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'th found': 493})"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter()\n",
    "\n",
    "for idx, sent in enumerate(filtered_th):\n",
    "    \n",
    "    if re.search(r'^โช [A-z]', sent) or re.search(r'^โช \\.', sent) or re.search(r'^โช\\sโช', sent):\n",
    "        counter['th found'] += 1\n",
    "        if counter['th found'] < 100:\n",
    "            print(filtered_en[idx])\n",
    "            print(sent)\n",
    "            print()\n",
    " \n",
    "\n",
    " \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter()\n",
    "\n",
    "for idx, sent in enumerate(filtered_en):\n",
    "    \n",
    "    if re.search('♪',sent):\n",
    "        counter['found;'] += 1\n",
    "        if counter['found'] < 100:\n",
    "            print(filtered_en[idx])\n",
    "            print(sent)\n",
    "            print()\n",
    "\n",
    "  \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter()\n",
    "# control_characters = [\n",
    "#     b'\\xc2\\x80',\n",
    "# b'\\xc2\\x81',\n",
    "# b'\\xc2\\x82',\n",
    "# b'\\xc2\\x83',\n",
    "# b'\\xc2\\x84',\n",
    "# b'\\xc2\\x85',\n",
    "# b'\\xc2\\x86',\n",
    "# b'\\xc2\\x87',\n",
    "# b'\\xc2\\x88',\n",
    "# b'\\xc2\\x89',\n",
    "# b'\\xc2\\x8a',\n",
    "# b'\\xc2\\x8b',\n",
    "# b'\\xc2\\x8c',\n",
    "# b'\\xc2\\x8d',\n",
    "# b'\\xc2\\x8e',\n",
    "# b'\\xc2\\x8f',\n",
    "# b'\\xc2\\x90',\n",
    "# b'\\xc2\\x91',\n",
    "# b'\\xc2\\x92',\n",
    "# b'\\xc2\\x93',\n",
    "# b'\\xc2\\x94',\n",
    "# b'\\xc2\\x95',\n",
    "# b'\\xc2\\x96',\n",
    "# b'\\xc2\\x97',\n",
    "# b'\\xc2\\x98',\n",
    "# b'\\xc2\\x99',\n",
    "# b'\\xc2\\x9a',\n",
    "# b'\\xc2\\x9b',\n",
    "# b'\\xc2\\x9c',\n",
    "# b'\\xc2\\x9d',\n",
    "# b'\\xc2\\x9e',\n",
    "# b'\\xc2\\x9f',\n",
    "# ]\n",
    "\n",
    "control_characters = [b'\\xc2\\xa1']\n",
    "for idx, sent in enumerate(filtered_th):\n",
    "    for token in control_characters:\n",
    "        if token in sent.encode('utf-8'):\n",
    "            print('found', token)\n",
    "            counter['th found \\fn'] += 1\n",
    "            if counter['th found \\fn'] < 100:\n",
    "                print(filtered_en[idx])\n",
    "                print(sent)\n",
    "                print(sent.encode('utf-8'))\n",
    "                print('--')\n",
    "            break\n",
    " \n",
    " \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter()\n",
    "\n",
    "for idx, sent in enumerate(filtered_th):\n",
    "    \n",
    "    if re.search(\"เเ\", sent):\n",
    "        counter['th found ? '] += 1\n",
    "        if counter['th found ? '] < 30:\n",
    "            print(filtered_en[idx])\n",
    "            print(sent)\n",
    "            print()\n",
    " \n",
    " \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_counter = {\n",
    "    'th': Counter(),\n",
    "    'en': Counter()\n",
    "}\n",
    "for i in range(len(filtered_th)):\n",
    "    for lang in ['th', 'en']:\n",
    "        if lang == 'th':\n",
    "            sentence_counter[lang][filtered_th[i]] += 1\n",
    "        if lang == 'en':\n",
    "            sentence_counter[lang][filtered_en[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ใช่', 10609),\n",
       " ('ไม่', 6691),\n",
       " ('โอเค', 6606),\n",
       " ('ขอบคุณ', 4760),\n",
       " ('เฮ้', 3365),\n",
       " ('อะไรนะ?', 3246),\n",
       " ('ครับ', 2709),\n",
       " ('อะไรนะ', 2664),\n",
       " ('อะไร?', 2601),\n",
       " ('ไม่!', 2097)]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_counter['th'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What?', 13268),\n",
       " ('Yeah.', 12373),\n",
       " ('No.', 10795),\n",
       " ('Okay.', 7995),\n",
       " ('Yes.', 6653),\n",
       " ('Thank you.', 6488),\n",
       " ('Hey.', 4278),\n",
       " ('No!', 3772),\n",
       " (\"I don't know.\", 3650),\n",
       " ('Why?', 3565)]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_counter['en'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โซ | Zo\n",
      "โซ | Zoe\n",
      "โซ | Zo?\n",
      "โซ | Zo?\n",
      "โซ | Zo?\n",
      "โซ | Zo?\n"
     ]
    }
   ],
   "source": [
    "print_only = 100\n",
    "count = 0\n",
    "for i in range(len(filtered_th)):\n",
    "    if filtered_th[i] == 'โซ' and count < print_only:\n",
    "        print(filtered_th[i], '|', filtered_en[i])\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "กรุณารอ.\n",
      "Wait, please.\n",
      "\n",
      "สวัสดี.\n",
      "Hello there.\n",
      "\n",
      "นั่นดีกว่า.\n",
      "That's better.\n",
      "\n",
      "สมเด็จพระราชินี.\n",
      "The Queen.\n",
      "\n",
      "ไปได้.\n",
      "Now, go.\n",
      "\n",
      "ใช่.\n",
      "Yes.\n",
      "\n",
      "มาสิ ชายไก่ ติดตามฉัน.\n",
      "Come on, hen... Men. Follow me.\n",
      "\n",
      "เงียบ.\n",
      "Quiet.\n",
      "\n",
      "อย่าปล่อยให้เขา หยุดเขา.\n",
      "Don't let him.Stop him.\n",
      "\n",
      "แค่นั้นแหละ.\n",
      "That's it.\n",
      "\n",
      "count 168365\n"
     ]
    }
   ],
   "source": [
    "print_only = 10\n",
    "count = 0\n",
    "for i in range(len(filtered_th)):\n",
    "    if re.search(r'\\.$', filtered_th[i]):\n",
    "        if count < print_only:\n",
    "            print(filtered_th[i])\n",
    "            print(filtered_en[i])\n",
    "            print()\n",
    "        count += 1\n",
    "print('count', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = {\n",
    "    'th': {\n",
    "        'sentencepiece': [],\n",
    "        'newmm':[]\n",
    "    },\n",
    "    'en': {\n",
    "        'sentencepiece': [],\n",
    "        'newmm':[]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a Segment texts into tokens with `newmm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.81156301498413 s\n",
      "39.77449178695679 s\n"
     ]
    }
   ],
   "source": [
    "toks['th']['newmm'] = tokenize_handler(filtered_th, lang='th')\n",
    "toks['en']['newmm'] = tokenize_handler(filtered_en, lang='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ทาส ใน กระจก วิเศษ , มาจาก พื้นที่ ที่ ไกล ที่สุด',\n",
       "  'ผ่าน ลม และ ความมืด ฉัน เรียก เจ้า',\n",
       "  'พูด !',\n",
       "  'ให้ ฉัน เห็น พระพักตร์ ของ พระองค์',\n",
       "  'สิ่ง ที่ เจ้า จะ รู้ ว่า สมเด็จ พระราชินี ของ ฉัน ได้ อย่างไร',\n",
       "  'กระจก วิเศษ บน ผนัง ผู้ ที่ เป็น สังขาร หนึ่ง ทั้งหมด หรือไม่',\n",
       "  'ที่ มีชื่อเสียง เป็น ความงาม ของ เจ้า พระ บาท สมเด็จ พระเจ้าอยู่หัว',\n",
       "  'แต่ ถือเป็น แม่บ้าน ที่ น่ารัก ที่ ฉัน เห็น',\n",
       "  'ยาจก ไม่ สามารถ ซ่อน พระคุณ อ่อนโยน ของ เธอ',\n",
       "  'อนิจจา เธอ มี ความเป็นธรรม มากขึ้น กว่า เจ้า'],\n",
       " ['Slave in the Magic Mirror , come from the farthest space .',\n",
       "  'Through wind and darkness , I summon thee .',\n",
       "  'Speak !',\n",
       "  'Let me see thy face .',\n",
       "  'What wouldst thou know , my Queen ?',\n",
       "  'Magic Mirror on the wall , who is the fairest one of all ?',\n",
       "  'Famed is thy beauty , Majesty .',\n",
       "  'But hold , a lovely maid I see .',\n",
       "  'Rags cannot hide her gentle grace .',\n",
       "  'Alas , she is more fair than thee .'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks['th']['newmm'][0:10], toks['en']['newmm'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b Segment texts into BPE tokens with SentencePiece (BPEmb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bpe(sentences, lang, n_vocab=25000):\n",
    "    \"\"\"Return a list of bpe tokens give a list of sentences\"\"\"\n",
    "    segmented_sentences = []\n",
    "    for sentence in tqdm_notebook(sentences, total=len(sentences)):\n",
    "#         print(sentence)\n",
    "        bpe_tokens = bpemb_pretrained[lang]['{}'.format(n_vocab)].encode(sentence)\n",
    "        segmented_sentences.append(' '.join(bpe_tokens))\n",
    "        \n",
    "    return segmented_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Thai language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce4eb5e8cbe469886307235a01f1dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3202751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['▁ท าส ใน กระจก วิเศษ , ▁มาจาก พื้นที่ ที่ ไกล ที่สุด', '▁ผ่าน ลม และความ มืด ฉัน เรียก เจ้า', '▁พูด !', '▁ให้ ฉัน เห็น พระพักตร์ ของ ▁พระองค์', '▁สิ่งที่ เจ้า จะ รู้ว่า สมเด็จพระราชินี ▁ของ ฉัน ได้อย่างไร', '▁กระจ ก วิเศษ บน ผนัง ▁ผู้ ที่เป็น สัง ขาร หนึ่ง ทั้งหมด ▁หรือไม่', '▁ที่มีชื่อเสียง เป็น ความงาม ของ ▁เจ้า พระบาทสมเด็จพระ เจ้าอยู่หัว', '▁แต่ ถือเป็น แม่ บ้าน ที่น ่ารัก ที่ ฉัน ▁เห็น', '▁ยา จก ไม่สามารถ ซ่อน พระคุณ ▁อ่อน โยน ของเธอ', '▁อน ิจ จา เธอ มีความเป็น ธรรม ▁มาก ขึ้น กว่า เจ้า']\n"
     ]
    }
   ],
   "source": [
    "toks['th']['sentencepiece'] = encode_bpe(filtered_th, 'th', 25000)\n",
    "\n",
    "print(toks['th']['sentencepiece'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 English language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0ac2a9e16c4fe0814a82c713b07a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3202751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['▁slave ▁in ▁the ▁magic ▁mirror , ▁come ▁from ▁the ▁fart hest ▁space .', '▁through ▁wind ▁and ▁darkness , ▁i ▁summon ▁the e .', '▁speak !', '▁let ▁me ▁see ▁thy ▁face .', '▁what ▁would st ▁thou ▁know , ▁my ▁queen ?', '▁magic ▁mirror ▁on ▁the ▁wall , ▁who ▁is ▁the ▁fa ire st ▁one ▁of ▁all ?', '▁famed ▁is ▁thy ▁beauty , ▁majesty .', '▁but ▁hold , ▁a ▁lov ely ▁maid ▁i ▁see .', '▁ra gs ▁cannot ▁hide ▁her ▁gentle ▁grace .', '▁al as , ▁she ▁is ▁more ▁fair ▁than ▁the e .']\n"
     ]
    }
   ],
   "source": [
    "toks['en']['sentencepiece']  = encode_bpe(filtered_en, 'en', 25000)\n",
    "print(toks['en']['sentencepiece'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split train-valid-test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N =  3202751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2562200, 320275, 320276)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train-valid-test split 80/10/10\n",
    "\n",
    "n = len(toks['th']['newmm'])\n",
    "\n",
    "print('N = ',n)\n",
    "idx = list(range(n))\n",
    "\n",
    "random.seed(1234) # Set SEED\n",
    "random.shuffle(idx)\n",
    "\n",
    "train_idx, valid_idx, test_idx = idx[:int(n*0.8)], idx[int(n*0.8):int(n*0.9)], idx[int(n*0.9):]\n",
    "\n",
    "dataset_split = {}\n",
    "dataset_split['train'] = train_idx\n",
    "dataset_split['valid'] = valid_idx\n",
    "dataset_split['test'] = test_idx\n",
    "\n",
    "\n",
    "len(train_idx),len(valid_idx),len(test_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    },\n",
    "    'valid': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    },\n",
    "    'test': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    for lang in ['th', 'en']:\n",
    "        for tok_type in ['sentencepiece', 'newmm']:\n",
    "\n",
    "            dataset[split_name][lang][tok_type] = [toks[lang][tok_type][i] for i in dataset_split[split_name]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['เบค กี้ เธอ ทำท่า แปลก ๆ เมื่อกี้ ใน ห้อง', 'อยู่ กับ เธอ แอน นา จะ นำทาง คุณ ผม จะ กลับ ไป'] \n",
      "\n",
      "['Becky , um , you were acting particularly strange in there just now .', \"Stay with her so Anna can guide you . I ' m going back .\"] \n",
      "\n",
      "['▁เบ ค กี้ ▁เธอ ทํา ท่า แปลก ๆ ▁เมื่อ กี้ ▁ในห้อง', '▁ อยู่กับ เธอ ▁แอนนา จะนํา ทาง คุณ ▁ผม จะ กลับไป'] \n",
      "\n",
      "['▁bec ky , ▁um , ▁you ▁were ▁acting ▁particularly ▁strange ▁in ▁there ▁just ▁now .', \"▁stay ▁with ▁her ▁so ▁anna ▁can ▁guide ▁you . ▁i ' m ▁going ▁back .\"] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['th']['newmm'][0:2],'\\n')\n",
    "print(dataset['train']['en']['newmm'][0:2],'\\n')\n",
    "print(dataset['train']['th']['sentencepiece'][0:2],'\\n')\n",
    "print(dataset['train']['en']['sentencepiece'][0:2],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'en_train_n_toks': 92383739, 'th_train_n_toks': 86683223, 'en_valid_n_toks': 11536351, 'en_test_n_toks': 11535798, 'th_test_n_toks': 10833242, 'th_valid_n_toks': 10826042})\n"
     ]
    }
   ],
   "source": [
    "# Counting number of tokens for train, valid, test\n",
    "counter = Counter( )\n",
    "for dataset_type in ['train', 'valid', 'test']:\n",
    "    for th_sent_toks in dataset[dataset_type]['th']['newmm']:\n",
    "        counter['th_{}_n_toks'.format(dataset_type)] += len(th_sent_toks)\n",
    "    for en_sent_toks in dataset[dataset_type]['en']['newmm']:\n",
    "        counter['en_{}_n_toks'.format(dataset_type)] += len(en_sent_toks)\n",
    "\n",
    "print(counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-sentencepiece/th-en\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-sentencepiece/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-sentencepiece/en-th\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-sentencepiece/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-newmm/th-en\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-newmm/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-newmm/en-th\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-newmm/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-sentencepiece/th-en\n",
      "dir: ../data/opensubtitles_bin/newmm-sentencepiece/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-sentencepiece/en-th\n",
      "dir: ../data/opensubtitles_bin/newmm-sentencepiece/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-newmm/th-en\n",
      "dir: ../data/opensubtitles_bin/newmm-newmm/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-newmm/en-th\n",
      "dir: ../data/opensubtitles_bin/newmm-newmm/en-th\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tok_type_src in ['sentencepiece', 'newmm']:\n",
    "    for tok_type_tgt in ['sentencepiece', 'newmm']:\n",
    "        langs = ['th', 'en']\n",
    "        for lang in langs:\n",
    "            src_lang = lang\n",
    "            tgt_lang = 'en' if lang =='th' else 'th'\n",
    "            FOLDER_NAME = \"opensubtitles_tok/{}-{}/{}-{}\".format(tok_type_src, tok_type_tgt, src_lang, tgt_lang )\n",
    "            FOLDER_NAME_BIN = \"opensubtitles_bin/{}-{}/{}-{}\".format(tok_type_src, tok_type_tgt, src_lang, tgt_lang)\n",
    "           \n",
    "            \n",
    "            # Create directories\n",
    "            print('create directories: ')\n",
    "            print('dir: ../data/{}'.format(FOLDER_NAME))\n",
    "            print('dir: ../data/{}'.format(FOLDER_NAME_BIN))\n",
    "\n",
    "            !mkdir -p ../data/{FOLDER_NAME}\n",
    "            !mkdir -p ../data/{FOLDER_NAME_BIN}\n",
    "\n",
    "            for split_name in ['train', 'valid', 'test']:\n",
    "                \n",
    "                write_spaced_tokens_to_file(dataset[split_name][src_lang][tok_type_src],\n",
    "                                            FOLDER_NAME, '{}.{}'.format(split_name, src_lang))\n",
    "                \n",
    "                write_spaced_tokens_to_file(dataset[split_name][tgt_lang][tok_type_tgt],\n",
    "                                            FOLDER_NAME, '{}.{}'.format(split_name, tgt_lang))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁bec ky , ▁um , ▁you ▁were ▁acting ▁particularly ▁strange ▁in ▁there ▁just ▁now .\n",
      "▁stay ▁with ▁her ▁so ▁anna ▁can ▁guide ▁you . ▁i ' m ▁going ▁back .\n",
      "▁look .\n",
      "▁oh , ▁no , ▁it ' s ▁the ▁other ▁way ▁around , ▁dr . ▁lewis .\n",
      "▁sort ▁of .\n",
      "▁bart ender , ▁something ▁really ▁strong , ▁please .\n",
      "▁yes , ▁obviously .\n",
      "▁la ' s ▁so ▁nice .\n",
      "▁i ' m ▁going ▁to ▁fix ▁it .\n",
      "▁i ▁get ▁b ored .\n"
     ]
    }
   ],
   "source": [
    "!head ../data/opensubtitles_tok/newmm-sentencepiece/th-en/train.en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "เบค กี้ เธอ ทำท่า แปลก ๆ เมื่อกี้ ใน ห้อง\n",
      "อยู่ กับ เธอ แอน นา จะ นำทาง คุณ ผม จะ กลับ ไป\n",
      "ฟัง นะ\n",
      "พอดี เลย ดร. ลี วิ ส\n",
      "แบบ ว่า\n",
      "เอ่อ บาร์ เท็น เด อร ์ ขอ อะไร ที่\n",
      "ก็ ใช่ ห น่ะ สิ\n",
      "แอลเอ สวย เนอะ\n",
      "ฉัน กำลังจะ แก้ ไขมัน\n",
      "ฉัน เบื่อ ละ\n"
     ]
    }
   ],
   "source": [
    "!head ../data/opensubtitles_tok/newmm-sentencepiece/th-en/train.th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
