{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# import sys\n",
    "# sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import copy \n",
    "import re\n",
    "import html\n",
    "import numpy as np\n",
    "\n",
    "from time import time \n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import pythainlp\n",
    "from pythainlp.util import *\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.ulmfit import *\n",
    "\n",
    "# subword-nmt\n",
    "from subword_nmt import learn_bpe as learner\n",
    "from subword_nmt import apply_bpe as subword_tokenizer\n",
    "\n",
    "import fairseq \n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pythainlp.tokenize import DEFAULT_DICT_TRIE\n",
    "\n",
    "from pythainlp.corpus import thai_words\n",
    "\n",
    "print(pythainlp.__version__)\n",
    "# assert pythainlp.__version__ == '2.1'\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import sentencepiece as spm\n",
    "import tf_sentencepiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install BPEmb (BPE embeddings)\n",
    "\n",
    "# !pip install --q bpemb emoji subword-nmt fairseq tensorflow_hub sentencepiece tf_sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# Universal Sentence Encoder\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/1\")\n",
    "    embedded_text = embed(text_input)\n",
    "    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "g.finalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_similarity(src_sent, tgt_sent):\n",
    "    \"\"\"\n",
    "        Calculate sentence similarity based on Google Universal Sentence Encoder (Multilingual Large)\n",
    "    \"\"\"\n",
    "    session = tf.Session(graph=g)\n",
    "    session.run(init_op)\n",
    "    \n",
    "    src_result = session.run(embedded_text, feed_dict={text_input: [src_sent]})\n",
    "    tgt_result = session.run(embedded_text, feed_dict={text_input: [tgt_sent]})\n",
    "    \n",
    "    return np.inner(src_result, tgt_result).reshape(1)[0]\n",
    "\n",
    "def compute_similarity_list(src_sent, tgt_sent):\n",
    "    \"\"\"\n",
    "        Calculate sentence similarity based on Google Universal Sentence Encoder (Multilingual Large)\n",
    "    \"\"\"\n",
    "    session = tf.Session(graph=g)\n",
    "    session.run(init_op)\n",
    "    \n",
    "    src_results = session.run(embedded_text, feed_dict={text_input: src_sent})\n",
    "    tgt_results = session.run(embedded_text, feed_dict={text_input: tgt_sent})\n",
    "\n",
    "    similarities = []\n",
    "    for index, src_result in enumerate(src_results):\n",
    "        similarity = np.inner(src_results[index], tgt_results[index]).reshape(1)[0]\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8449457\n",
      "0.7397201\n",
      "[0.8449458, 0.7397201]\n"
     ]
    }
   ],
   "source": [
    "print(compute_similarity(\"ฉันชื่อยีน\",\"My Name is Gene\"))\n",
    "# print(compute_similarity(\"ฉันชื่อยีน\",\"My Name is Jane\"))\n",
    "print(compute_similarity(\"ฉันชื่อยีน\",\"My Name is Jeans\"))\n",
    "# print(compute_similarity(\"ฉันชื่อยีน\",\"My Name is Jean\"))\n",
    "print(compute_similarity_list([\"ฉันชื่อยีน\", \"ฉันชื่อยีน\"], [\"My Name is Gene\", \"My Name is Jeans\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "\n",
    "bpemb_pretrained ={\n",
    "    'th': {\n",
    "        '25000': BPEmb(lang=\"th\", vs=25000)\n",
    "    },\n",
    "    'en': {\n",
    "        '25000': BPEmb(lang=\"en\", vs=25000)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3281534,\n",
       " ['Slave in the Magic Mirror, come from the farthest space.',\n",
       "  'Through wind and darkness, I summon thee.',\n",
       "  'Speak!'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/opensubtitle_v2018/OpenSubtitles.en-th.en','r', encoding='utf-8') as f:\n",
    "    en = f.read().split('\\n')\n",
    "len(en),en[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3281534,\n",
       " ['ทาสในกระจกวิเศษ, มาจากพื้นที่ที่ไกลที่สุด',\n",
       "  'ผ่านลมและความมืดฉันเรียกเจ้า',\n",
       "  'พูด!'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/opensubtitle_v2018/OpenSubtitles.en-th.th','r', encoding='utf-8') as f:\n",
    "    th = f.read().split('\\n')\n",
    "    \n",
    "len(th),th[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Opensubtitles_v2018\n",
    "\n",
    "\n",
    "## 1.1 Text cleaning, filtering out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt_opus.preprocess import (\n",
    "    SentenceLengthLessThanOrEqualToOne,\n",
    "    SentenceContainsUnknownSymbol,\n",
    "    SentenceContainsAdSymbol,\n",
    "    ThaiSentenceContainsNoThaiCharacters,\n",
    "    ThaiSentenceContainsNoThaiCharactersPattern,\n",
    "    ThaiSentenceContainsUnwantedPattern,\n",
    "    EnglishSentenceContainsUnwantedPattern,\n",
    "    SentenceContainsOnlyAsterisk,\n",
    "    \n",
    "    UnescapeString,\n",
    "    RemoveUnwantedSymbols,\n",
    "    RemoveUnwantedPattern,\n",
    "    ReplaceDashInSentence,\n",
    "    RemoveHashtagInSentence,\n",
    "    RemoveFullStopInThaiSentence,\n",
    "    NormalizeThaiVowel,\n",
    "    ReplaceAsteriskInSentence,\n",
    "    RemoveColonInSentence,\n",
    "    RemoveSemiColonInSentence,\n",
    "    FormatTime,\n",
    "    \n",
    "    SentencePairFoundRepeatedText,\n",
    "    SentencePairTokenLengthsDifferGreaterThreashold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to filter out a sentence pair\n",
    "filtering_rules = [\n",
    "    SentenceLengthLessThanOrEqualToOne(),\n",
    "    SentenceContainsAdSymbol(),\n",
    "    SentenceContainsUnknownSymbol(),\n",
    "    ThaiSentenceContainsNoThaiCharacters(),\n",
    "    ThaiSentenceContainsNoThaiCharactersPattern(),\n",
    "    SentenceContainsOnlyAsterisk(),\n",
    "]\n",
    "\n",
    "cleaning_rules = [\n",
    "    UnescapeString(),\n",
    "    RemoveUnwantedSymbols(),\n",
    "    RemoveUnwantedPattern(),\n",
    "    ReplaceDashInSentence(),\n",
    "    RemoveHashtagInSentence(),\n",
    "    RemoveFullStopInThaiSentence(),\n",
    "    NormalizeThaiVowel(),\n",
    "    ReplaceDashInSentence(),\n",
    "    ReplaceAsteriskInSentence(),\n",
    "    RemoveColonInSentence(),\n",
    "    RemoveSemiColonInSentence(),\n",
    "    FormatTime(),\n",
    "]\n",
    "\n",
    "filtering_sentence_pair_rules = [\n",
    "    SentencePairFoundRepeatedText(),\n",
    "    SentencePairTokenLengthsDifferGreaterThreashold(threshold=0.85),\n",
    "]\n",
    "\n",
    "filtering_rules_post = [\n",
    "    SentenceLengthLessThanOrEqualToOne(),\n",
    "    ThaiSentenceContainsUnwantedPattern(),\n",
    "    EnglishSentenceContainsUnwantedPattern(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could be parallelized\n",
    "def filter_sentence(sentence, lang, rules=filtering_rules):\n",
    "    \"\"\"\n",
    "        Return True if a sentence match filtering pattern\n",
    "    \"\"\"\n",
    "    for rule in rules:\n",
    "        rule_obj = rule\n",
    "        if rule_obj.test(sentence, lang=lang):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filter_sentences(sentences, lang, rules=filtering_rules):\n",
    "    \"\"\"\n",
    "        Returns a list of Boolean value, if such element is True, it means filter that sentence pair.\n",
    "        Otherwise, keep that sentnence pair.\n",
    "    \"\"\"\n",
    "    filtering_indices = []\n",
    "    p = Pool() # use all available cores\n",
    "    t = time()\n",
    "\n",
    "    _filter_sentence = partial(filter_sentence, lang=lang, rules=rules)\n",
    "    filtering_indices = p.map(_filter_sentence, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    filtering_indices_np = np.array(filtering_indices)\n",
    "    number_of_filtered_out = np.sum(filtering_indices_np)\n",
    "    \n",
    "    print('Time taken: {} s'.format(time() -t))\n",
    "    print('# sentences ({}) before filtered out'.format(lang), len(sentences))\n",
    "    print('# sentences ({}) filtered out'.format(lang), number_of_filtered_out)\n",
    "    print('# sentences ({}) after filtered out'.format(lang), len(sentences) - number_of_filtered_out)\n",
    "\n",
    "    return filtering_indices_np, number_of_filtered_out\n",
    "\n",
    "\n",
    "def filter_sentence_pair(sentence_pair, rules=filtering_sentence_pair_rules):\n",
    "    \"\"\"\n",
    "        Return True if a sentence match filtering sentence pair pattern\n",
    "    \"\"\"\n",
    "    for rule in rules:\n",
    "        rule_obj = rule\n",
    "        if rule_obj.test(sentence_pair):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filter_sentence_pairs(sentence_pairs, rules=filtering_sentence_pair_rules):\n",
    "    \"\"\"\n",
    "        Returns a list of Boolean value, if such element is True, it means filter that sentence pair.\n",
    "        Otherwise, keep that sentnence pair.\n",
    "    \"\"\"\n",
    "    filtering_indices = []\n",
    "    p = Pool() # use all available cores\n",
    "    t = time()\n",
    "\n",
    "    _filter_sentence_pair = partial(filter_sentence_pair, rules=rules)\n",
    "    filtering_indices = p.map(_filter_sentence_pair, sentence_pairs)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    filtering_indices_np = np.array(filtering_indices)\n",
    "    number_of_filtered_out = np.sum(filtering_indices_np)\n",
    "    \n",
    "    print('Time taken: {} s'.format(time() -t))\n",
    "    print('# sentences before filtered out', len(sentence_pairs))\n",
    "    print('# sentences filtered out', number_of_filtered_out)\n",
    "    print('# sentences after filtered out', len(sentence_pairs) - number_of_filtered_out)\n",
    "\n",
    "    return filtering_indices_np, number_of_filtered_out\n",
    "\n",
    "\n",
    "def clean_sentence(sentence, lang, rules=cleaning_rules):\n",
    "    for rule in rules: \n",
    "        rule_obj = rule\n",
    "        if rule_obj.test(sentence, lang=lang):\n",
    "            sentence = rule_obj.replace(sentence, lang=lang)\n",
    "    return sentence\n",
    "\n",
    "def clean_sentences(sentences, lang, rules=cleaning_rules):\n",
    "    \"\"\"\n",
    "        Clean the sentence with the specified text cleaning rules\n",
    "        Return a list of cleaned sentences\n",
    "    \"\"\"\n",
    "    p = Pool() # use all available cores\n",
    "    t = time()\n",
    "    \n",
    "    _clean_sentence = partial(clean_sentence, lang=lang, rules=rules)\n",
    "    cleaned_sentences = p.map(_clean_sentence, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    print('Time taken: {} s'.format(time() -t))\n",
    "    \n",
    "    return cleaned_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Filter by each language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 23.54386019706726 s\n",
      "# sentences (th) before filtered out 3281534\n",
      "# sentences (th) filtered out 77301\n",
      "# sentences (th) after filtered out 3204233\n"
     ]
    }
   ],
   "source": [
    "filtering_indices_th, _ = filter_sentences(th, lang='th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 5.6949920654296875 s\n",
      "# sentences (en) before filtered out 3281534\n",
      "# sentences (en) filtered out 4577\n",
      "# sentences (en) after filtered out 3276957\n"
     ]
    }
   ],
   "source": [
    "filtering_indices_en, _ = filter_sentences(en, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# filtering_indices_th_en 79608\n"
     ]
    }
   ],
   "source": [
    "filtering_indices_th_en = filtering_indices_th | filtering_indices_en\n",
    "\n",
    "print('# filtering_indices_th_en', sum(filtering_indices_th_en))\n",
    "\n",
    "filtered_th = [th[i] for i, filtered_out in enumerate(filtering_indices_th_en) if not filtered_out]\n",
    "filtered_en = [en[i] for i, filtered_out in enumerate(filtering_indices_th_en) if not filtered_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# filtered_th 3201926\n",
      "# filtered_en 3201926\n"
     ]
    }
   ],
   "source": [
    "print('# filtered_th', len(filtered_th))\n",
    "print('# filtered_en', len(filtered_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3202604"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3281534 - 78930"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Filter by pair of source and target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 142.86196208000183 s\n",
      "# sentences before filtered out 3201926\n",
      "# sentences filtered out 6985\n",
      "# sentences after filtered out 3194941\n"
     ]
    }
   ],
   "source": [
    "\n",
    "th_en_tuples = [(filtered_th[i], filtered_en[i]) for i in range(0, len(filtered_th))]\n",
    "filtering_pairs_indices_th_en, _ = filter_sentence_pairs(th_en_tuples)\n",
    "\n",
    "filtered_pairs_th = [filtered_th[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if not filtered_out]\n",
    "filtered_pairs_en = [filtered_en[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if not filtered_out]\n",
    "sentence_paris_filted_out = [th_en_tuples[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if filtered_out]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# th_en_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# filtered_pairs_th 3194941\n",
      "# filtered_pairs_en 3194941\n"
     ]
    }
   ],
   "source": [
    "print('# filtered_pairs_th', len(filtered_pairs_th))\n",
    "print('# filtered_pairs_en', len(filtered_pairs_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in filtered_pairs_en:\n",
    "    if len(sent) <= 1:\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 104.53533387184143 s\n"
     ]
    }
   ],
   "source": [
    "cleaned_th = clean_sentences(filtered_pairs_th, lang=\"th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 69.936194896698 s\n"
     ]
    }
   ],
   "source": [
    "cleaned_en = clean_sentences(filtered_pairs_en, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_th 3194941\n"
     ]
    }
   ],
   "source": [
    "print('cleaned_th', len(cleaned_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore cleaned sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = Counter()\n",
    "# for i, sent in enumerate(cleaned_th):\n",
    "#     if len(sent) <= 1:\n",
    "#         c['found'] += 1\n",
    "#         if c['found'] <= 15:\n",
    "#             print(sent,'<>', cleaned_en[i])\n",
    "#             print()\n",
    "# c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Post-filtering out rule\n",
    "\n",
    "- redo the filtering rules for 1) each langauge individually and 2) a language pair again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 5.934873104095459 s\n",
      "# sentences (th) before filtered out 3194941\n",
      "# sentences (th) filtered out 111\n",
      "# sentences (th) after filtered out 3194830\n",
      "Time taken: 5.803123950958252 s\n",
      "# sentences (en) before filtered out 3194941\n",
      "# sentences (en) filtered out 277\n",
      "# sentences (en) after filtered out 3194664\n"
     ]
    }
   ],
   "source": [
    "filtering_post_indices_th, _ = filter_sentences(cleaned_th, lang='th', rules=filtering_rules_post)\n",
    "filtering_post_indices_en, _ = filter_sentences(cleaned_en, lang='en', rules=filtering_rules_post)\n",
    "\n",
    "filtering_post_indices_th_en = filtering_post_indices_th | filtering_post_indices_en\n",
    "\n",
    "\n",
    "cleaned_and_filtered_th = [cleaned_th[i] for i, filtered_out in enumerate(filtering_post_indices_th_en) if not filtered_out]\n",
    "cleaned_and_filtered_en = [cleaned_en[i] for i, filtered_out in enumerate(filtering_post_indices_th_en) if not filtered_out]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 157.341943025589 s\n",
      "# sentences before filtered out 3194570\n",
      "# sentences filtered out 1200\n",
      "# sentences after filtered out 3193370\n"
     ]
    }
   ],
   "source": [
    "th_en_tuples = [(cleaned_and_filtered_th[i], cleaned_and_filtered_en[i]) for i in range(0, len(cleaned_and_filtered_th))]\n",
    "filtering_pairs_indices_th_en, _ = filter_sentence_pairs(th_en_tuples)\n",
    "\n",
    "cleaned_and_filtered_th = [cleaned_and_filtered_th[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if not filtered_out]\n",
    "cleaned_and_filtered_en = [cleaned_and_filtered_en[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if not filtered_out]\n",
    "sentence_paris_filted_out = [th_en_tuples[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if filtered_out]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After 4 steps\n",
    "\n",
    "# number of sentence pairs is 3,201,120\n",
    "# number of sentence pairs is 3,200,973 (Add SentenceContainsAsterink rule)\n",
    "# number of sentence pairs is 3,194,664(Add SentencePairTokenLengthsDifferGreaterThreashold rule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = Counter()\n",
    "# for i, sent in enumerate(cleaned_and_filtered_th):\n",
    "\n",
    "#     if re.search(r\"--\", sent):\n",
    "#         c['count'] += 1\n",
    "#         if c['count'] <= 20:\n",
    "#             print(cleaned_and_filtered_th[i])\n",
    "#             print(cleaned_and_filtered_en[i])\n",
    "#             print('')\n",
    "\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter()\n",
    "_tokenize = partial(word_tokenize, engine=\"newmm\", keep_whitespace=False)\n",
    "\n",
    "threashold = 0.85\n",
    "for i, sent in tqdm_notebook(enumerate(cleaned_and_filtered_th), total=len(cleaned_and_filtered_th)):\n",
    "    \n",
    "    src_toks, tgt_toks = _tokenize(cleaned_and_filtered_th[i]), _tokenize(cleaned_and_filtered_en[i])\n",
    "    src_toks_len, tgt_toks_len = len(src_toks), len(tgt_toks)\n",
    "        \n",
    "    diff = abs(src_toks_len- tgt_toks_len )\n",
    "    diff_ratio = diff / ( max(src_toks_len, tgt_toks_len) ) \n",
    "\n",
    "    if  diff_ratio >= threashold:\n",
    "        c['counter_{:.2}'.format(threashold)] += 1\n",
    "\n",
    "        if c['counter_{:.2}'.format(threashold)] <= 200:\n",
    "\n",
    "            print(sent, '---', cleaned_and_filtered_en[i], ' --- diff/ diff_ratio:', diff, diff_ratio)\n",
    "            print()\n",
    "print('count:', c['counter_{:.2}'.format(threashold)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07870741374a455f9707432088b2a90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=320), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ฉันหวังว่าฉันหวัง \n",
      " I'm hoрing -I'm hoрing \n",
      " similariry 0.20642447 \n",
      "\n",
      "จมูกของฉัน! \n",
      " Take them away. \n",
      " similariry 0.10670066 \n",
      "\n",
      "ลัปล แดปลินส \n",
      " Lumple dapplins? \n",
      " similariry 0.15752248 \n",
      "\n",
      "ทำไมเงียบไป? \n",
      " Cat got your tongue? \n",
      " similariry 0.2079348 \n",
      "\n",
      "ไม่ไม่ไม่ไม่ได้รับความตื่นเต้น! \n",
      " That's the tub. \n",
      " similariry 0.060022634 \n",
      "\n",
      "เดอะควีนเก่ากลับกลอกหนึ่ง \n",
      " The... \n",
      " similariry 0.041958585 \n",
      "\n",
      "เคลีย พื้นที่ \n",
      " Stand clear! \n",
      " similariry 0.14087138 \n",
      "\n",
      "เคลีย พื้นที่ \n",
      " Stand clear. \n",
      " similariry 0.20175225 \n",
      "\n",
      "ครับผม ตรวจฉนวน \n",
      " Yes, the fuse. \n",
      " similariry 0.19460742 \n",
      "\n",
      "ให้ตายสิ เผ่นเร็ว \n",
      " Look out! Run! \n",
      " similariry 0.14973816 \n",
      "\n",
      "เครื่องบิน ข้าศึก \n",
      " Air raid. After Big Bertha. \n",
      " similariry 0.19183967 \n",
      "\n",
      "ขอบใจหลาย ในความกรุณา โทไมเนีย ภูมิใจที่มีนาย \n",
      " Thank you. I'll see you get the Tomainian Cross for this. \n",
      " similariry 0.21292156 \n",
      "\n",
      "ผมเต็มใจช่วย ครับผม \n",
      " Only too willing to oblige, sir. \n",
      " similariry 0.24054368 \n",
      "\n",
      "โอ้ย.. เรื่องขี้หมา \n",
      " I can try. \n",
      " similariry 0.067670666 \n",
      "\n",
      "จะต้องไปส่งเอกสารสำคัญ \n",
      " Hold on to these dispatches. \n",
      " similariry 0.07264836 \n",
      "\n",
      "มันล้ำลึกเกินฉันจะเข้าใจ \n",
      " I can't reach it. \n",
      " similariry 0.20106304 \n",
      "\n",
      "วิธีที่เธอดูแลมัน \n",
      " How she loves daffodils! \n",
      " similariry 0.24357036 \n",
      "\n",
      "อดีนอย เฮนเคิล กล่าวและฉันแปล \n",
      " Adenoid Hynkel said, \"Tomainia was down but today has risen.\" \n",
      " similariry 0.08030888 \n",
      "\n",
      "ผู้นำของคุณ \n",
      " This is the Pari-Mutual network, bringing you \n",
      " similariry 0.22762144 \n",
      "\n",
      "โทไมเนีย \n",
      " Stand by for further commentary. Go ahead, Tomainia. \n",
      " similariry 0.004185604 \n",
      "\n",
      "มันไม่แน่นอนหรอก \n",
      " Now you said it. \n",
      " similariry 0.1128034 \n",
      "\n",
      "เซ็น.. ไว้ก่อน \n",
      " Just charge it to my account! \n",
      " similariry 0.11659287 \n",
      "\n",
      "ทุกคนมีสิทธิ์จะได้มัน \n",
      " If I were a man I'd show you. \n",
      " similariry 0.2037933 \n",
      "\n",
      "ฉันก็มีมือ มีเท้า ที่จะมีการโต้ตอบแกบ้าง \n",
      " Not one of you has the guts to stand up alone and fight! \n",
      " similariry 0.06469639 \n",
      "\n",
      "หุบปาก \n",
      " Shut up! \n",
      " similariry 0.20029831 \n",
      "\n",
      "ฉันไม่ได้ทำร้ายใคร แต่ฉันเป็นผู้ให้มากกว่า \n",
      " Don't rob the poor girl, boys. Give her back her tomatoes. \n",
      " similariry 0.19794509 \n",
      "\n",
      "เร็วเข้า ไสหัวออกไป \n",
      " Come on, get out of here! \n",
      " similariry 0.14930457 \n",
      "\n",
      "เดี๋ยวเล่าให้ฟัง \n",
      " More are coming! \n",
      " similariry 0.2288554 \n",
      "\n",
      "ความเป็นธรรมไม่มีอีกต่อไปแล้ว \n",
      " All right, they've gone. \n",
      " similariry 0.11197429 \n",
      "\n",
      "หัวหน้าหน่วย \n",
      " First in command. \n",
      " similariry 0.23669486 \n",
      "\n",
      "รองหัวหน้าหน่วย \n",
      " Second in command. \n",
      " similariry 0.2389047 \n",
      "\n",
      "เซ็น.. ไว้ก่อน \n",
      " Borrow it! \n",
      " similariry 0.13314573 \n",
      "\n",
      "เอฟสติน \n",
      " Epstein. \n",
      " similariry 0.22217071 \n",
      "\n",
      "เจ๋ง ช่างมัน เราจะใช้เครดิต เอฟสติน \n",
      " Well, let's be big about it. We'll borrow from Epstein. \n",
      " similariry 0.2326447 \n",
      "\n",
      "ยากที่จะทำสำเร็จ ที่จะกู้เงิน เอฟสติน เขาเป็นยิว \n",
      " It might be difficult in view of our policy towards his people. \n",
      " similariry 0.1358498 \n",
      "\n",
      "ไม่ถึงไหน \n",
      " Very slow. \n",
      " similariry 0.17517488 \n",
      "\n",
      "ตลกสิ้นดี ไม่รู้เมื่อไหร่เขาจะทอดทิ้งเร \n",
      " Funny how they've left us alone. \n",
      " similariry 0.23201106 \n",
      "\n",
      "แล้วผมโกนเคราให้คุณทำไม \n",
      " Isn't that foolish of me? \n",
      " similariry 0.17507213 \n",
      "\n",
      "เมื่อไหร่คุณจะเซ็น \n",
      " I'll get you a pen. \n",
      " similariry 0.20576344 \n",
      "\n",
      "ให้มันขึ้นรถไฟที่นั่งVIP สายด่วนไปนรก \n",
      " Let's train others first, then shoot them! \n",
      " similariry 0.243789 \n",
      "\n",
      "หยวนๆให้พวกเขาหน่อยล่ะกัน \n",
      " Cannot afford to be lenient. \n",
      " similariry 0.18394591 \n",
      "\n",
      "ก็มีผมสีน้ำตาล \n",
      " Of the world! \n",
      " similariry 0.07687859 \n",
      "\n",
      "หรือ ซีซ่า หรือ นุนลูส \n",
      " Aut Caesar aut nullus. \n",
      " similariry 0.22186013 \n",
      "\n",
      "จะเป็น ซีซ่า หรือ นูนลูส \n",
      " Aut Caesar aut nullus. \n",
      " similariry 0.23664221 \n",
      "\n",
      "สนธิสัญญา อยู่ที่ อดีนอย เฮนเคิล เด็กๆแห่งสัญญาดับเบิล-คลอส \n",
      " At six, Adenoid Hynkel will address the children of the double-cross. \n",
      " similariry 0.24789964 \n",
      "\n",
      "ไอ้ยิว เอฟสติน ถุย \n",
      " Epstein refuses, eh? \n",
      " similariry 0.105728246 \n",
      "\n",
      "จับ ชูลท์ซ \n",
      " Place Schultz under arrest. \n",
      " similariry 0.21926305 \n",
      "\n",
      "จำเอาไว้ ทุกสิ่งที่คุณทำจะล้ม \n",
      " Very well, but remember my words. \n",
      " similariry 0.14898932 \n",
      "\n",
      "แกมันขัดต่อคำปฏิญาณ \n",
      " You're a double-dyed democrat! \n",
      " similariry 0.17068467 \n",
      "\n",
      "ก็ใช่ \n",
      " Most amusing. \n",
      " similariry 0.18370244 \n",
      "\n",
      "รูปสะท้อนแสงได้ \n",
      " A fine photo on each! \n",
      " similariry 0.10083852 \n",
      "\n",
      "ผบ \n",
      " Commander Schultz escaped. \n",
      " similariry -0.023867391 \n",
      "\n",
      "โดยผู้นั้นได้จากการจับฉลาก \n",
      " At a feast by lottery the victim was chosen. \n",
      " similariry 0.17174453 \n",
      "\n",
      "แล้ว ผบ. ล่ะ \n",
      " Where's the Commander? \n",
      " similariry 0.16940914 \n",
      "\n",
      "ถุงกอร์ฟผม \n",
      " My golf clubs! \n",
      " similariry 0.22009242 \n",
      "\n",
      "ทำเวลา \n",
      " Take this. \n",
      " similariry 0.111934125 \n",
      "\n",
      "พวกแกอยู่ไหน \n",
      " Here's your friend. \n",
      " similariry 0.14126343 \n",
      "\n",
      "ออสตินลิค \n",
      " Osterlich! \n",
      " similariry 0.12035279 \n",
      "\n",
      "แด่ ออสตินลิค \n",
      " To the invasion of Osterlich! \n",
      " similariry 0.14076248 \n",
      "\n",
      "เพื่อบุก ออสตินลิค \n",
      " To take Osterlich! \n",
      " similariry 0.1302158 \n",
      "\n",
      "แกแหกตาฉัน \n",
      " You can't believe it! \n",
      " similariry 0.15428653 \n",
      "\n",
      "ไหนว่ามีแค่เรา \n",
      " You let him steal a march on us. \n",
      " similariry 0.107251815 \n",
      "\n",
      "ท่านจะทำยังไงต่อไป \n",
      " I had the ground covered. \n",
      " similariry -0.023339227 \n",
      "\n",
      "หุบปาก \n",
      " Shut up! \n",
      " similariry 0.20029831 \n",
      "\n",
      "ไม่ครับ ไข้หวัดลงคอ ครับ \n",
      " No, I mean he can't talk. \n",
      " similariry 0.1757024 \n",
      "\n",
      "ปูพรม เร็ว \n",
      " Lay it down here. Quick. \n",
      " similariry 0.16425815 \n",
      "\n",
      "หุบปาก \n",
      " Shut up! \n",
      " similariry 0.20029831 \n",
      "\n",
      "ไงสหาย \n",
      " My friend! \n",
      " similariry 0.20557308 \n",
      "\n",
      "เยี่ยมมากๆ เชิญ \n",
      " Very well, this way. \n",
      " similariry 0.17519346 \n",
      "\n",
      "ที่ลานสวนสนาม และผู้ชมเกือบครึ่งล้าน \n",
      " The Hynkel stadium. \n",
      " similariry 0.18967749 \n",
      "\n",
      "การสวนสนามที่ยิ่งใหญ่ของกองทัพ \n",
      " Before half a million spectators the greatest ever display of arms marches by in review. \n",
      " similariry 0.186106 \n",
      "\n",
      "กระจอก \n",
      " Very light! \n",
      " similariry 0.23370549 \n",
      "\n",
      "การ์บริทซ เราจะบุก ออสตินลิค \n",
      " Garbitsch! The invasion of Osterlich. \n",
      " similariry 0.24683613 \n",
      "\n",
      "มันเป็นข้ออ้างที่ดี \n",
      " It'll carry weight. \n",
      " similariry 0.11944296 \n",
      "\n",
      "อดีนอย ที่รัก \n",
      " My dear Adenoid. \n",
      " similariry 0.1933999 \n",
      "\n",
      "เซ็นบุ๊บ ไปบั๊บ \n",
      " Then I remove my troops after. \n",
      " similariry 0.06061632 \n",
      "\n",
      "เพื่อให้แน่ใจว่าคุณจะไม่โกง \n",
      " You said I remove first. \n",
      " similariry 0.17113243 \n",
      "\n",
      "เรื่องของผม \n",
      " Why should I? \n",
      " similariry 0.20529667 \n",
      "\n",
      "ไม่ไม่ส่งผลดีต่อพวกคุณเลย \n",
      " This won't get us anywhere. \n",
      " similariry 0.24452034 \n",
      "\n",
      "ฉันดิ้นเหมือนหนอนโดนน้ำร้อน \n",
      " My little bambino! \n",
      " similariry 0.13750423 \n",
      "\n",
      "แผนบุก ออสตินลิค \n",
      " The invasion of Osterlich. \n",
      " similariry 0.2240307 \n",
      "\n",
      "เราจะผ่านมันเหรอ \n",
      " Through the woods? \n",
      " similariry 0.21632132 \n",
      "\n",
      "ทำอย่างไรดี \n",
      " Looking this way. \n",
      " similariry 0.1560964 \n",
      "\n",
      "ใส่เกียร์อะไรดี \n",
      " Just a little bit? \n",
      " similariry 0.20079279 \n",
      "\n",
      "เราใส่เกียร์ 2 เลยดีไหม \n",
      " We could walk faster. \n",
      " similariry 0.22148132 \n",
      "\n",
      "ใจเย็นๆ \n",
      " There's no hurry. \n",
      " similariry 0.15929574 \n",
      "\n",
      "ให้สัญญาน \n",
      " Sound assembly! \n",
      " similariry 0.17245917 \n",
      "\n",
      "แถว... ตรง \n",
      " Attention! \n",
      " similariry 0.096427694 \n",
      "\n",
      "วันทยา... .. วุธ \n",
      " Present arms! \n",
      " similariry 0.11834729 \n",
      "\n",
      "เสือกับกวางอยู่ถ้ำเดียวกันไม่ได้ ในเมื่อเราเป็นเสือ \n",
      " They are inferior and therefore enemies of the state. \n",
      " similariry 0.091896534 \n",
      "\n",
      "เอ่อโอ้เอ่อ \n",
      " Uh, oh, uh, Cricket's the name. \n",
      " similariry 0.17634195 \n",
      "\n",
      "อ๋อ ติดตลาด \n",
      " Temptations. \n",
      " similariry 0.13587612 \n",
      "\n",
      "ใช่ \n",
      " But hurry now. \n",
      " similariry 0.18923685 \n",
      "\n",
      "ถ้าเราเล่นไพ่ของเราได้เรา จะ \n",
      " Listen. \n",
      " similariry 0.011654927 \n",
      "\n",
      "ที่คุณจะไปไหน \n",
      " An actor's life is fun \n",
      " similariry -0.010430727 \n",
      "\n",
      "จากร้านค้าที่ดีที่สุดเฮ้ ปีโนก \n",
      " Hey, Pinoke. \n",
      " similariry 0.048918217 \n",
      "\n",
      "ลาก่อน? ฮะ? \n",
      " Hi diddle dee dee An actor's life for me \n",
      " similariry 0.05304729 \n",
      "\n",
      "ฉันจะทำงานและบอก พ่อ ของ เขา \n",
      " Hi diddle dee day An actor's life is gay \n",
      " similariry 0.11112346 \n",
      "\n",
      "ไม่มีที่ต้องการจะแจ้ง \n",
      " No, that'd be snitching. \n",
      " similariry 0.1899296 \n",
      "\n",
      "แอดเดรสของคุณคืออูลาลา \n",
      " Your savoir faire is ooo-la-la \n",
      " similariry 0.20960167 \n",
      "\n",
      "\n",
      "count: 206158\n",
      "4788.720229148865\n"
     ]
    }
   ],
   "source": [
    "c = Counter()\n",
    "t = time()\n",
    "threashold = 0.25\n",
    "step = 10000\n",
    "\n",
    "counter = 0\n",
    "for n in tqdm_notebook(range(0, len(cleaned_and_filtered_th), step)):\n",
    "    similarities = compute_similarity_list(cleaned_and_filtered_th[n:n + step], cleaned_and_filtered_en[n:n + step])\n",
    "\n",
    "    for i, sent in enumerate(cleaned_and_filtered_th[n:n+step]):\n",
    "        src_sent, tgt_sebt = cleaned_and_filtered_th[i], cleaned_and_filtered_en[i]\n",
    "\n",
    "        similariry = similarities[i]\n",
    "        if similariry < threashold:\n",
    "            counter += 1\n",
    "            \n",
    "print('count: {}'.format(counter))\n",
    "print('{}'.format(time() - t ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/opensubtitle_v2018/sent.cleaned.v1.th', 'w') as f:\n",
    "    for sent in cleaned_and_filtered_th:\n",
    "        f.write(sent + '\\n')\n",
    "        \n",
    "with open('../data/opensubtitle_v2018/sent.cleaned.v1.en', 'w') as f:\n",
    "    for sent in cleaned_and_filtered_en:\n",
    "        f.write(sent + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentence\n",
    "N = len(cleaned_and_filtered_th)\n",
    "\n",
    "print('N = ',N)\n",
    "idx = list(range(N))\n",
    "\n",
    "random.seed(1234) # Set SEED\n",
    "random.shuffle(idx)\n",
    "\n",
    "train_idx, test_idx = idx[:int(n*0.8)], idx[int(n*0.9):]\n",
    "\n",
    "dataset_split = {}\n",
    "cleaned_and_filtered_th_train = [cleaned_and_filtered_th[i] for i in train_idx]\n",
    "cleaned_and_filtered_th_test = [cleaned_and_filtered_th[i] for i in test_idx]\n",
    "cleaned_and_filtered_en_train = [cleaned_and_filtered_en[i] for i in train_idx]\n",
    "cleaned_and_filtered_en_test =[cleaned_and_filtered_en[i] for i in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/opensubtitle_v2018/sent.cleaned.v1.train.th', 'w') as f:\n",
    "    for sent in cleaned_and_filtered_th_train:\n",
    "        f.write(sent + '\\n')\n",
    "with open('../data/opensubtitle_v2018/sent.cleaned.v1.test.th', 'w') as f:\n",
    "    for sent in cleaned_and_filtered_th_test:\n",
    "        f.write(sent + '\\n')     \n",
    "with open('../data/opensubtitle_v2018/sent.cleaned.v1.train.en', 'w') as f:\n",
    "    for sent in cleaned_and_filtered_en_train:\n",
    "        f.write(sent + '\\n')\n",
    "with open('../data/opensubtitle_v2018/sent.cleaned.v1.test.en', 'w') as f:\n",
    "    for sent in cleaned_and_filtered_en_test:\n",
    "        f.write(sent + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Text Segmentation (word, subword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for newmm tokenization\n",
    "\n",
    "def tokenize_worker(sentence, lang, trie):\n",
    "    \n",
    "    _tokenizer_newmm = partial(pythainlp.tokenize.word_tokenize, engine='newmm',\n",
    "                               keep_whitespace=False,\n",
    "                              custom_dict=(trie if trie != None else DEFAULT_DICT_TRIE))\n",
    "    return ' '.join(_tokenizer_newmm(sentence))\n",
    "  \n",
    "def tokenize_handler(sentences, lang, trie=None):\n",
    "    toks = []\n",
    "    p = Pool(6)\n",
    "    t = time()\n",
    "    _tokenize_worker = partial(tokenize_worker, lang=lang, trie=trie)\n",
    "    toks = p.map(_tokenize_worker, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    print('{} s'.format(time() -t))\n",
    "\n",
    "    return toks\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_spaced_tokens_to_file(data, folder_name, filename):\n",
    "    with open('/root/mt-opus/data/{}/{}'.format(folder_name, filename),'w') as f:\n",
    "        for item in data:\n",
    "            f.write(item + '\\n')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = {\n",
    "    'th': {\n",
    "        'sentencepiece': [],\n",
    "        'newmm':[]\n",
    "    },\n",
    "    'en': {\n",
    "        'sentencepiece': [],\n",
    "        'newmm':[]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Segment text into words with a dictionary-based tokenizer (`newmm`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.45962905883789 s\n",
      "63.490153074264526 s\n"
     ]
    }
   ],
   "source": [
    "# Thai\n",
    "toks['th']['newmm'] = tokenize_handler(filtered_th, lang='th')\n",
    "# English\n",
    "toks['en']['newmm'] = tokenize_handler(filtered_en, lang='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ทาส ใน กระจก วิเศษ , มาจาก พื้นที่ ที่ ไกล ที่สุด',\n",
       "  'ผ่าน ลม และ ความมืด ฉัน เรียก เจ้า',\n",
       "  'พูด !',\n",
       "  'ให้ ฉัน เห็น พระพักตร์ ของ พระองค์',\n",
       "  'สิ่ง ที่ เจ้า จะ รู้ ว่า สมเด็จ พระราชินี ของ ฉัน ได้ อย่างไร',\n",
       "  'กระจก วิเศษ บน ผนัง ผู้ ที่ เป็น สังขาร หนึ่ง ทั้งหมด หรือไม่',\n",
       "  'ที่ มีชื่อเสียง เป็น ความงาม ของ เจ้า พระ บาท สมเด็จ พระเจ้าอยู่หัว',\n",
       "  'แต่ ถือเป็น แม่บ้าน ที่ น่ารัก ที่ ฉัน เห็น',\n",
       "  'ยาจก ไม่ สามารถ ซ่อน พระคุณ อ่อนโยน ของ เธอ',\n",
       "  'อนิจจา เธอ มี ความเป็นธรรม มากขึ้น กว่า เจ้า'],\n",
       " ['Slave in the Magic Mirror , come from the farthest space .',\n",
       "  'Through wind and darkness , I summon thee .',\n",
       "  'Speak !',\n",
       "  'Let me see thy face .',\n",
       "  'What wouldst thou know , my Queen ?',\n",
       "  'Magic Mirror on the wall , who is the fairest one of all ?',\n",
       "  'Famed is thy beauty , Majesty .',\n",
       "  'But hold , a lovely maid I see .',\n",
       "  'Rags cannot hide her gentle grace .',\n",
       "  'Alas , she is more fair than thee .'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks['th']['newmm'][0:10], toks['en']['newmm'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Segment text into subword (ie. BPE tokens) with Pretrained SentencePiece (`BPEmb`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bpe(sentences, lang, n_vocab=25000):\n",
    "    \"\"\"Return a list of bpe tokens give a list of sentences\"\"\"\n",
    "    segmented_sentences = []\n",
    "    for sentence in tqdm_notebook(sentences, total=len(sentences)):\n",
    "#         print(sentence)\n",
    "        bpe_tokens = bpemb_pretrained[lang]['{}'.format(n_vocab)].encode(sentence)\n",
    "        segmented_sentences.append(' '.join(bpe_tokens))\n",
    "        \n",
    "    return segmented_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Thai language__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce4eb5e8cbe469886307235a01f1dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3202751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['▁ท าส ใน กระจก วิเศษ , ▁มาจาก พื้นที่ ที่ ไกล ที่สุด', '▁ผ่าน ลม และความ มืด ฉัน เรียก เจ้า', '▁พูด !', '▁ให้ ฉัน เห็น พระพักตร์ ของ ▁พระองค์', '▁สิ่งที่ เจ้า จะ รู้ว่า สมเด็จพระราชินี ▁ของ ฉัน ได้อย่างไร', '▁กระจ ก วิเศษ บน ผนัง ▁ผู้ ที่เป็น สัง ขาร หนึ่ง ทั้งหมด ▁หรือไม่', '▁ที่มีชื่อเสียง เป็น ความงาม ของ ▁เจ้า พระบาทสมเด็จพระ เจ้าอยู่หัว', '▁แต่ ถือเป็น แม่ บ้าน ที่น ่ารัก ที่ ฉัน ▁เห็น', '▁ยา จก ไม่สามารถ ซ่อน พระคุณ ▁อ่อน โยน ของเธอ', '▁อน ิจ จา เธอ มีความเป็น ธรรม ▁มาก ขึ้น กว่า เจ้า']\n"
     ]
    }
   ],
   "source": [
    "toks['th']['sentencepiece'] = encode_bpe(filtered_th, 'th', 25000)\n",
    "\n",
    "print(toks['th']['sentencepiece'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__English language__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0ac2a9e16c4fe0814a82c713b07a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3202751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['▁slave ▁in ▁the ▁magic ▁mirror , ▁come ▁from ▁the ▁fart hest ▁space .', '▁through ▁wind ▁and ▁darkness , ▁i ▁summon ▁the e .', '▁speak !', '▁let ▁me ▁see ▁thy ▁face .', '▁what ▁would st ▁thou ▁know , ▁my ▁queen ?', '▁magic ▁mirror ▁on ▁the ▁wall , ▁who ▁is ▁the ▁fa ire st ▁one ▁of ▁all ?', '▁famed ▁is ▁thy ▁beauty , ▁majesty .', '▁but ▁hold , ▁a ▁lov ely ▁maid ▁i ▁see .', '▁ra gs ▁cannot ▁hide ▁her ▁gentle ▁grace .', '▁al as , ▁she ▁is ▁more ▁fair ▁than ▁the e .']\n"
     ]
    }
   ],
   "source": [
    "toks['en']['sentencepiece']  = encode_bpe(filtered_en, 'en', 25000)\n",
    "print(toks['en']['sentencepiece'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) Segment text into subword (ie. BPE tokens) with SentencePiece model (unigram) trained from Opensubtitles_v2018 dataset\n",
    "\n",
    "### Build SentencePiece BPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: --input must not be empty\n",
      "\n",
      "sentencepiece\n",
      "\n",
      "Usage: sentencepiece [options] files\n",
      "\n",
      "   --accept_language (comma-separated list of languages this model can accept)  type: string  default: \n",
      "   --add_dummy_prefix (Add dummy whitespace at the beginning of text)  type: bool  default: true\n",
      "   --bos_id (Override BOS (<s>) id. Set -1 to disable BOS.)  type: int32  default: 1\n",
      "   --bos_piece (Override BOS (<s>) piece.)  type: string  default: <s>\n",
      "   --character_coverage (character coverage to determine the minimum symbols)  type: double  default: 0.9995\n",
      "   --control_symbols (comma separated list of control symbols)  type: string  default: \n",
      "   --eos_id (Override EOS (</s>) id. Set -1 to disable EOS.)  type: int32  default: 2\n",
      "   --eos_piece (Override EOS (</s>) piece.)  type: string  default: </s>\n",
      "   --hard_vocab_limit (If set to false, --vocab_size is considered as a soft limit.)  type: bool  default: true\n",
      "   --input (comma separated list of input sentences)  type: string  default: \n",
      "   --input_format (Input format. Supported format is `text` or `tsv`.)  type: string  default: \n",
      "   --input_sentence_size (maximum size of sentences the trainer loads)  type: int32  default: 0\n",
      "   --max_sentence_length (maximum length of sentence in byte)  type: int32  default: 4192\n",
      "   --max_sentencepiece_length (maximum length of sentence piece)  type: int32  default: 16\n",
      "   --model_prefix (output model prefix)  type: string  default: \n",
      "   --model_type (model algorithm: unigram, bpe, word or char)  type: string  default: unigram\n",
      "   --normalization_rule_name (Normalization rule name. Choose from nfkc or identity)  type: string  default: nmt_nfkc\n",
      "   --normalization_rule_tsv (Normalization rule TSV file. )  type: string  default: \n",
      "   --num_sub_iterations (number of EM sub-iterations)  type: int32  default: 2\n",
      "   --num_threads (number of threads for training)  type: int32  default: 16\n",
      "   --pad_id (Override PAD (<pad>) id. Set -1 to disable PAD.)  type: int32  default: -1\n",
      "   --pad_piece (Override PAD (<pad>) piece.)  type: string  default: <pad>\n",
      "   --remove_extra_whitespaces (Removes leading, trailing, and duplicate internal whitespace)  type: bool  default: true\n",
      "   --seed_sentencepiece_size (the size of seed sentencepieces)  type: int32  default: 1000000\n",
      "   --self_test_sample_size (the size of self test samples)  type: int32  default: 0\n",
      "   --shrinking_factor (Keeps top shrinking_factor pieces with respect to the loss)  type: double  default: 0.75\n",
      "   --shuffle_input_sentence (Randomly sample input sentences in advance. Valid when --input_sentence_size > 0)  type: bool  default: true\n",
      "   --split_by_number (split tokens by numbers (0-9))  type: bool  default: true\n",
      "   --split_by_unicode_script (use Unicode script to split sentence pieces)  type: bool  default: true\n",
      "   --split_by_whitespace (use a white space to split sentence pieces)  type: bool  default: true\n",
      "   --treat_whitespace_as_suffix (treat whitespace marker as suffix instead of prefix.)  type: bool  default: false\n",
      "   --unk_id (Override UNK (<unk>) id.)  type: int32  default: 0\n",
      "   --unk_piece (Override UNK (<unk>) piece.)  type: string  default: <unk>\n",
      "   --unk_surface (Dummy surface string for <unk>. In decoding <unk> is decoded to `unk_surface`.)  type: string  default:  ⁇ \n",
      "   --use_all_vocab (If set to true, use all tokens as vocab. Valid for word/char models.)  type: bool  default: false\n",
      "   --user_defined_symbols (comma separated list of user defined symbols)  type: string  default: \n",
      "   --vocab_size (vocabulary size)  type: int32  default: 8000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_file = '../data/opensubtitle_v2018/sent.cleaned.v1.train'\n",
    "vocab_file = '../data/opensubtitle_v2018/opensubtitles.vocab'\n",
    "!cat {train_file}.th {train_file}.en | shuffle > train\n",
    "!spm_train --input=train --model_prefix=spm --vocab_size=8000 --character_coverage=0.9995 --normalization_rule_name identity\n",
    "!spm_encode --model=spm.model --generate_vocabulary < {train_file}.L1 > {vocab_file}.th\n",
    "!spm_encode --model=spm.model --generate_vocabulary < {train_file}.L2 > {vocab_file}.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Split train-valid-test \n",
    "\n",
    "- split: train/valid/test (80/10/10)\n",
    "\n",
    "- seed for rando,.shuffle: 1234\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N =  3202751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2562200, 320275, 320276)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train-valid-test split 80/10/10\n",
    "\n",
    "n = len(toks['th']['newmm'])\n",
    "\n",
    "print('N = ',n)\n",
    "idx = list(range(n))\n",
    "\n",
    "random.seed(1234) # Set SEED\n",
    "random.shuffle(idx)\n",
    "\n",
    "train_idx, valid_idx, test_idx = idx[:int(n*0.8)], idx[int(n*0.8):int(n*0.9)], idx[int(n*0.9):]\n",
    "\n",
    "dataset_split = {}\n",
    "dataset_split['train'] = train_idx\n",
    "dataset_split['valid'] = valid_idx\n",
    "dataset_split['test'] = test_idx\n",
    "\n",
    "\n",
    "len(train_idx),len(valid_idx),len(test_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    },\n",
    "    'valid': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    },\n",
    "    'test': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    for lang in ['th', 'en']:\n",
    "        for tok_type in ['sentencepiece', 'newmm']:\n",
    "\n",
    "            dataset[split_name][lang][tok_type] = [toks[lang][tok_type][i] for i in dataset_split[split_name]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['เบค กี้ เธอ ทำท่า แปลก ๆ เมื่อกี้ ใน ห้อง', 'อยู่ กับ เธอ แอน นา จะ นำทาง คุณ ผม จะ กลับ ไป'] \n",
      "\n",
      "['Becky , um , you were acting particularly strange in there just now .', \"Stay with her so Anna can guide you . I ' m going back .\"] \n",
      "\n",
      "['▁เบ ค กี้ ▁เธอ ทํา ท่า แปลก ๆ ▁เมื่อ กี้ ▁ในห้อง', '▁ อยู่กับ เธอ ▁แอนนา จะนํา ทาง คุณ ▁ผม จะ กลับไป'] \n",
      "\n",
      "['▁bec ky , ▁um , ▁you ▁were ▁acting ▁particularly ▁strange ▁in ▁there ▁just ▁now .', \"▁stay ▁with ▁her ▁so ▁anna ▁can ▁guide ▁you . ▁i ' m ▁going ▁back .\"] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['th']['newmm'][0:2],'\\n')\n",
    "print(dataset['train']['en']['newmm'][0:2],'\\n')\n",
    "print(dataset['train']['th']['sentencepiece'][0:2],'\\n')\n",
    "print(dataset['train']['en']['sentencepiece'][0:2],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'en_train_n_toks': 92383739, 'th_train_n_toks': 86683223, 'en_valid_n_toks': 11536351, 'en_test_n_toks': 11535798, 'th_test_n_toks': 10833242, 'th_valid_n_toks': 10826042})\n"
     ]
    }
   ],
   "source": [
    "# Counting number of tokens for train, valid, test\n",
    "counter = Counter( )\n",
    "for dataset_type in ['train', 'valid', 'test']:\n",
    "    for th_sent_toks in dataset[dataset_type]['th']['newmm']:\n",
    "        counter['th_{}_n_toks'.format(dataset_type)] += len(th_sent_toks)\n",
    "    for en_sent_toks in dataset[dataset_type]['en']['newmm']:\n",
    "        counter['en_{}_n_toks'.format(dataset_type)] += len(en_sent_toks)\n",
    "\n",
    "print(counter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write segmented text to files__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-sentencepiece/th-en\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-sentencepiece/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-sentencepiece/en-th\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-sentencepiece/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-newmm/th-en\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-newmm/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-newmm/en-th\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-newmm/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-sentencepiece/th-en\n",
      "dir: ../data/opensubtitles_bin/newmm-sentencepiece/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-sentencepiece/en-th\n",
      "dir: ../data/opensubtitles_bin/newmm-sentencepiece/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-newmm/th-en\n",
      "dir: ../data/opensubtitles_bin/newmm-newmm/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-newmm/en-th\n",
      "dir: ../data/opensubtitles_bin/newmm-newmm/en-th\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tok_type_src in ['sentencepiece', 'newmm']:\n",
    "    for tok_type_tgt in ['sentencepiece', 'newmm']:\n",
    "        langs = ['th', 'en']\n",
    "        for lang in langs:\n",
    "            src_lang = lang\n",
    "            tgt_lang = 'en' if lang =='th' else 'th'\n",
    "            FOLDER_NAME = \"opensubtitles_tok/{}-{}/{}-{}\".format(tok_type_src, tok_type_tgt, src_lang, tgt_lang )\n",
    "            FOLDER_NAME_BIN = \"opensubtitles_bin/{}-{}/{}-{}\".format(tok_type_src, tok_type_tgt, src_lang, tgt_lang)\n",
    "           \n",
    "            \n",
    "            # Create directories\n",
    "            print('create directories: ')\n",
    "            print('dir: ../data/{}'.format(FOLDER_NAME))\n",
    "            print('dir: ../data/{}'.format(FOLDER_NAME_BIN))\n",
    "\n",
    "            !mkdir -p ../data/{FOLDER_NAME}\n",
    "            !mkdir -p ../data/{FOLDER_NAME_BIN}\n",
    "\n",
    "            for split_name in ['train', 'valid', 'test']:\n",
    "                \n",
    "                write_spaced_tokens_to_file(dataset[split_name][src_lang][tok_type_src],\n",
    "                                            FOLDER_NAME, '{}.{}'.format(split_name, src_lang))\n",
    "                \n",
    "                write_spaced_tokens_to_file(dataset[split_name][tgt_lang][tok_type_tgt],\n",
    "                                            FOLDER_NAME, '{}.{}'.format(split_name, tgt_lang))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁bec ky , ▁um , ▁you ▁were ▁acting ▁particularly ▁strange ▁in ▁there ▁just ▁now .\n",
      "▁stay ▁with ▁her ▁so ▁anna ▁can ▁guide ▁you . ▁i ' m ▁going ▁back .\n",
      "▁look .\n",
      "▁oh , ▁no , ▁it ' s ▁the ▁other ▁way ▁around , ▁dr . ▁lewis .\n",
      "▁sort ▁of .\n",
      "▁bart ender , ▁something ▁really ▁strong , ▁please .\n",
      "▁yes , ▁obviously .\n",
      "▁la ' s ▁so ▁nice .\n",
      "▁i ' m ▁going ▁to ▁fix ▁it .\n",
      "▁i ▁get ▁b ored .\n"
     ]
    }
   ],
   "source": [
    "!head ../data/opensubtitles_tok/newmm-sentencepiece/th-en/train.en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "เบค กี้ เธอ ทำท่า แปลก ๆ เมื่อกี้ ใน ห้อง\n",
      "อยู่ กับ เธอ แอน นา จะ นำทาง คุณ ผม จะ กลับ ไป\n",
      "ฟัง นะ\n",
      "พอดี เลย ดร. ลี วิ ส\n",
      "แบบ ว่า\n",
      "เอ่อ บาร์ เท็น เด อร ์ ขอ อะไร ที่\n",
      "ก็ ใช่ ห น่ะ สิ\n",
      "แอลเอ สวย เนอะ\n",
      "ฉัน กำลังจะ แก้ ไขมัน\n",
      "ฉัน เบื่อ ละ\n"
     ]
    }
   ],
   "source": [
    "!head ../data/opensubtitles_tok/newmm-sentencepiece/th-en/train.th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
