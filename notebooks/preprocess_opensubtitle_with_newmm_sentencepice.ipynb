{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# import sys\n",
    "# sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import copy \n",
    "import re\n",
    "import html\n",
    "import numpy as np\n",
    "\n",
    "from time import time \n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import pythainlp\n",
    "from pythainlp.util import *\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.ulmfit import *\n",
    "\n",
    "# subword-nmt\n",
    "from subword_nmt import learn_bpe as learner\n",
    "from subword_nmt import apply_bpe as subword_tokenizer\n",
    "\n",
    "import fairseq \n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pythainlp.tokenize import DEFAULT_DICT_TRIE\n",
    "\n",
    "from pythainlp.corpus import thai_words\n",
    "\n",
    "print(pythainlp.__version__)\n",
    "# assert pythainlp.__version__ == '2.1'\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import sentencepiece as spm\n",
    "import tf_sentencepiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install BPEmb (BPE embeddings)\n",
    "\n",
    "# !pip install --q bpemb emoji subword-nmt fairseq tensorflow_hub sentencepiece tf_sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# Universal Sentence Encoder\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/1\")\n",
    "    embedded_text = embed(text_input)\n",
    "    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "g.finalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_similarity(src_sent, tgt_sent):\n",
    "    \"\"\"\n",
    "        Calculate sentence similarity based on Google Universal Sentence Encoder (Multilingual Large)\n",
    "    \"\"\"\n",
    "    session = tf.Session(graph=g)\n",
    "    session.run(init_op)\n",
    "    \n",
    "    src_result = session.run(embedded_text, feed_dict={text_input: [src_sent]})\n",
    "    tgt_result = session.run(embedded_text, feed_dict={text_input: [tgt_sent]})\n",
    "    \n",
    "    return np.inner(src_result, tgt_result).reshape(1)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8449457\n",
      "0.64079666\n",
      "0.7397201\n",
      "0.7669394\n"
     ]
    }
   ],
   "source": [
    "print(compute_similarity(\"ฉันชื่อยีน\",\"My Name is Gene\"))\n",
    "print(compute_similarity(\"ฉันชื่อยีน\",\"My Name is Jane\"))\n",
    "print(compute_similarity(\"ฉันชื่อยีน\",\"My Name is Jeans\"))\n",
    "print(compute_similarity(\"ฉันชื่อยีน\",\"My Name is Jean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "\n",
    "bpemb_pretrained ={\n",
    "    'th': {\n",
    "        '25000': BPEmb(lang=\"th\", vs=25000)\n",
    "    },\n",
    "    'en': {\n",
    "        '25000': BPEmb(lang=\"en\", vs=25000)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3281534,\n",
       " ['Slave in the Magic Mirror, come from the farthest space.',\n",
       "  'Through wind and darkness, I summon thee.',\n",
       "  'Speak!'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/opensubtitle_v2018/OpenSubtitles.en-th.en','r', encoding='utf-8') as f:\n",
    "    en = f.read().split('\\n')\n",
    "len(en),en[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3281534,\n",
       " ['ทาสในกระจกวิเศษ, มาจากพื้นที่ที่ไกลที่สุด',\n",
       "  'ผ่านลมและความมืดฉันเรียกเจ้า',\n",
       "  'พูด!'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/opensubtitle_v2018/OpenSubtitles.en-th.th','r', encoding='utf-8') as f:\n",
    "    th = f.read().split('\\n')\n",
    "    \n",
    "len(th),th[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Opensubtitles_v2018\n",
    "\n",
    "\n",
    "## 1.1 Text cleaning, filtering out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt_opus.preprocess import (\n",
    "    SentenceLengthLessThanOrEqualToOne,\n",
    "    SentenceContainsUnknownSymbol,\n",
    "    SentenceContainsAdSymbol,\n",
    "    ThaiSentenceContainsNoThaiCharacters,\n",
    "    ThaiSentenceContainsNoThaiCharactersPattern,\n",
    "    ThaiSentenceContainsUnwantedPattern,\n",
    "    EnglishSentenceContainsUnwantedPattern,\n",
    "    SentenceContainsOnlyAsterisk,\n",
    "    \n",
    "    UnescapeString,\n",
    "    RemoveUnwantedSymbols,\n",
    "    RemoveUnwantedPattern,\n",
    "    ReplaceDashInSentence,\n",
    "    RemoveHashtagInSentence,\n",
    "    RemoveFullStopInThaiSentence,\n",
    "    NormalizeThaiVowel,\n",
    "    ReplaceAsteriskInSentence,\n",
    "    RemoveColonInSentence,\n",
    "    RemoveSemiColonInSentence,\n",
    "    FormatTime,\n",
    "    \n",
    "    SentencePairFoundRepeatedText,\n",
    "    SentencePairTokenLengthsDifferGreaterThreashold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to filter out a sentence pair\n",
    "filtering_rules = [\n",
    "    SentenceLengthLessThanOrEqualToOne(),\n",
    "    SentenceContainsAdSymbol(),\n",
    "    SentenceContainsUnknownSymbol(),\n",
    "    ThaiSentenceContainsNoThaiCharacters(),\n",
    "    ThaiSentenceContainsNoThaiCharactersPattern(),\n",
    "    SentenceContainsOnlyAsterisk(),\n",
    "]\n",
    "\n",
    "cleaning_rules = [\n",
    "    UnescapeString(),\n",
    "    RemoveUnwantedSymbols(),\n",
    "    RemoveUnwantedPattern(),\n",
    "    ReplaceDashInSentence(),\n",
    "    RemoveHashtagInSentence(),\n",
    "    RemoveFullStopInThaiSentence(),\n",
    "    NormalizeThaiVowel(),\n",
    "    ReplaceDashInSentence(),\n",
    "    ReplaceAsteriskInSentence(),\n",
    "    RemoveColonInSentence(),\n",
    "    RemoveSemiColonInSentence(),\n",
    "    FormatTime(),\n",
    "]\n",
    "\n",
    "filtering_sentence_pair_rules = [\n",
    "    SentencePairFoundRepeatedText(),\n",
    "    SentencePairTokenLengthsDifferGreaterThreashold(threshold=0.85),\n",
    "]\n",
    "\n",
    "filtering_rules_post = [\n",
    "    SentenceLengthLessThanOrEqualToOne(),\n",
    "    ThaiSentenceContainsUnwantedPattern(),\n",
    "    EnglishSentenceContainsUnwantedPattern(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could be parallelized\n",
    "def filter_sentence(sentence, lang, rules=filtering_rules):\n",
    "    \"\"\"\n",
    "        Return True if a sentence match filtering pattern\n",
    "    \"\"\"\n",
    "    for rule in rules:\n",
    "        rule_obj = rule\n",
    "        if rule_obj.test(sentence, lang=lang):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filter_sentences(sentences, lang, rules=filtering_rules):\n",
    "    \"\"\"\n",
    "        Returns a list of Boolean value, if such element is True, it means filter that sentence pair.\n",
    "        Otherwise, keep that sentnence pair.\n",
    "    \"\"\"\n",
    "    filtering_indices = []\n",
    "    p = Pool() # use all available cores\n",
    "    t = time()\n",
    "\n",
    "    _filter_sentence = partial(filter_sentence, lang=lang, rules=rules)\n",
    "    filtering_indices = p.map(_filter_sentence, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    filtering_indices_np = np.array(filtering_indices)\n",
    "    number_of_filtered_out = np.sum(filtering_indices_np)\n",
    "    \n",
    "    print('Time taken: {} s'.format(time() -t))\n",
    "    print('# sentences ({}) before filtered out'.format(lang), len(sentences))\n",
    "    print('# sentences ({}) filtered out'.format(lang), number_of_filtered_out)\n",
    "    print('# sentences ({}) after filtered out'.format(lang), len(sentences) - number_of_filtered_out)\n",
    "\n",
    "    return filtering_indices_np, number_of_filtered_out\n",
    "\n",
    "\n",
    "def filter_sentence_pair(sentence_pair, rules=filtering_sentence_pair_rules):\n",
    "    \"\"\"\n",
    "        Return True if a sentence match filtering sentence pair pattern\n",
    "    \"\"\"\n",
    "    for rule in rules:\n",
    "        rule_obj = rule\n",
    "        if rule_obj.test(sentence_pair):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filter_sentence_pairs(sentence_pairs, rules=filtering_sentence_pair_rules):\n",
    "    \"\"\"\n",
    "        Returns a list of Boolean value, if such element is True, it means filter that sentence pair.\n",
    "        Otherwise, keep that sentnence pair.\n",
    "    \"\"\"\n",
    "    filtering_indices = []\n",
    "    p = Pool() # use all available cores\n",
    "    t = time()\n",
    "\n",
    "    _filter_sentence_pair = partial(filter_sentence_pair, rules=rules)\n",
    "    filtering_indices = p.map(_filter_sentence_pair, sentence_pairs)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    filtering_indices_np = np.array(filtering_indices)\n",
    "    number_of_filtered_out = np.sum(filtering_indices_np)\n",
    "    \n",
    "    print('Time taken: {} s'.format(time() -t))\n",
    "    print('# sentences before filtered out', len(sentence_pairs))\n",
    "    print('# sentences filtered out', number_of_filtered_out)\n",
    "    print('# sentences after filtered out', len(sentence_pairs) - number_of_filtered_out)\n",
    "\n",
    "    return filtering_indices_np, number_of_filtered_out\n",
    "\n",
    "\n",
    "def clean_sentence(sentence, lang, rules=cleaning_rules):\n",
    "    for rule in rules: \n",
    "        rule_obj = rule\n",
    "        if rule_obj.test(sentence, lang=lang):\n",
    "            sentence = rule_obj.replace(sentence, lang=lang)\n",
    "    return sentence\n",
    "\n",
    "def clean_sentences(sentences, lang, rules=cleaning_rules):\n",
    "    \"\"\"\n",
    "        Clean the sentence with the specified text cleaning rules\n",
    "        Return a list of cleaned sentences\n",
    "    \"\"\"\n",
    "    p = Pool() # use all available cores\n",
    "    t = time()\n",
    "    \n",
    "    _clean_sentence = partial(clean_sentence, lang=lang, rules=rules)\n",
    "    cleaned_sentences = p.map(_clean_sentence, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    print('Time taken: {} s'.format(time() -t))\n",
    "    \n",
    "    return cleaned_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Filter by each language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 21.084517002105713 s\n",
      "# sentences (th) before filtered out 3281534\n",
      "# sentences (th) filtered out 77301\n",
      "# sentences (th) after filtered out 3204233\n"
     ]
    }
   ],
   "source": [
    "filtering_indices_th, _ = filter_sentences(th, lang='th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 5.643071889877319 s\n",
      "# sentences (en) before filtered out 3281534\n",
      "# sentences (en) filtered out 4577\n",
      "# sentences (en) after filtered out 3276957\n"
     ]
    }
   ],
   "source": [
    "filtering_indices_en, _ = filter_sentences(en, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# filtering_indices_th_en 79608\n"
     ]
    }
   ],
   "source": [
    "filtering_indices_th_en = filtering_indices_th | filtering_indices_en\n",
    "\n",
    "print('# filtering_indices_th_en', sum(filtering_indices_th_en))\n",
    "\n",
    "filtered_th = [th[i] for i, filtered_out in enumerate(filtering_indices_th_en) if not filtered_out]\n",
    "filtered_en = [en[i] for i, filtered_out in enumerate(filtering_indices_th_en) if not filtered_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# filtered_th 3201926\n",
      "# filtered_en 3201926\n"
     ]
    }
   ],
   "source": [
    "print('# filtered_th', len(filtered_th))\n",
    "print('# filtered_en', len(filtered_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3202604"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3281534 - 78930"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Filter by pair of source and target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 126.75016498565674 s\n",
      "# sentences before filtered out 3201926\n",
      "# sentences filtered out 6985\n",
      "# sentences after filtered out 3194941\n"
     ]
    }
   ],
   "source": [
    "\n",
    "th_en_tuples = [(filtered_th[i], filtered_en[i]) for i in range(0, len(filtered_th))]\n",
    "filtering_pairs_indices_th_en, _ = filter_sentence_pairs(th_en_tuples)\n",
    "\n",
    "filtered_pairs_th = [filtered_th[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if not filtered_out]\n",
    "filtered_pairs_en = [filtered_en[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if not filtered_out]\n",
    "sentence_paris_filted_out = [th_en_tuples[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if filtered_out]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# th_en_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# filtered_pairs_th 3194941\n",
      "# filtered_pairs_en 3194941\n"
     ]
    }
   ],
   "source": [
    "print('# filtered_pairs_th', len(filtered_pairs_th))\n",
    "print('# filtered_pairs_en', len(filtered_pairs_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in filtered_pairs_en:\n",
    "    if len(sent) <= 1:\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 97.21989512443542 s\n"
     ]
    }
   ],
   "source": [
    "cleaned_th = clean_sentences(filtered_pairs_th, lang=\"th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 63.437785148620605 s\n"
     ]
    }
   ],
   "source": [
    "cleaned_en = clean_sentences(filtered_pairs_en, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_th 3194941\n"
     ]
    }
   ],
   "source": [
    "print('cleaned_th', len(cleaned_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore cleaned sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = Counter()\n",
    "# for i, sent in enumerate(cleaned_th):\n",
    "#     if len(sent) <= 1:\n",
    "#         c['found'] += 1\n",
    "#         if c['found'] <= 15:\n",
    "#             print(sent,'<>', cleaned_en[i])\n",
    "#             print()\n",
    "# c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Post-filtering out rule\n",
    "\n",
    "- redo the filtering rules for 1) each langauge individually and 2) a language pair again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 4.486587047576904 s\n",
      "# sentences (th) before filtered out 3194941\n",
      "# sentences (th) filtered out 111\n",
      "# sentences (th) after filtered out 3194830\n",
      "Time taken: 3.4240500926971436 s\n",
      "# sentences (en) before filtered out 3194941\n",
      "# sentences (en) filtered out 277\n",
      "# sentences (en) after filtered out 3194664\n"
     ]
    }
   ],
   "source": [
    "filtering_post_indices_th, _ = filter_sentences(cleaned_th, lang='th', rules=filtering_rules_post)\n",
    "filtering_post_indices_en, _ = filter_sentences(cleaned_en, lang='en', rules=filtering_rules_post)\n",
    "\n",
    "filtering_post_indices_th_en = filtering_post_indices_th | filtering_post_indices_en\n",
    "\n",
    "\n",
    "cleaned_and_filtered_th = [cleaned_th[i] for i, filtered_out in enumerate(filtering_post_indices_th_en) if not filtered_out]\n",
    "cleaned_and_filtered_en = [cleaned_en[i] for i, filtered_out in enumerate(filtering_post_indices_th_en) if not filtered_out]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 135.33339309692383 s\n",
      "# sentences before filtered out 3194570\n",
      "# sentences filtered out 1200\n",
      "# sentences after filtered out 3193370\n"
     ]
    }
   ],
   "source": [
    "th_en_tuples = [(cleaned_and_filtered_th[i], cleaned_and_filtered_en[i]) for i in range(0, len(cleaned_and_filtered_th))]\n",
    "filtering_pairs_indices_th_en, _ = filter_sentence_pairs(th_en_tuples)\n",
    "\n",
    "cleaned_and_filtered_th = [cleaned_and_filtered_th[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if not filtered_out]\n",
    "cleaned_and_filtered_en = [cleaned_and_filtered_en[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if not filtered_out]\n",
    "sentence_paris_filted_out = [th_en_tuples[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if filtered_out]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After 4 steps\n",
    "\n",
    "# number of sentence pairs is 3,201,120\n",
    "# number of sentence pairs is 3,200,973 (Add SentenceContainsAsterink rule)\n",
    "# number of sentence pairs is 3,194,664(Add SentencePairTokenLengthsDifferGreaterThreashold rule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "และจากที่ฉันได้ยินในอุโมงค์--/เธอบอกว่าเธอไม่ได้เห็นอะไรหนิ\n",
      "And from what I heard in that tunnel -You said you didn't see anything.\n",
      "\n",
      "เบื้องต้นนะ--22 คดี\n",
      "first count I have 22.\n",
      "\n",
      "อ่า ปี 1985--2 ครั้ง\n",
      "Uh,1985 for 2 years. Yeah.\n",
      "\n",
      "ให้ฉันเดา--12ปี กับอีก 9 เดือนใช่มั๊ย\n",
      "Let me guess-12 years and 9 months ago?\n",
      "\n",
      "คุณนับ--1 2 3\n",
      "Well, uh, you count one, two, three.\n",
      "\n",
      "ตอนที่ผมสร้างเดอะแมชชีนขึ้นมาครั้งแรก ผมเห็นหมายเลขเดิมๆ ปรากฏขึ้นมาตลอด--1 สัปดาห์ 1 เดือน 6 เดือนถัดมา\n",
      "When I was first building the machine, I kept seeing the same numbers come up a week, a month, six months apart.\n",
      "\n",
      "1--ยกเลิกโทษประหาร\n",
      "One no death penalty.\n",
      "\n",
      "2--ผมอยากย้ายไปอยู่ฝั่งตะวันออก\n",
      "2--I want a transfer to the East Coast.\n",
      "\n",
      "อาห์,โบนส์ยิงไกลเข้าเป้า--3คะแนนไปเลย\n",
      "Ah, Bones shoots from the outside three points.\n",
      "\n",
      "คอนราด เกรย์สัน เกี่ยวข้องกับ M.V.A ความดันโลหิต--170/100, ชีพจร--120 บาดเจ็บที่ศีรษะ ควรใช้เครื่องตรวจศีรษะ\n",
      "Conrad Grayson involved in M.V.A., blood pressure--170/100, pulse--120, possible head trauma.\n",
      "\n",
      "Counter({'count': 10})\n"
     ]
    }
   ],
   "source": [
    "c = Counter()\n",
    "for i, sent in enumerate(cleaned_and_filtered_th):\n",
    "\n",
    "    if re.search(r\"--\", sent):\n",
    "        c['count'] += 1\n",
    "        if c['count'] <= 20:\n",
    "            print(cleaned_and_filtered_th[i])\n",
    "            print(cleaned_and_filtered_en[i])\n",
    "            print('')\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ส', 'ทร', 'อม', 'โบ', 'ลี', 'นักแสดง', 'โท', 'ที่', 'ฉัน', 'และ', 'โดย', 'การอนุญาต', 'เป็นพิเศษ', 'ใน', 'การบริหาร', 'จัดการ', 'ที่', 'ฉัน', 'ด้วย'] ['Stromboli', 'the', 'Master', 'Showman', 'that', \"'\", 's', 'a-me', 'and', 'by', 'special', 'permission', 'of', 'the', 'management', 'that', \"'\", 's', 'a-me', 'too', 'is', 'presenting', 'to', 'you', 'something', '...', 'you', 'will', 'absolutely', 'refuse', 'to', 'believe', '.']\n",
      "19 33\n",
      "0.42424242424242425\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ไม่', 'กัด', 'วัน'] ['Not', 'a', 'bit', 'for', 'days', '.']\n",
      "3 6\n",
      "0.5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58c5107ef594cf59fb3ef64be84d726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3200965), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ใช่ --- Uh, two big monsters... with big green eyes!  --- diff/ diff_ratio: 10 0.9090909090909091\n",
      "\n",
      "เขา --- He ain't here. He  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ไม่คะ --- Don't sail, do you? No, I don't.  --- diff/ diff_ratio: 13 0.8666666666666667\n",
      "\n",
      "สวัสดี --- Hello. What's the idea?  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "ที่หนึ่ง --- The boy admitted leaving the house at eight o'clock at night after being slapped by his father.  --- diff/ diff_ratio: 19 0.95\n",
      "\n",
      "ส่งตรงนี้ --- No, I haven't, it's gone past! Send this one straight.  --- diff/ diff_ratio: 16 0.8888888888888888\n",
      "\n",
      "โอ้ --- I think it is a laser.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ไฟแดง --- Oh, feet! Red light.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "เข้ามา --- I ain't got none.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "เฟือง --- I don't want your best boots.  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "ยิ่งไปกว่านั้น --- They began to kill me with worry.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "ไม่ฉันเสี่ยงในชีวิตของฉัน ที่นี่ --- 18.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "เปเปอ? --- It's pepper. Pepper?  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ไม่ต้อง --- No! No, you can't have them.  --- diff/ diff_ratio: 10 0.9090909090909091\n",
      "\n",
      "โนวา --- Nova! Nova, Nova, wake up.  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "ดี --- We'll get some water from the well.  --- diff/ diff_ratio: 9 0.9\n",
      "\n",
      "หมาจิ้งจอกไม่ได้พูดอย่างนั้น --- It's what The Fox said. I'll tell you right now, you're not going to allow yourself to be poisoned.  --- diff/ diff_ratio: 24 0.8571428571428571\n",
      "\n",
      "ผู้ที่แสดงเป็น 4 พี่น้องหญิง --- Daughters  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "โอเค --- Oh, it's okay.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "หุบปาก --- That's great! That's just great! Shut your mouth!  --- diff/ diff_ratio: 14 0.9333333333333333\n",
      "\n",
      "ก็ได้ ตกลง --- I think that Dawn Screen here has got the most to trade in. I went to you first.  --- diff/ diff_ratio: 18 0.9\n",
      "\n",
      "แบล์ร --- I can't find Blair.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "โอเค --- But I'm damn well sure some of you do !  --- diff/ diff_ratio: 11 0.9166666666666666\n",
      "\n",
      "อืม --- We could take a sample of each person's blood.  --- diff/ diff_ratio: 11 0.9166666666666666\n",
      "\n",
      "อืม --- We could mix it with uncontaminated blood.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "วินโดวส์ --- That key ring is always hooked to your belt.  --- diff/ diff_ratio: 9 0.9\n",
      "\n",
      "ไม่มีใคร --- We still have nothing to go on.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "เฮ้ แบล์ร --- Who says I want you goin' with me ? All right, cut the bullshit !  --- diff/ diff_ratio: 15 0.8823529411764706\n",
      "\n",
      "ใช่ --- I want to come back inside. Don't you understand ? I'm all right.  --- diff/ diff_ratio: 18 0.9473684210526315\n",
      "\n",
      "นายเข้าไปข้างใน แล้วบอกกับคนอื่น ๆ ว่าพวกเราเจอฟุชแล้ว --- Yeah.  --- diff/ diff_ratio: 12 0.8571428571428571\n",
      "\n",
      "ดร --- Dr. floyd to the medical bay.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "ไม่เป็นไร --- No. That's okay.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ความคิดไม่เลวนี่ --- Yeah. Might not be a bad idea... give some of the rest of us a shot at some lovin'.  --- diff/ diff_ratio: 19 0.8636363636363636\n",
      "\n",
      "ไม่ไหวแล้ว --- Oh, god. I can't look. Can't...  --- diff/ diff_ratio: 12 0.8571428571428571\n",
      "\n",
      "นั่นแหละ --- Captain! That's it.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ใช่ ฉันมาช่วยนาย ฉันคงห่วงนายจริงๆ เพราะจริงๆแล้ว ฉันไม่ชอบทั้งม้า ทั้งปืน โยนทิ้งไปเร็ว ฉันจะไปเอาม้า ไปเร็ว! --- look!  --- diff/ diff_ratio: 29 0.9354838709677419\n",
      "\n",
      "ไม่ --- all we have to do is switch sending boards.  --- diff/ diff_ratio: 9 0.9\n",
      "\n",
      "ไม่น่าเชื่อ --- I can't believe it.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ไม่เป็นไร --- Yeah, I'm fine.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "เรื่องโฮมสเตย์... ของอันย่ากันเอว่า --- Horsey  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "พวกเค้ากำลังจะกลับไปที่บ้านเค้าเหรอเนี่ย --- No  --- diff/ diff_ratio: 9 0.9\n",
      "\n",
      "สุขภาพเค้าไม่ค่อยดีหรอก เค้าเป็นคุณปู่แล้วหนะ --- Sheesh  --- diff/ diff_ratio: 10 0.9090909090909091\n",
      "\n",
      "เพราะคังอินแหละ พวกเค้าให้ของขวัญแค่อันย่ากับเอว่า --- Tough break  --- diff/ diff_ratio: 12 0.8571428571428571\n",
      "\n",
      "เยอะไปแล้ว! --- We simply drop the tinker toy frame to the ground, the elastic stretches, taking in te impact allowing the egg to remain gently in its resting place, Michelles booty  --- diff/ diff_ratio: 28 0.875\n",
      "\n",
      "พอกันที --- Okay, that's it.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "หุบปาก --- Mr. B. Shut up!  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "เดี๋ยวก่อนสิ --- Oh, God, what is it? Harry, wait a minute!  --- diff/ diff_ratio: 12 0.8571428571428571\n",
      "\n",
      "ขอบใจ --- Oh, yeah, here you go. Thanks.  --- diff/ diff_ratio: 9 0.9\n",
      "\n",
      "โกหก ลุกขึ้น --- My knee! You broke my knee! That's bull, man.  --- diff/ diff_ratio: 13 0.8666666666666667\n",
      "\n",
      "ตรงไหน --- Fourth floor, storeroom, in the back.  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "เฟสสิก มีหินอยู่ข้างหน้าหรือไม่? --- INIGO  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "นั่งลง --- (SWORD DROPS) Have a seat.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "คุณ --- Why'd you become a reporter?  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "เดี๋ยว ๆ --- Do you ever go by Simpkins' desk when she's grading papers?  --- diff/ diff_ratio: 13 0.8666666666666667\n",
      "\n",
      "ดี ๆ --- So? What do ya mean, \"So?\" That's it she's available!  --- diff/ diff_ratio: 18 0.9473684210526315\n",
      "\n",
      "โอ พระเจ้า --- OK, I'm ready. Here I go. I'm ready.  --- diff/ diff_ratio: 14 0.875\n",
      "\n",
      "อธิษฐาน --- Josh Baskin. How're you doin'? This is Derek.  --- diff/ diff_ratio: 12 0.9230769230769231\n",
      "\n",
      "ตํารวจ --- Bring down Rachel with you, all right?  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "ช่วยด้วย --- My birthday's November 3rd. I got a \"B\" in history.  --- diff/ diff_ratio: 15 0.8823529411764706\n",
      "\n",
      "ไฮ --- Shimmy, shimmy, go-go bop Shimmy, shimmy, rock  --- diff/ diff_ratio: 10 0.9090909090909091\n",
      "\n",
      "ไฮ --- Shimmy, shimmy, go-go bop Shimmy, shimmy, rock  --- diff/ diff_ratio: 10 0.9090909090909091\n",
      "\n",
      "แม่ --- From my dad's top drawer. You stole it?  --- diff/ diff_ratio: 11 0.9166666666666666\n",
      "\n",
      "ครับพ่อ --- I'm gonna be thirty years old for the rest of my life.  --- diff/ diff_ratio: 13 0.8666666666666667\n",
      "\n",
      "โชคดีนะ --- Your paper route. I don't think I can put that.  --- diff/ diff_ratio: 12 0.8571428571428571\n",
      "\n",
      "ไม่เป็นไรเลย --- Well, I really don't know where I can put her. Put her on unemployment!  --- diff/ diff_ratio: 17 0.8947368421052632\n",
      "\n",
      "ท่านครับ --- We'll start you off with last week's preschool orders.  --- diff/ diff_ratio: 12 0.8571428571428571\n",
      "\n",
      "ฮัลโหล --- It should take a few days give you a chance to find your way around.  --- diff/ diff_ratio: 15 0.9375\n",
      "\n",
      "สาบานเลยนะ ถ้าคุณทําอะไรเขาละก็ ถ้าคุณแตะผมเขาแม้ซักเส้นนึง --- Bye!  --- diff/ diff_ratio: 17 0.8947368421052632\n",
      "\n",
      "ก็ได้ --- What are you trying to do? Get us all fired?  --- diff/ diff_ratio: 11 0.9166666666666666\n",
      "\n",
      "เบรนเน่น --- I gave it to you yesterday!  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ไฮ --- Let's not lie to ourselves.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "จริงด้วย --- Josh is all right. I mean, he's OK and everything.  --- diff/ diff_ratio: 13 0.8666666666666667\n",
      "\n",
      "การสุ่มตัวอย่าง --- I'm gonna blow you away, Josh.  --- diff/ diff_ratio: 9 0.9\n",
      "\n",
      "เป็นครั้งคราว --- And they pay you for that? Yeah.  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "เรียบร้อย --- Could you get Media to send up the video of the Super Bowl?  --- diff/ diff_ratio: 13 0.9285714285714286\n",
      "\n",
      "โจชัว --- You're the luckiest guy I know!  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "เปล่า --- Well done, Josh. Well done.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "ดี --- Now that's what I call a tuxedo!  --- diff/ diff_ratio: 9 0.9\n",
      "\n",
      "เยี่ยม --- Hey, there's Miss Patterson. Hi!  --- diff/ diff_ratio: 9 0.9\n",
      "\n",
      "เอาละ --- ..it only works on your hips. You need a whole 'nother exercise for your thighs.  --- diff/ diff_ratio: 18 0.9473684210526315\n",
      "\n",
      "ผมรู้วิธีเล่นนะครับ เราสามารถทําทีม บริษัท แม็คมิลเลนได้นะ --- Gesundheit.  --- diff/ diff_ratio: 15 0.8823529411764706\n",
      "\n",
      "ผมพยายามเล่นเกมส์ของเขาแล้ว แต่เขาก็ยังทําร้ายผม --- Hey!  --- diff/ diff_ratio: 12 0.8571428571428571\n",
      "\n",
      "พระเจ้า --- Don't... No... no, don't. What?  --- diff/ diff_ratio: 13 0.9285714285714286\n",
      "\n",
      "สุขสันต์วันเกิด --- You don't need quarters for this either.  --- diff/ diff_ratio: 9 0.9\n",
      "\n",
      "ที่ไหน --- Maybe we could talk for a few minutes. Jesus!  --- diff/ diff_ratio: 10 0.9090909090909091\n",
      "\n",
      "ขอกาแฟถ้วยนึง คุณแพตเตอร์สัน แต่คุณไม่ดื่มกาแฟนี่คะ --- Argh!  --- diff/ diff_ratio: 16 0.8888888888888888\n",
      "\n",
      "กีฬาทุกชนิด --- You work as hard as he does and you're not like that.  --- diff/ diff_ratio: 13 0.8666666666666667\n",
      "\n",
      "ที่ผมไม่ได้บอกอะไรบางอย่างกับคุณ เพราะไม่ติดว่าคุณจะเชื่อผม ถึงคุณจะเชื่อ --- Sure.  --- diff/ diff_ratio: 18 0.9\n",
      "\n",
      "โอ ดีแล้ว --- We don't have to if you don't want to. OK.  --- diff/ diff_ratio: 14 0.875\n",
      "\n",
      "ครับ --- And, uh... make it black.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "ซูซาน --- I'll tell him, Mr...  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "บาย --- I'm in the middle of something, OK?  --- diff/ diff_ratio: 10 0.9090909090909091\n",
      "\n",
      "แม่ --- Can you give me a minute, please? Yes. What?  --- diff/ diff_ratio: 12 0.9230769230769231\n",
      "\n",
      "โธ่ --- I'm thirteen. Ohh...  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ตกลง --- All right, I need a tape.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "สายไม่ว่าง --- \"You know how subway get its name?  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "ใช่ --- This is like a good training device then.  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "ใช่ --- Yes, I'm a doctor.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "ขอบคุณ --- That's very nice of you.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "คะ --- No, \"carry.\" Come here.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "มี --- Why didn't you tell me?  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "หมายความว่าไง --- \"Talk like toilets\"? You know what I'm talking about.  --- diff/ diff_ratio: 12 0.8571428571428571\n",
      "\n",
      "ไม่ต้องหลอกฉันเลย --- You haven't figured that out, but you're funny. Look, don't kid me about this, all right?  --- diff/ diff_ratio: 23 0.8518518518518519\n",
      "\n",
      "วิเศษจริงๆ --- Yeah, this is great. This is really great, you know.  --- diff/ diff_ratio: 12 0.8571428571428571\n",
      "\n",
      "พร้อม --- We are on a \"go.\"  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ขอบคุณ --- I can't thank you enough.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "จริง --- That's what I said.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "เออ --- Yeah. So let's go, then.  --- diff/ diff_ratio: 9 0.9\n",
      "\n",
      "เปล่า --- No. So, what is it?  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "เซ็งบรม --- What did I do to deserve this? I'm sick of babysitting this whore.  --- diff/ diff_ratio: 15 0.8823529411764706\n",
      "\n",
      "ช้าๆ --- Okay, okay, take it easy.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "ลูกผู้ชาย --- Be a man! Do it!  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "เอาเลย --- Do it! Don't do it!  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "ถอยหลังเข้าคลอง --- We're getting it backwards.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ครับผม --- Yes, sir. Yes, sir.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "เดี๋ยวก่อน --- Back to the '60s.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ลูกคลื่น --- This is the wave! Way to go, man!  --- diff/ diff_ratio: 10 0.9090909090909091\n",
      "\n",
      "เฮนรี่ --- How're you doing, Henry?  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "เขามีโอกาส --- He's a cowboy. He's got too much to prove.  --- diff/ diff_ratio: 13 0.8666666666666667\n",
      "\n",
      "นั่นไง --- That's it. There it is.  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "ดีมาก --- That's nice! Yeah!  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ไม่. --- The ex-wife teaches him everything she knows, makes him rich and he dumps her, huh?  --- diff/ diff_ratio: 16 0.8888888888888888\n",
      "\n",
      "โดด! --- Here goes, better throw my hand in Wish me happy landing',all I gotta do is... jump!  --- diff/ diff_ratio: 19 0.9047619047619048\n",
      "\n",
      "จนปัญญา --- I'm at my wit's-end. Wit's-end.  --- diff/ diff_ratio: 12 0.9230769230769231\n",
      "\n",
      "เหมือนๆๆๆๆๆๆๆๆ --- Duplicated, duplicated, duplicated, duplicated, duplicated, duplicated, duplicated, duplicated, duplicated.  --- diff/ diff_ratio: 16 0.8888888888888888\n",
      "\n",
      "ไม่ใช่ --- Uh, chicken a'la king?  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "ใช่ --- Yes! Esalalumbo, shimin dumbo!  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ความจริง! --- If Jasmine found out I was really some crummy street rat, she'd laugh at me.  --- diff/ diff_ratio: 17 0.8947368421052632\n",
      "\n",
      "ความจริง --- Tell me the truth! The truth?  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "62 รอบ --- Sixty-two times, I think. Heh, heh, heh. Sixty-two times?  --- diff/ diff_ratio: 13 0.8666666666666667\n",
      "\n",
      "ทำไม --- Why? I can't sing.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "ก็ได้ --- All right, I can do that.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "ค'ะ --- Yeah, that's right.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ฉันไม่แน่ใจ --- What's the name of the bar? I'm not sure.  --- diff/ diff_ratio: 13 0.8666666666666667\n",
      "\n",
      "ที่ไหน --- What do you mean? Where?  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "เสียงดีนะ --- Don't walk away from me Not a bad voice. Sounds great.  --- diff/ diff_ratio: 13 0.8666666666666667\n",
      "\n",
      "เรียบร้อย --- Let the guy come and get his wallet.  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "พี่ พี่ --- She's flying to N.Y. for a Silver Mike Award.  --- diff/ diff_ratio: 13 0.8666666666666667\n",
      "\n",
      "หัวหน้า --- It turns out the accusations were false?  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "ครับ --- We expose the guy's whole life.  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "เค้าเป็นโปรตัวจริงเลยล่ะ ทำให้งานอาร์ตเป็นงานที่น่าสนใจขึ้นเยอะเลย [โปรโมชั่นพิเศษสำหรับถ่ายรูปคู่รัก] --- Damn it!  --- diff/ diff_ratio: 19 0.8636363636363636\n",
      "\n",
      "หือ --- You've gotta push hard!  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "คือผมพบนักลงทุนคนนึงน่ะครับ แต่ว่าหล่อนเคยเปนนักขุดทองมาก่อน --- Dad!  --- diff/ diff_ratio: 15 0.8823529411764706\n",
      "\n",
      "[แก้ไข .. --- I don't need an ambulance. I'm looking for my car. lt must've burnt up.  --- diff/ diff_ratio: 20 0.8695652173913043\n",
      "\n",
      "นี่ไม่คิดว่าชั้นทำทั้งหมดนี่ก้อเพื่อพวกนายเหรอ แน่สิ พาทุกคนมาที่นี่เลยนะ ด่วน --- Far out.  --- diff/ diff_ratio: 19 0.8636363636363636\n",
      "\n",
      "ขอบคุณนะครับ --- For that reason, we at Channel 4 will offer this unknown hero a reward of $1 million.  --- diff/ diff_ratio: 17 0.85\n",
      "\n",
      "ไปให้พ้น --- Leave me alone. Get out.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ครับ --- Yeah, it's that way.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "ใช่ --- I'll come back tomorrow.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ไม่มีปัญหา --- No problem. Ah, yes.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ใช่ --- Yes, I have a problem.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "นี่แหละ --- This is it. We hope you all enjoy your stay.  --- diff/ diff_ratio: 11 0.9166666666666666\n",
      "\n",
      "นั่นแหละ --- That's right. Ah!  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "เอามานี่ --- Gimme that thing. Now this is our first real push start... but don't worry about it, okay?  --- diff/ diff_ratio: 20 0.8695652173913043\n",
      "\n",
      "ดี นั่นแหละ --- Good. That's it. 'Even though the road seems very long and narrow'  --- diff/ diff_ratio: 16 0.8888888888888888\n",
      "\n",
      "เยี่ยมเลย --- Oh, goody. The Alliance has decided to change the qualifying time... from a minute-two to a minute-flat.  --- diff/ diff_ratio: 19 0.9047619047619048\n",
      "\n",
      "คณะกรรมการ --- Oh, it's the Alliance. God.  --- diff/ diff_ratio: 9 0.9\n",
      "\n",
      "ใช่ --- His punishment must be more fulsome, more lingering.  --- diff/ diff_ratio: 9 0.9\n",
      "\n",
      "โอ้ว --- Oh, I've missed you.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "อย่า! --- Oh, my, my, my, my, my. Don't!  --- diff/ diff_ratio: 14 0.875\n",
      "\n",
      "รถเมล์ --- To convey gorgeous creatures such as yourselves... to your most forbidden desires.  --- diff/ diff_ratio: 13 0.9285714285714286\n",
      "\n",
      "ขอโทษ --- Did you find them? Sorry.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "บ๊ายบาย --- And you will know our love was meant to be Bye-bye.  --- diff/ diff_ratio: 11 0.9166666666666666\n",
      "\n",
      "คนบ้า --- Tsk, tsk, tsk. Dog fart!  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "โอ้.. --- If he can go down a chimney... he can fit down here.  --- diff/ diff_ratio: 12 0.8571428571428571\n",
      "\n",
      "แจ๊ค. --- He'll fix things, Jack.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "ประชาสัมพันธ์.. --- I'd make sure it's known the company's in business.  --- diff/ diff_ratio: 14 0.875\n",
      "\n",
      "ประสานงาน --- I'd see that it had a certain panache.  --- diff/ diff_ratio: 10 0.9090909090909091\n",
      "\n",
      "ไม่มีปัญหา --- This won't be a problem.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "ทุกอย่าง --- He's highly skilled. Give him your card.  --- diff/ diff_ratio: 10 0.9090909090909091\n",
      "\n",
      "แด็งก้า.. --- Danka, look at the snow.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ทางโน้น --- That line. [ Whistle Blowing ]  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ได้โปรด --- Please hide ! Please ! [ Muttering ] ...warmed by the sun.  --- diff/ diff_ratio: 12 0.8571428571428571\n",
      "\n",
      "ดีมาก --- Oh, that's very good.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "พระเจ้า --- [ Gun Clicks ] Oh, Christ.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "คัดเลือก --- [ Officer ] Alle in einer Einzelreihe aufstellen !  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "ไม่เห็นเขาเลย --- I didn't see him ! They're hiding. I'm telling you, they're hiding.  --- diff/ diff_ratio: 20 0.8695652173913043\n",
      "\n",
      "ดี --- That's it, that's it.  --- diff/ diff_ratio: 9 0.9\n",
      "\n",
      "ไม่น่าเลย --- This is really cruel, Oskar.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ห๋า.. --- I have 20 meters at home in my garden.  --- diff/ diff_ratio: 9 0.9\n",
      "\n",
      "เป็นเรื่องเงินรึเปล่า --- They should be pitied, not punished. They should receive treatment. This is as real as typhus.  --- diff/ diff_ratio: 17 0.85\n",
      "\n",
      "เมื่อไหร่ --- The party's over, Oskar.  --- diff/ diff_ratio: 7 0.875\n",
      "\n",
      "ไม่ได้ --- I'll never find a maid as well trained as her at Brinnlitz.  --- diff/ diff_ratio: 14 0.9333333333333333\n",
      "\n",
      "มาเลย --- [ Soldier ] Mensch, seitJahren hab' ich so etwas nicht mehr gesehen.  --- diff/ diff_ratio: 13 0.8666666666666667\n",
      "\n",
      "เกราะรถถัง --- What is it ? We received an angry complaint from the Armaments Board.  --- diff/ diff_ratio: 12 0.8571428571428571\n",
      "\n",
      "ไม่มี --- Do you have any money, uh, hidden away someplace... that I don't know about ?  --- diff/ diff_ratio: 19 0.95\n",
      "\n",
      "อาชญากร --- I'm a profiteer of slave labor.  --- diff/ diff_ratio: 8 0.8888888888888888\n",
      "\n",
      "จากระยะทางที่ต้องใช้ ในการเดินทาง จากส่วนลึกของอวกาศ จะต้องใช้พลังงานจำนวนมาก เกินกว่ายานอวกาศ จะทำได้ --- Conventional wisdom.  --- diff/ diff_ratio: 19 0.8636363636363636\n",
      "\n",
      "ใช่ --- We're trying to find a connection in these deaths.  --- diff/ diff_ratio: 11 0.9166666666666666\n",
      "\n",
      "คุณหมอ ? --- I'm wondering if we can do a cursory medical exam on Peggy.  --- diff/ diff_ratio: 13 0.8666666666666667\n",
      "\n",
      "มากกว่า --- Do you have a better explanation?  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "ผมจ้องนาฬิกาผม ก่อนที่จะมีแสงจ้า นั่นเวลา 9: 03 แต่ตอนนี้เวลา 9 --- Look!  --- diff/ diff_ratio: 15 0.8823529411764706\n",
      "\n",
      "ค่ะ --- You're shaking. I need to sit down.  --- diff/ diff_ratio: 10 0.9090909090909091\n",
      "\n",
      "เรากลับไปโรงแรม กันเถอะ อะไรนะ ? พวกเขาขโมยศพไป รึ ? --- FBI.  --- diff/ diff_ratio: 13 0.8666666666666667\n",
      "\n",
      "มีค่ะ --- I'm going to die, aren't I? I'm gonna be next!  --- diff/ diff_ratio: 17 0.8947368421052632\n",
      "\n",
      "อะไร ? --- Peggy O'Dell's watch stopped a couple of minutes after nine.  --- diff/ diff_ratio: 13 0.8666666666666667\n",
      "\n",
      "ตรงนี้ --- I would wait for their orders.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "แสงสว่าง --- They said it would be OK. No one would know.  --- diff/ diff_ratio: 11 0.9166666666666666\n",
      "\n",
      "ฮัลโหล ? --- Scully, it's me. I haven't been able to sleep.  --- diff/ diff_ratio: 14 0.875\n",
      "\n",
      "ไม่ --- No, I'm not.  --- diff/ diff_ratio: 6 0.8571428571428571\n",
      "\n",
      "\n",
      "count: 7139\n"
     ]
    }
   ],
   "source": [
    "c = Counter()\n",
    "_tokenize = partial(word_tokenize, engine=\"newmm\", keep_whitespace=False)\n",
    "\n",
    "threashold = 0.85\n",
    "for i, sent in tqdm_notebook(enumerate(cleaned_and_filtered_th), total=len(cleaned_and_filtered_th)):\n",
    "    \n",
    "    src_toks, tgt_toks = _tokenize(cleaned_and_filtered_th[i]), _tokenize(cleaned_and_filtered_en[i])\n",
    "    src_toks_len, tgt_toks_len = len(src_toks), len(tgt_toks)\n",
    "        \n",
    "    diff = abs(src_toks_len- tgt_toks_len )\n",
    "    diff_ratio = diff / ( max(src_toks_len, tgt_toks_len) ) \n",
    "\n",
    "    if  diff_ratio >= threashold:\n",
    "        c['counter_{:.2}'.format(threashold)] += 1\n",
    "\n",
    "        if c['counter_{:.2}'.format(threashold)] <= 200:\n",
    "\n",
    "            print(sent, '---', cleaned_and_filtered_en[i], ' --- diff/ diff_ratio:', diff, diff_ratio)\n",
    "            print()\n",
    "print('count:', c['counter_{:.2}'.format(threashold)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/opensubtitle_v2018/sent.cleaned.v1.th', 'w') as f:\n",
    "    for sent in cleaned_and_filtered_th:\n",
    "        f.write(sent + '\\n')\n",
    "        \n",
    "with open('../data/opensubtitle_v2018/sent.cleaned.v1.en', 'w') as f:\n",
    "    for sent in cleaned_and_filtered_en:\n",
    "        f.write(sent + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Text Segmentation (word, subword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for newmm tokenization\n",
    "\n",
    "def tokenize_worker(sentence, lang, trie):\n",
    "    \n",
    "    _tokenizer_newmm = partial(pythainlp.tokenize.word_tokenize, engine='newmm',\n",
    "                               keep_whitespace=False,\n",
    "                              custom_dict=(trie if trie != None else DEFAULT_DICT_TRIE))\n",
    "    return ' '.join(_tokenizer_newmm(sentence))\n",
    "  \n",
    "def tokenize_handler(sentences, lang, trie=None):\n",
    "    toks = []\n",
    "    p = Pool(6)\n",
    "    t = time()\n",
    "    _tokenize_worker = partial(tokenize_worker, lang=lang, trie=trie)\n",
    "    toks = p.map(_tokenize_worker, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    print('{} s'.format(time() -t))\n",
    "\n",
    "    return toks\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_spaced_tokens_to_file(data, folder_name, filename):\n",
    "    with open('/root/mt-opus/data/{}/{}'.format(folder_name, filename),'w') as f:\n",
    "        for item in data:\n",
    "            f.write(item + '\\n')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = {\n",
    "    'th': {\n",
    "        'sentencepiece': [],\n",
    "        'newmm':[]\n",
    "    },\n",
    "    'en': {\n",
    "        'sentencepiece': [],\n",
    "        'newmm':[]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Segment text into words with a dictionary-based tokenizer (`newmm`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.45962905883789 s\n",
      "63.490153074264526 s\n"
     ]
    }
   ],
   "source": [
    "# Thai\n",
    "toks['th']['newmm'] = tokenize_handler(filtered_th, lang='th')\n",
    "# English\n",
    "toks['en']['newmm'] = tokenize_handler(filtered_en, lang='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ทาส ใน กระจก วิเศษ , มาจาก พื้นที่ ที่ ไกล ที่สุด',\n",
       "  'ผ่าน ลม และ ความมืด ฉัน เรียก เจ้า',\n",
       "  'พูด !',\n",
       "  'ให้ ฉัน เห็น พระพักตร์ ของ พระองค์',\n",
       "  'สิ่ง ที่ เจ้า จะ รู้ ว่า สมเด็จ พระราชินี ของ ฉัน ได้ อย่างไร',\n",
       "  'กระจก วิเศษ บน ผนัง ผู้ ที่ เป็น สังขาร หนึ่ง ทั้งหมด หรือไม่',\n",
       "  'ที่ มีชื่อเสียง เป็น ความงาม ของ เจ้า พระ บาท สมเด็จ พระเจ้าอยู่หัว',\n",
       "  'แต่ ถือเป็น แม่บ้าน ที่ น่ารัก ที่ ฉัน เห็น',\n",
       "  'ยาจก ไม่ สามารถ ซ่อน พระคุณ อ่อนโยน ของ เธอ',\n",
       "  'อนิจจา เธอ มี ความเป็นธรรม มากขึ้น กว่า เจ้า'],\n",
       " ['Slave in the Magic Mirror , come from the farthest space .',\n",
       "  'Through wind and darkness , I summon thee .',\n",
       "  'Speak !',\n",
       "  'Let me see thy face .',\n",
       "  'What wouldst thou know , my Queen ?',\n",
       "  'Magic Mirror on the wall , who is the fairest one of all ?',\n",
       "  'Famed is thy beauty , Majesty .',\n",
       "  'But hold , a lovely maid I see .',\n",
       "  'Rags cannot hide her gentle grace .',\n",
       "  'Alas , she is more fair than thee .'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks['th']['newmm'][0:10], toks['en']['newmm'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Segment text into subword (ie. BPE tokens) with Pretrained SentencePiece (`BPEmb`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bpe(sentences, lang, n_vocab=25000):\n",
    "    \"\"\"Return a list of bpe tokens give a list of sentences\"\"\"\n",
    "    segmented_sentences = []\n",
    "    for sentence in tqdm_notebook(sentences, total=len(sentences)):\n",
    "#         print(sentence)\n",
    "        bpe_tokens = bpemb_pretrained[lang]['{}'.format(n_vocab)].encode(sentence)\n",
    "        segmented_sentences.append(' '.join(bpe_tokens))\n",
    "        \n",
    "    return segmented_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Thai language__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce4eb5e8cbe469886307235a01f1dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3202751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['▁ท าส ใน กระจก วิเศษ , ▁มาจาก พื้นที่ ที่ ไกล ที่สุด', '▁ผ่าน ลม และความ มืด ฉัน เรียก เจ้า', '▁พูด !', '▁ให้ ฉัน เห็น พระพักตร์ ของ ▁พระองค์', '▁สิ่งที่ เจ้า จะ รู้ว่า สมเด็จพระราชินี ▁ของ ฉัน ได้อย่างไร', '▁กระจ ก วิเศษ บน ผนัง ▁ผู้ ที่เป็น สัง ขาร หนึ่ง ทั้งหมด ▁หรือไม่', '▁ที่มีชื่อเสียง เป็น ความงาม ของ ▁เจ้า พระบาทสมเด็จพระ เจ้าอยู่หัว', '▁แต่ ถือเป็น แม่ บ้าน ที่น ่ารัก ที่ ฉัน ▁เห็น', '▁ยา จก ไม่สามารถ ซ่อน พระคุณ ▁อ่อน โยน ของเธอ', '▁อน ิจ จา เธอ มีความเป็น ธรรม ▁มาก ขึ้น กว่า เจ้า']\n"
     ]
    }
   ],
   "source": [
    "toks['th']['sentencepiece'] = encode_bpe(filtered_th, 'th', 25000)\n",
    "\n",
    "print(toks['th']['sentencepiece'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__English language__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0ac2a9e16c4fe0814a82c713b07a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3202751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['▁slave ▁in ▁the ▁magic ▁mirror , ▁come ▁from ▁the ▁fart hest ▁space .', '▁through ▁wind ▁and ▁darkness , ▁i ▁summon ▁the e .', '▁speak !', '▁let ▁me ▁see ▁thy ▁face .', '▁what ▁would st ▁thou ▁know , ▁my ▁queen ?', '▁magic ▁mirror ▁on ▁the ▁wall , ▁who ▁is ▁the ▁fa ire st ▁one ▁of ▁all ?', '▁famed ▁is ▁thy ▁beauty , ▁majesty .', '▁but ▁hold , ▁a ▁lov ely ▁maid ▁i ▁see .', '▁ra gs ▁cannot ▁hide ▁her ▁gentle ▁grace .', '▁al as , ▁she ▁is ▁more ▁fair ▁than ▁the e .']\n"
     ]
    }
   ],
   "source": [
    "toks['en']['sentencepiece']  = encode_bpe(filtered_en, 'en', 25000)\n",
    "print(toks['en']['sentencepiece'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) Segment text into subword (ie. BPE tokens) with SentencePiece model (unigram) trained from Opensubtitles_v2018 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: --input must not be empty\n",
      "\n",
      "sentencepiece\n",
      "\n",
      "Usage: sentencepiece [options] files\n",
      "\n",
      "   --accept_language (comma-separated list of languages this model can accept)  type: string  default: \n",
      "   --add_dummy_prefix (Add dummy whitespace at the beginning of text)  type: bool  default: true\n",
      "   --bos_id (Override BOS (<s>) id. Set -1 to disable BOS.)  type: int32  default: 1\n",
      "   --bos_piece (Override BOS (<s>) piece.)  type: string  default: <s>\n",
      "   --character_coverage (character coverage to determine the minimum symbols)  type: double  default: 0.9995\n",
      "   --control_symbols (comma separated list of control symbols)  type: string  default: \n",
      "   --eos_id (Override EOS (</s>) id. Set -1 to disable EOS.)  type: int32  default: 2\n",
      "   --eos_piece (Override EOS (</s>) piece.)  type: string  default: </s>\n",
      "   --hard_vocab_limit (If set to false, --vocab_size is considered as a soft limit.)  type: bool  default: true\n",
      "   --input (comma separated list of input sentences)  type: string  default: \n",
      "   --input_format (Input format. Supported format is `text` or `tsv`.)  type: string  default: \n",
      "   --input_sentence_size (maximum size of sentences the trainer loads)  type: int32  default: 0\n",
      "   --max_sentence_length (maximum length of sentence in byte)  type: int32  default: 4192\n",
      "   --max_sentencepiece_length (maximum length of sentence piece)  type: int32  default: 16\n",
      "   --model_prefix (output model prefix)  type: string  default: \n",
      "   --model_type (model algorithm: unigram, bpe, word or char)  type: string  default: unigram\n",
      "   --normalization_rule_name (Normalization rule name. Choose from nfkc or identity)  type: string  default: nmt_nfkc\n",
      "   --normalization_rule_tsv (Normalization rule TSV file. )  type: string  default: \n",
      "   --num_sub_iterations (number of EM sub-iterations)  type: int32  default: 2\n",
      "   --num_threads (number of threads for training)  type: int32  default: 16\n",
      "   --pad_id (Override PAD (<pad>) id. Set -1 to disable PAD.)  type: int32  default: -1\n",
      "   --pad_piece (Override PAD (<pad>) piece.)  type: string  default: <pad>\n",
      "   --remove_extra_whitespaces (Removes leading, trailing, and duplicate internal whitespace)  type: bool  default: true\n",
      "   --seed_sentencepiece_size (the size of seed sentencepieces)  type: int32  default: 1000000\n",
      "   --self_test_sample_size (the size of self test samples)  type: int32  default: 0\n",
      "   --shrinking_factor (Keeps top shrinking_factor pieces with respect to the loss)  type: double  default: 0.75\n",
      "   --shuffle_input_sentence (Randomly sample input sentences in advance. Valid when --input_sentence_size > 0)  type: bool  default: true\n",
      "   --split_by_number (split tokens by numbers (0-9))  type: bool  default: true\n",
      "   --split_by_unicode_script (use Unicode script to split sentence pieces)  type: bool  default: true\n",
      "   --split_by_whitespace (use a white space to split sentence pieces)  type: bool  default: true\n",
      "   --treat_whitespace_as_suffix (treat whitespace marker as suffix instead of prefix.)  type: bool  default: false\n",
      "   --unk_id (Override UNK (<unk>) id.)  type: int32  default: 0\n",
      "   --unk_piece (Override UNK (<unk>) piece.)  type: string  default: <unk>\n",
      "   --unk_surface (Dummy surface string for <unk>. In decoding <unk> is decoded to `unk_surface`.)  type: string  default:  ⁇ \n",
      "   --use_all_vocab (If set to true, use all tokens as vocab. Valid for word/char models.)  type: bool  default: false\n",
      "   --user_defined_symbols (comma separated list of user defined symbols)  type: string  default: \n",
      "   --vocab_size (vocabulary size)  type: int32  default: 8000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Split train-valid-test \n",
    "\n",
    "- split: train/valid/test (80/10/10)\n",
    "\n",
    "- seed for rando,.shuffle: 1234\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N =  3202751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2562200, 320275, 320276)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train-valid-test split 80/10/10\n",
    "\n",
    "n = len(toks['th']['newmm'])\n",
    "\n",
    "print('N = ',n)\n",
    "idx = list(range(n))\n",
    "\n",
    "random.seed(1234) # Set SEED\n",
    "random.shuffle(idx)\n",
    "\n",
    "train_idx, valid_idx, test_idx = idx[:int(n*0.8)], idx[int(n*0.8):int(n*0.9)], idx[int(n*0.9):]\n",
    "\n",
    "dataset_split = {}\n",
    "dataset_split['train'] = train_idx\n",
    "dataset_split['valid'] = valid_idx\n",
    "dataset_split['test'] = test_idx\n",
    "\n",
    "\n",
    "len(train_idx),len(valid_idx),len(test_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    },\n",
    "    'valid': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    },\n",
    "    'test': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    for lang in ['th', 'en']:\n",
    "        for tok_type in ['sentencepiece', 'newmm']:\n",
    "\n",
    "            dataset[split_name][lang][tok_type] = [toks[lang][tok_type][i] for i in dataset_split[split_name]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['เบค กี้ เธอ ทำท่า แปลก ๆ เมื่อกี้ ใน ห้อง', 'อยู่ กับ เธอ แอน นา จะ นำทาง คุณ ผม จะ กลับ ไป'] \n",
      "\n",
      "['Becky , um , you were acting particularly strange in there just now .', \"Stay with her so Anna can guide you . I ' m going back .\"] \n",
      "\n",
      "['▁เบ ค กี้ ▁เธอ ทํา ท่า แปลก ๆ ▁เมื่อ กี้ ▁ในห้อง', '▁ อยู่กับ เธอ ▁แอนนา จะนํา ทาง คุณ ▁ผม จะ กลับไป'] \n",
      "\n",
      "['▁bec ky , ▁um , ▁you ▁were ▁acting ▁particularly ▁strange ▁in ▁there ▁just ▁now .', \"▁stay ▁with ▁her ▁so ▁anna ▁can ▁guide ▁you . ▁i ' m ▁going ▁back .\"] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['th']['newmm'][0:2],'\\n')\n",
    "print(dataset['train']['en']['newmm'][0:2],'\\n')\n",
    "print(dataset['train']['th']['sentencepiece'][0:2],'\\n')\n",
    "print(dataset['train']['en']['sentencepiece'][0:2],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'en_train_n_toks': 92383739, 'th_train_n_toks': 86683223, 'en_valid_n_toks': 11536351, 'en_test_n_toks': 11535798, 'th_test_n_toks': 10833242, 'th_valid_n_toks': 10826042})\n"
     ]
    }
   ],
   "source": [
    "# Counting number of tokens for train, valid, test\n",
    "counter = Counter( )\n",
    "for dataset_type in ['train', 'valid', 'test']:\n",
    "    for th_sent_toks in dataset[dataset_type]['th']['newmm']:\n",
    "        counter['th_{}_n_toks'.format(dataset_type)] += len(th_sent_toks)\n",
    "    for en_sent_toks in dataset[dataset_type]['en']['newmm']:\n",
    "        counter['en_{}_n_toks'.format(dataset_type)] += len(en_sent_toks)\n",
    "\n",
    "print(counter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write segmented text to files__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-sentencepiece/th-en\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-sentencepiece/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-sentencepiece/en-th\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-sentencepiece/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-newmm/th-en\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-newmm/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-newmm/en-th\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-newmm/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-sentencepiece/th-en\n",
      "dir: ../data/opensubtitles_bin/newmm-sentencepiece/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-sentencepiece/en-th\n",
      "dir: ../data/opensubtitles_bin/newmm-sentencepiece/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-newmm/th-en\n",
      "dir: ../data/opensubtitles_bin/newmm-newmm/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-newmm/en-th\n",
      "dir: ../data/opensubtitles_bin/newmm-newmm/en-th\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tok_type_src in ['sentencepiece', 'newmm']:\n",
    "    for tok_type_tgt in ['sentencepiece', 'newmm']:\n",
    "        langs = ['th', 'en']\n",
    "        for lang in langs:\n",
    "            src_lang = lang\n",
    "            tgt_lang = 'en' if lang =='th' else 'th'\n",
    "            FOLDER_NAME = \"opensubtitles_tok/{}-{}/{}-{}\".format(tok_type_src, tok_type_tgt, src_lang, tgt_lang )\n",
    "            FOLDER_NAME_BIN = \"opensubtitles_bin/{}-{}/{}-{}\".format(tok_type_src, tok_type_tgt, src_lang, tgt_lang)\n",
    "           \n",
    "            \n",
    "            # Create directories\n",
    "            print('create directories: ')\n",
    "            print('dir: ../data/{}'.format(FOLDER_NAME))\n",
    "            print('dir: ../data/{}'.format(FOLDER_NAME_BIN))\n",
    "\n",
    "            !mkdir -p ../data/{FOLDER_NAME}\n",
    "            !mkdir -p ../data/{FOLDER_NAME_BIN}\n",
    "\n",
    "            for split_name in ['train', 'valid', 'test']:\n",
    "                \n",
    "                write_spaced_tokens_to_file(dataset[split_name][src_lang][tok_type_src],\n",
    "                                            FOLDER_NAME, '{}.{}'.format(split_name, src_lang))\n",
    "                \n",
    "                write_spaced_tokens_to_file(dataset[split_name][tgt_lang][tok_type_tgt],\n",
    "                                            FOLDER_NAME, '{}.{}'.format(split_name, tgt_lang))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁bec ky , ▁um , ▁you ▁were ▁acting ▁particularly ▁strange ▁in ▁there ▁just ▁now .\n",
      "▁stay ▁with ▁her ▁so ▁anna ▁can ▁guide ▁you . ▁i ' m ▁going ▁back .\n",
      "▁look .\n",
      "▁oh , ▁no , ▁it ' s ▁the ▁other ▁way ▁around , ▁dr . ▁lewis .\n",
      "▁sort ▁of .\n",
      "▁bart ender , ▁something ▁really ▁strong , ▁please .\n",
      "▁yes , ▁obviously .\n",
      "▁la ' s ▁so ▁nice .\n",
      "▁i ' m ▁going ▁to ▁fix ▁it .\n",
      "▁i ▁get ▁b ored .\n"
     ]
    }
   ],
   "source": [
    "!head ../data/opensubtitles_tok/newmm-sentencepiece/th-en/train.en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "เบค กี้ เธอ ทำท่า แปลก ๆ เมื่อกี้ ใน ห้อง\n",
      "อยู่ กับ เธอ แอน นา จะ นำทาง คุณ ผม จะ กลับ ไป\n",
      "ฟัง นะ\n",
      "พอดี เลย ดร. ลี วิ ส\n",
      "แบบ ว่า\n",
      "เอ่อ บาร์ เท็น เด อร ์ ขอ อะไร ที่\n",
      "ก็ ใช่ ห น่ะ สิ\n",
      "แอลเอ สวย เนอะ\n",
      "ฉัน กำลังจะ แก้ ไขมัน\n",
      "ฉัน เบื่อ ละ\n"
     ]
    }
   ],
   "source": [
    "!head ../data/opensubtitles_tok/newmm-sentencepiece/th-en/train.th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
