{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.7\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import copy \n",
    "import re\n",
    "import html\n",
    "\n",
    "from time import time \n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import pythainlp\n",
    "from pythainlp.util import *\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.ulmfit import *\n",
    "\n",
    "# subword-nmt\n",
    "from subword_nmt import learn_bpe as learner\n",
    "from subword_nmt import apply_bpe as subword_tokenizer\n",
    "\n",
    "import fairseq \n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pythainlp.tokenize import DEFAULT_DICT_TRIE\n",
    "\n",
    "from pythainlp.corpus import thai_words\n",
    "\n",
    "print(pythainlp.__version__)\n",
    "# assert pythainlp.__version__ == '2.1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install BPEmb (BPE embeddings)\n",
    "\n",
    "!pip install --q bpemb emoji subword-nmt fairseq tensorflow_hub sentencepiece tf_sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USE\n",
    "\n",
    "# g = tf.Graph()\n",
    "# with g.as_default():\n",
    "#     text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "#     embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/1\")\n",
    "#     embedded_text = embed(text_input)\n",
    "#     init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "# g.finalize()\n",
    "\n",
    "# # def compute_similarity(src, tgt):\n",
    "# #     \"\"\"\n",
    "# #         Calculate sentence similarity based on Google Universal Sentence Encoder (Multilingual Large)\n",
    "# #     \"\"\"\n",
    "\n",
    "# # # Initialize session.\n",
    "# # session = tf.Session(graph=g)\n",
    "# # session.run(init_op)\n",
    "\n",
    "# # # Compute embeddings.\n",
    "# # en_result = session.run(embedded_text, feed_dict={text_input: english_sentences})\n",
    "# # it_result = session.run(embedded_text, feed_dict={text_input: italian_sentences})\n",
    "# # ja_result = session.run(embedded_text, feed_dict={text_input: japanese_sentences})\n",
    "\n",
    "# # # Compute similarity matrix. Higher score indicates greater similarity.\n",
    "# # similarity_matrix_it = np.inner(en_result, it_result)\n",
    "# # similarity_matrix_ja = np.inner(en_result, ja_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "\n",
    "bpemb_pretrained ={\n",
    "    'th': {\n",
    "        '25000': BPEmb(lang=\"th\", vs=25000)\n",
    "    },\n",
    "    'en': {\n",
    "        '25000': BPEmb(lang=\"en\", vs=25000)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3281534,\n",
       " ['Slave in the Magic Mirror, come from the farthest space.',\n",
       "  'Through wind and darkness, I summon thee.',\n",
       "  'Speak!'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/opensubtitle_v2018/OpenSubtitles.en-th.en','r', encoding='utf-8') as f:\n",
    "    en = f.read().split('\\n')\n",
    "len(en),en[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3281534,\n",
       " ['ทาสในกระจกวิเศษ, มาจากพื้นที่ที่ไกลที่สุด',\n",
       "  'ผ่านลมและความมืดฉันเรียกเจ้า',\n",
       "  'พูด!'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/opensubtitle_v2018/OpenSubtitles.en-th.th','r', encoding='utf-8') as f:\n",
    "    th = f.read().split('\\n')\n",
    "    \n",
    "len(th),th[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess Opensubtitles_v2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt_opus.preprocess import (\n",
    "    SentenceLengthLessThanOrEqualToOne,\n",
    "    SentenceContainsUnknownSymbol,\n",
    "    SentenceContainsAdSymbol,\n",
    "    ThaiSentenceContainsNoThaiCharacters,\n",
    "    ThaiSentenceContainsNoThaiCharactersPattern,\n",
    "\n",
    "    UnescapeString,\n",
    "    RemoveUnwantedSymbols,\n",
    "    RemoveUnwantedPattern,\n",
    "    ReplaceDashInSentence,\n",
    "    RemoveHashtagInSentence,\n",
    "    RemoveFullStopInThaiSentence,\n",
    "    NormalizeThaiVowel,\n",
    "    \n",
    "    SentencePairFoundRepeatedText,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to filter out a sentence pair\n",
    "filtering_rules = [\n",
    "    SentenceLengthLessThanOrEqualToOne,\n",
    "    SentenceContainsAdSymbol,\n",
    "    SentenceContainsUnknownSymbol,\n",
    "    ThaiSentenceContainsNoThaiCharacters,\n",
    "    ThaiSentenceContainsNoThaiCharactersPattern,\n",
    "]\n",
    "\n",
    "cleaning_rules = [\n",
    "    UnescapeString,\n",
    "    RemoveUnwantedSymbols,\n",
    "    RemoveUnwantedPattern,\n",
    "    ReplaceDashInSentence,\n",
    "    RemoveHashtagInSentence,\n",
    "    RemoveFullStopInThaiSentence,\n",
    "    NormalizeThaiVowel,\n",
    "]\n",
    "\n",
    "filtering_sentence_pair_rules = [\n",
    "    SentencePairFoundRepeatedText\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could be parallelized\n",
    "def filter_sentence(sentence, lang, rules=filtering_rules):\n",
    "    \"\"\"\n",
    "        Return True if a sentence match filtering pattern\n",
    "    \"\"\"\n",
    "    for rule in rules:\n",
    "        rule_obj = rule()\n",
    "        if rule_obj.test(sentence, lang=lang):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filter_sentences(sentences, lang, rules=filtering_rules):\n",
    "    \"\"\"\n",
    "        Returns a list of Boolean value, if such element is True, it means filter that sentence pair.\n",
    "        Otherwise, keep that sentnence pair.\n",
    "    \"\"\"\n",
    "    filtering_indices = []\n",
    "    p = Pool() # use all available cores\n",
    "    t = time()\n",
    "\n",
    "    _filter_sentence = partial(filter_sentence, lang=lang)\n",
    "    filtering_indices = p.map(_filter_sentence, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    filtering_indices_np = np.array(filtering_indices)\n",
    "    number_of_filtered_out = np.sum(filtering_indices_np)\n",
    "    \n",
    "    print('Time taken: {} s'.format(time() -t))\n",
    "    print('# sentences ({}) before filtered out'.format(lang), len(sentences))\n",
    "    print('# sentences ({})filtered out'.format(lang), number_of_filtered_out)\n",
    "    print('# sentences ({})after filtered out'.format(lang), len(sentences) - number_of_filtered_out)\n",
    "\n",
    "    return filtering_indices_np, number_of_filtered_out\n",
    "\n",
    "\n",
    "def filter_sentence_pair(sentence_pairs, rules=filtering_sentence_pair_rules):\n",
    "    \"\"\"\n",
    "        Return True if a sentence match filtering sentence pair pattern\n",
    "    \"\"\"\n",
    "    for rule in rules:\n",
    "        rule_obj = rule()\n",
    "        if rule_obj.test(sentence_pairs):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filter_sentence_pairs(sentence_pairs, rules=filtering_sentence_pair_rules):\n",
    "    \"\"\"\n",
    "        Returns a list of Boolean value, if such element is True, it means filter that sentence pair.\n",
    "        Otherwise, keep that sentnence pair.\n",
    "    \"\"\"\n",
    "    filtering_indices = []\n",
    "    p = Pool() # use all available cores\n",
    "    t = time()\n",
    "\n",
    "    _filter_sentence_pair = partial(filter_sentence_pair)\n",
    "    filtering_indices = p.map(_filter_sentence_pair, sentence_pairs)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    filtering_indices_np = np.array(filtering_indices)\n",
    "    number_of_filtered_out = np.sum(filtering_indices_np)\n",
    "    \n",
    "    print('Time taken: {} s'.format(time() -t))\n",
    "    print('# sentences before filtered out', len(sentence_pairs))\n",
    "    print('# sentences filtered out', number_of_filtered_out)\n",
    "    print('# sentences after filtered out', len(sentence_pairs) - number_of_filtered_out)\n",
    "\n",
    "    return filtering_indices_np, number_of_filtered_out\n",
    "\n",
    "\n",
    "def clean_sentence(sentence, lang, rules=cleaning_rules):\n",
    "    for rule in rules: \n",
    "        rule_obj = rule()\n",
    "        if rule_obj.test(sentence, lang=lang):\n",
    "            sentence = rule_obj.replace(sentence, lang=lang)\n",
    "    return sentence\n",
    "\n",
    "def clean_sentences(sentences, lang, rules=cleaning_rules):\n",
    "    \"\"\"\n",
    "        Clean the sentence with the specified text cleaning rules\n",
    "        Return a list of cleaned sentences\n",
    "    \"\"\"\n",
    "    p = Pool() # use all available cores\n",
    "    t = time()\n",
    "    \n",
    "    _clean_sentence = partial(clean_sentence, lang=lang)\n",
    "    cleaned_sentences = p.map(_clean_sentence, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    print('Time taken: {} s'.format(time() -t))\n",
    "    \n",
    "    return cleaned_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Filter by each language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 20.27482318878174 s\n",
      "# sentences (th) before filtered out 3281534\n",
      "# sentences (th)filtered out 77301\n",
      "# sentences (th)after filtered out 3204233\n"
     ]
    }
   ],
   "source": [
    "filtering_indices_th, _ = filter_sentences(th, lang='th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 6.393211841583252 s\n",
      "# sentences (en) before filtered out 3281534\n",
      "# sentences (en)filtered out 4453\n",
      "# sentences (en)after filtered out 3277081\n"
     ]
    }
   ],
   "source": [
    "filtering_indices_en, _ = filter_sentences(en, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# filtering_indices_th_en 79519\n"
     ]
    }
   ],
   "source": [
    "filtering_indices_th_en = filtering_indices_th | filtering_indices_en\n",
    "\n",
    "print('# filtering_indices_th_en', sum(filtering_indices_th_en))\n",
    "\n",
    "filtered_th = [th[i] for i, filtered_out in enumerate(filtering_indices_th_en) if not filtered_out]\n",
    "filtered_en = [en[i] for i, filtered_out in enumerate(filtering_indices_th_en) if not filtered_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# filtered_th 3202015\n",
      "# filtered_en 3202015\n"
     ]
    }
   ],
   "source": [
    "print('# filtered_th', len(filtered_th))\n",
    "print('# filtered_en', len(filtered_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3202604"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3281534 - 78930"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Filter by pair of source and target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 3.5652689933776855 s\n",
      "# sentences before filtered out 3202015\n",
      "# sentences filtered out 574\n",
      "# sentences after filtered out 3201441\n"
     ]
    }
   ],
   "source": [
    "\n",
    "th_en_tuples = [(filtered_th[i], filtered_en[i]) for i in range(0, len(filtered_th))]\n",
    "filtering_pairs_indices_th_en, _ = filter_sentence_pairs(th_en_tuples)\n",
    "\n",
    "filtered_pairs_th = [filtered_th[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if not filtered_out]\n",
    "filtered_pairs_en = [filtered_en[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if not filtered_out]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# th_en_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# filtered_pairs_th 3201441\n",
      "# filtered_pairs_en 3201441\n"
     ]
    }
   ],
   "source": [
    "print('# filtered_pairs_th', len(filtered_pairs_th))\n",
    "print('# filtered_pairs_en', len(filtered_pairs_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xe0\\xb9\\x82\\xc2\\x99\\xe0\\xb8\\x8a'\n"
     ]
    }
   ],
   "source": [
    "print('โช'.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter()\n",
    "for i in range(len(filtered_th)):\n",
    "    \n",
    "    if re.search('^โช', filtered_th[i]):\n",
    "        c['found'] += 1\n",
    "        if c['found'] <= 100:\n",
    "            print(i)\n",
    "            print(filtered_th[i])\n",
    "            print(filtered_en[i])\n",
    "            print()\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 67.09233093261719 s\n"
     ]
    }
   ],
   "source": [
    "cleaned_th = clean_sentences(filtered_pairs_th, lang=\"th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 27.437071800231934 s\n"
     ]
    }
   ],
   "source": [
    "cleaned_en = clean_sentences(filtered_pairs_en, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_th 3201441\n"
     ]
    }
   ],
   "source": [
    "print('cleaned_th', len(cleaned_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./sent.th', 'w') as f:\n",
    "    for sent in cleaned_th:\n",
    "        f.write(sent + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./sent.en', 'w') as f:\n",
    "    for sent in cleaned_en:\n",
    "        f.write(sent + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore cleaned sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "แสงดาว, ดาวสดใส ดาวแรกที่ฉันเห็นคืนนี้; <> Star light, star bright, first star I see tonight;\n",
      "\n",
      "มันจะเปิดพื้นที่ป่าไม้นี้ การพัฒนาซึ่งจะเป็นประโยชน์ต่อเศรษฐกิจ; <> and it would drive a harpoon right into the heart of the Communist concentration.\n",
      "\n",
      "\"จุดประสงค์ของเรื่องราวคือ to inflame ราคะ; ทั้งหมดจะถูกยินยอม <> \"The Great Hall will be adequately heated\n",
      "\n",
      "เรื่องราวของฉันจะสนใจท่าน... ...แด่ของเราโดยเฉพาะประธานาธิบดี; <> Enough. I am eager to hear Signora Maggi's voice\n",
      "\n",
      "ไม่คิดว่าน้ำตาของคุณ .ยับยั้ง my desire ; ที่เขาทำฉันไม่มีความเมตตามากกว่า <> Go on!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'found': 282})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter()\n",
    "for i, sent in enumerate(cleaned_th):\n",
    "    if ';' in sent:\n",
    "        c['found'] += 1\n",
    "        if c['found'] <= 5:\n",
    "            print(sent,'<>', cleaned_en[i])\n",
    "            print()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len th 3281534\n",
      "len filtered_th 3202015\n",
      "len filtered_pairs_th 3201441\n",
      "len cleaned_th 3201441\n",
      "\n",
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "c = Counter()\n",
    "print('len th', len(th))\n",
    "print('len filtered_th', len(filtered_th))\n",
    "print('len filtered_pairs_th', len(filtered_pairs_th))\n",
    "print('len cleaned_th', len(cleaned_th))\n",
    "print('')\n",
    "for i, sent in enumerate(cleaned_th):\n",
    "    if 'โช I' in sent:\n",
    "        c['found'] += 1\n",
    "        if c['found'] <= 5:\n",
    "            print(filtered_th[i])\n",
    "            print(filtered_pairs_th[i])\n",
    "            print(cleaned_th[i])\n",
    "            print(cleaned_en[i])\n",
    "            print()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3202027\n",
      "3202027\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_pairs_th))\n",
    "print(len(cleaned_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask my sisters - แม่! <> Ask my sisters\n",
      "\n",
      "นายทำสิ่งที่นายต้องทำ ลิงค์ <> \n",
      "\n",
      "ฉันเสียใจจริงๆ ที่ต้องบอกว่านายไม่ใช่ปัญหาอีกต่อไปแล้ว <> \n",
      "\n",
      "ทำไมมาข้างหลังล่ะ <> \n",
      "\n",
      "มีคนเกาะอยู่สองคน <> \n",
      "\n",
      "นิ่งไว้ <> \n",
      "\n",
      "ไปเร็ว เร็วๆๆ <> \n",
      "\n",
      "วิสท์เลอร์ตกไปแล้ว <> \n",
      "\n",
      "Missile Command - โมริโมโต้บอกว่า <> Missile Command\n",
      "\n",
      "โทษทีนะ เวลานี้ไม่เหมาะ <> \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'found': 201})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# important\n",
    "c = Counter()\n",
    "for i, sent in enumerate(cleaned_th):\n",
    "    if cleaned_en[i] in sent:\n",
    "        c['found'] += 1\n",
    "        if c['found'] <= 10:\n",
    "            print(sent,'<>', cleaned_en[i])\n",
    "            print()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# important\n",
    "c = Counter()\n",
    "for i, sent in enumerate(filtered_pairs_th):\n",
    "    if 'โช I ' in sent:\n",
    "        c['found'] += 1\n",
    "        if c['found'] <= 10:\n",
    "            print(sent,'<>', filtered_pairs_en[i])\n",
    "            print()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_worker(sentence, lang, trie):\n",
    "    \n",
    "    _tokenizer_newmm = partial(pythainlp.tokenize.word_tokenize, engine='newmm',\n",
    "                               keep_whitespace=False,\n",
    "                              custom_dict=(trie if trie != None else DEFAULT_DICT_TRIE))\n",
    "    return ' '.join(_tokenizer_newmm(sentence))\n",
    "  \n",
    "def tokenize_handler(sentences, lang, trie=None):\n",
    "    toks = []\n",
    "    p = Pool(6)\n",
    "    t = time()\n",
    "    _tokenize_worker = partial(tokenize_worker, lang=lang, trie=trie)\n",
    "    toks = p.map(_tokenize_worker, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    print('{} s'.format(time() -t))\n",
    "\n",
    "    return toks\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_spaced_tokens_to_file(data, folder_name, filename):\n",
    "    with open('/root/mt-opus/data/{}/{}'.format(folder_name, filename),'w') as f:\n",
    "        for item in data:\n",
    "            f.write(item + '\\n')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence filtering (th)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef83021ce54a47478f80cc983baa3d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3281534), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "102619\n",
      "sentence filtering (en)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae4b4e1732d40ea8f810f42a00f8266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3281534), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4560\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "print('sentence filtering (th)')\n",
    "indices_to_filter_out_th = sentences_filter(th, lang='th')\n",
    "print(len(indices_to_filter_out_th))\n",
    "\n",
    "print('sentence filtering (en)')\n",
    "indices_to_filter_out_en = sentences_filter(en, lang='en')\n",
    "\n",
    "print(len(indices_to_filter_out_en))\n",
    "\n",
    "indices_to_filter_out = indices_to_filter_out_th + indices_to_filter_out_en\n",
    "indices_to_filter_out = set(indices_to_filter_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93721\n"
     ]
    }
   ],
   "source": [
    "print(len(indices_to_filter_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean sentence (th)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fce21425d0b465fa558fa59b4894726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3281534), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "clean sentence (en)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6adb6da2d6b4dcca76e57ce8d9c8ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3281534), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "228.31016874313354 seconds\n"
     ]
    }
   ],
   "source": [
    "# Clean Sentence\n",
    "\n",
    "print('clean sentence (th)')\n",
    "filtered_th = [clean_sentence(x, lang='th') for i, x in tqdm_notebook(enumerate(th), total=len(th)) if i not in indices_to_filter_out]\n",
    "print('clean sentence (en)')\n",
    "filtered_en = [clean_sentence(x, lang='en') for i, x in tqdm_notebook(enumerate(en), total=len(en)) if i not in indices_to_filter_out]\n",
    "\n",
    "print('{} seconds'.format(time() -t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3281534\n"
     ]
    }
   ],
   "source": [
    "print(len(th))\n",
    "# print(len(filtered_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Have forgot ##\n",
      "{\\cHFFFFFF}## ลืม ##\n",
      "## Will be ##\n",
      "{\\cHFFFFFF}## จะ ##\n",
      "##And for bonny ##\n",
      "{\\cHFFFFFF}## และสำหรับ Bonny ##\n",
      "##Annie Laurie ##\n",
      "{\\cHFFFFFF}## แอนนี่ลอรี่ ##\n",
      "##I would lay ##\n",
      "{\\cHFFFFFF}## ฉันจะวาง ##\n",
      "# Me doon #\n",
      "{\\cHFFFFFF}# ฉัน Doon #\n",
      "#And dee ##\n",
      "{\\cHFFFFFF}#And Dee ##\n",
      "Writing's on the wall\n",
      "# Writing's on the wall #\n",
      "Very superstitious\n",
      "# Very superstitious #\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'th found': 18512})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore\n",
    "\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for idx, sent in enumerate(th):\n",
    "    \n",
    "    if re.search('#',sent):\n",
    "        counter['th found'] += 1\n",
    "        if counter['th found'] < 10:\n",
    "            print(en[idx])\n",
    "            print(sent)\n",
    "   \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = Counter()\n",
    "\n",
    "# for idx, sent in enumerate(th):\n",
    "    \n",
    "#     if re.search('เเ',sent):\n",
    "#         counter['th found'] += 1\n",
    "#         if counter['th found'] < 10:\n",
    "#             print(en[idx])\n",
    "#             print(sent)\n",
    "   \n",
    "# counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = Counter()\n",
    "\n",
    "# for idx, sent in enumerate(th):\n",
    "    \n",
    "#     if re.search(r'โช [A-z]', sent):\n",
    "#         counter['th found'] += 1\n",
    "#         if counter['th found'] < 10:\n",
    "#             print(en[idx])\n",
    "#             print(sent)\n",
    "#             print()\n",
    " \n",
    "\n",
    " \n",
    "# counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = Counter()\n",
    "\n",
    "# for idx, sent in enumerate(filtered_th):\n",
    "    \n",
    "#     if re.search('/ N',sent):\n",
    "#         counter['found;'] += 1\n",
    "#         if counter['found'] < 100:\n",
    "#             print(filtered_en[idx])\n",
    "#             print(sent)\n",
    "#             print()\n",
    "\n",
    "  \n",
    "# counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = Counter()\n",
    "\n",
    "# for idx, sent in enumerate(filtered_th):\n",
    "    \n",
    "#     if re.search(\"เเ\", sent):\n",
    "#         counter['th found ? '] += 1\n",
    "#         if counter['th found ? '] < 30:\n",
    "#             print(filtered_en[idx])\n",
    "#             print(sent)\n",
    "#             print()\n",
    " \n",
    " \n",
    "# counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_counter = {\n",
    "    'th': Counter(),\n",
    "    'en': Counter()\n",
    "}\n",
    "for i in range(len(filtered_th)):\n",
    "    for lang in ['th', 'en']:\n",
    "        if lang == 'th':\n",
    "            sentence_counter[lang][filtered_th[i]] += 1\n",
    "        if lang == 'en':\n",
    "            sentence_counter[lang][filtered_en[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ใช่', 10609),\n",
       " ('ไม่', 6691),\n",
       " ('โอเค', 6606),\n",
       " ('ขอบคุณ', 4760),\n",
       " ('เฮ้', 3365),\n",
       " ('อะไรนะ?', 3246),\n",
       " ('ครับ', 2709),\n",
       " ('อะไรนะ', 2664),\n",
       " ('อะไร?', 2601),\n",
       " ('ไม่!', 2097)]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_counter['th'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What?', 13268),\n",
       " ('Yeah.', 12373),\n",
       " ('No.', 10795),\n",
       " ('Okay.', 7995),\n",
       " ('Yes.', 6653),\n",
       " ('Thank you.', 6488),\n",
       " ('Hey.', 4278),\n",
       " ('No!', 3772),\n",
       " (\"I don't know.\", 3650),\n",
       " ('Why?', 3565)]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_counter['en'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โซ | Zo\n",
      "โซ | Zoe\n",
      "โซ | Zo?\n",
      "โซ | Zo?\n",
      "โซ | Zo?\n",
      "โซ | Zo?\n"
     ]
    }
   ],
   "source": [
    "print_only = 100\n",
    "count = 0\n",
    "for i in range(len(filtered_th)):\n",
    "    if filtered_th[i] == 'โซ' and count < print_only:\n",
    "        print(filtered_th[i], '|', filtered_en[i])\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "กรุณารอ.\n",
      "Wait, please.\n",
      "\n",
      "สวัสดี.\n",
      "Hello there.\n",
      "\n",
      "นั่นดีกว่า.\n",
      "That's better.\n",
      "\n",
      "สมเด็จพระราชินี.\n",
      "The Queen.\n",
      "\n",
      "ไปได้.\n",
      "Now, go.\n",
      "\n",
      "ใช่.\n",
      "Yes.\n",
      "\n",
      "มาสิ ชายไก่ ติดตามฉัน.\n",
      "Come on, hen... Men. Follow me.\n",
      "\n",
      "เงียบ.\n",
      "Quiet.\n",
      "\n",
      "อย่าปล่อยให้เขา หยุดเขา.\n",
      "Don't let him.Stop him.\n",
      "\n",
      "แค่นั้นแหละ.\n",
      "That's it.\n",
      "\n",
      "count 168365\n"
     ]
    }
   ],
   "source": [
    "print_only = 10\n",
    "count = 0\n",
    "for i in range(len(filtered_th)):\n",
    "    if re.search(r'\\.$', filtered_th[i]):\n",
    "        if count < print_only:\n",
    "            print(filtered_th[i])\n",
    "            print(filtered_en[i])\n",
    "            print()\n",
    "        count += 1\n",
    "print('count', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = {\n",
    "    'th': {\n",
    "        'sentencepiece': [],\n",
    "        'newmm':[]\n",
    "    },\n",
    "    'en': {\n",
    "        'sentencepiece': [],\n",
    "        'newmm':[]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a Segment texts into tokens with `newmm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.81156301498413 s\n",
      "39.77449178695679 s\n"
     ]
    }
   ],
   "source": [
    "toks['th']['newmm'] = tokenize_handler(filtered_th, lang='th')\n",
    "toks['en']['newmm'] = tokenize_handler(filtered_en, lang='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ทาส ใน กระจก วิเศษ , มาจาก พื้นที่ ที่ ไกล ที่สุด',\n",
       "  'ผ่าน ลม และ ความมืด ฉัน เรียก เจ้า',\n",
       "  'พูด !',\n",
       "  'ให้ ฉัน เห็น พระพักตร์ ของ พระองค์',\n",
       "  'สิ่ง ที่ เจ้า จะ รู้ ว่า สมเด็จ พระราชินี ของ ฉัน ได้ อย่างไร',\n",
       "  'กระจก วิเศษ บน ผนัง ผู้ ที่ เป็น สังขาร หนึ่ง ทั้งหมด หรือไม่',\n",
       "  'ที่ มีชื่อเสียง เป็น ความงาม ของ เจ้า พระ บาท สมเด็จ พระเจ้าอยู่หัว',\n",
       "  'แต่ ถือเป็น แม่บ้าน ที่ น่ารัก ที่ ฉัน เห็น',\n",
       "  'ยาจก ไม่ สามารถ ซ่อน พระคุณ อ่อนโยน ของ เธอ',\n",
       "  'อนิจจา เธอ มี ความเป็นธรรม มากขึ้น กว่า เจ้า'],\n",
       " ['Slave in the Magic Mirror , come from the farthest space .',\n",
       "  'Through wind and darkness , I summon thee .',\n",
       "  'Speak !',\n",
       "  'Let me see thy face .',\n",
       "  'What wouldst thou know , my Queen ?',\n",
       "  'Magic Mirror on the wall , who is the fairest one of all ?',\n",
       "  'Famed is thy beauty , Majesty .',\n",
       "  'But hold , a lovely maid I see .',\n",
       "  'Rags cannot hide her gentle grace .',\n",
       "  'Alas , she is more fair than thee .'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks['th']['newmm'][0:10], toks['en']['newmm'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b Segment texts into BPE tokens with SentencePiece (BPEmb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bpe(sentences, lang, n_vocab=25000):\n",
    "    \"\"\"Return a list of bpe tokens give a list of sentences\"\"\"\n",
    "    segmented_sentences = []\n",
    "    for sentence in tqdm_notebook(sentences, total=len(sentences)):\n",
    "#         print(sentence)\n",
    "        bpe_tokens = bpemb_pretrained[lang]['{}'.format(n_vocab)].encode(sentence)\n",
    "        segmented_sentences.append(' '.join(bpe_tokens))\n",
    "        \n",
    "    return segmented_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Thai language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce4eb5e8cbe469886307235a01f1dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3202751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['▁ท าส ใน กระจก วิเศษ , ▁มาจาก พื้นที่ ที่ ไกล ที่สุด', '▁ผ่าน ลม และความ มืด ฉัน เรียก เจ้า', '▁พูด !', '▁ให้ ฉัน เห็น พระพักตร์ ของ ▁พระองค์', '▁สิ่งที่ เจ้า จะ รู้ว่า สมเด็จพระราชินี ▁ของ ฉัน ได้อย่างไร', '▁กระจ ก วิเศษ บน ผนัง ▁ผู้ ที่เป็น สัง ขาร หนึ่ง ทั้งหมด ▁หรือไม่', '▁ที่มีชื่อเสียง เป็น ความงาม ของ ▁เจ้า พระบาทสมเด็จพระ เจ้าอยู่หัว', '▁แต่ ถือเป็น แม่ บ้าน ที่น ่ารัก ที่ ฉัน ▁เห็น', '▁ยา จก ไม่สามารถ ซ่อน พระคุณ ▁อ่อน โยน ของเธอ', '▁อน ิจ จา เธอ มีความเป็น ธรรม ▁มาก ขึ้น กว่า เจ้า']\n"
     ]
    }
   ],
   "source": [
    "toks['th']['sentencepiece'] = encode_bpe(filtered_th, 'th', 25000)\n",
    "\n",
    "print(toks['th']['sentencepiece'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 English language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0ac2a9e16c4fe0814a82c713b07a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3202751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['▁slave ▁in ▁the ▁magic ▁mirror , ▁come ▁from ▁the ▁fart hest ▁space .', '▁through ▁wind ▁and ▁darkness , ▁i ▁summon ▁the e .', '▁speak !', '▁let ▁me ▁see ▁thy ▁face .', '▁what ▁would st ▁thou ▁know , ▁my ▁queen ?', '▁magic ▁mirror ▁on ▁the ▁wall , ▁who ▁is ▁the ▁fa ire st ▁one ▁of ▁all ?', '▁famed ▁is ▁thy ▁beauty , ▁majesty .', '▁but ▁hold , ▁a ▁lov ely ▁maid ▁i ▁see .', '▁ra gs ▁cannot ▁hide ▁her ▁gentle ▁grace .', '▁al as , ▁she ▁is ▁more ▁fair ▁than ▁the e .']\n"
     ]
    }
   ],
   "source": [
    "toks['en']['sentencepiece']  = encode_bpe(filtered_en, 'en', 25000)\n",
    "print(toks['en']['sentencepiece'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split train-valid-test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N =  3202751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2562200, 320275, 320276)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train-valid-test split 80/10/10\n",
    "\n",
    "n = len(toks['th']['newmm'])\n",
    "\n",
    "print('N = ',n)\n",
    "idx = list(range(n))\n",
    "\n",
    "random.seed(1234) # Set SEED\n",
    "random.shuffle(idx)\n",
    "\n",
    "train_idx, valid_idx, test_idx = idx[:int(n*0.8)], idx[int(n*0.8):int(n*0.9)], idx[int(n*0.9):]\n",
    "\n",
    "dataset_split = {}\n",
    "dataset_split['train'] = train_idx\n",
    "dataset_split['valid'] = valid_idx\n",
    "dataset_split['test'] = test_idx\n",
    "\n",
    "\n",
    "len(train_idx),len(valid_idx),len(test_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    },\n",
    "    'valid': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    },\n",
    "    'test': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    for lang in ['th', 'en']:\n",
    "        for tok_type in ['sentencepiece', 'newmm']:\n",
    "\n",
    "            dataset[split_name][lang][tok_type] = [toks[lang][tok_type][i] for i in dataset_split[split_name]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['เบค กี้ เธอ ทำท่า แปลก ๆ เมื่อกี้ ใน ห้อง', 'อยู่ กับ เธอ แอน นา จะ นำทาง คุณ ผม จะ กลับ ไป'] \n",
      "\n",
      "['Becky , um , you were acting particularly strange in there just now .', \"Stay with her so Anna can guide you . I ' m going back .\"] \n",
      "\n",
      "['▁เบ ค กี้ ▁เธอ ทํา ท่า แปลก ๆ ▁เมื่อ กี้ ▁ในห้อง', '▁ อยู่กับ เธอ ▁แอนนา จะนํา ทาง คุณ ▁ผม จะ กลับไป'] \n",
      "\n",
      "['▁bec ky , ▁um , ▁you ▁were ▁acting ▁particularly ▁strange ▁in ▁there ▁just ▁now .', \"▁stay ▁with ▁her ▁so ▁anna ▁can ▁guide ▁you . ▁i ' m ▁going ▁back .\"] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['th']['newmm'][0:2],'\\n')\n",
    "print(dataset['train']['en']['newmm'][0:2],'\\n')\n",
    "print(dataset['train']['th']['sentencepiece'][0:2],'\\n')\n",
    "print(dataset['train']['en']['sentencepiece'][0:2],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'en_train_n_toks': 92383739, 'th_train_n_toks': 86683223, 'en_valid_n_toks': 11536351, 'en_test_n_toks': 11535798, 'th_test_n_toks': 10833242, 'th_valid_n_toks': 10826042})\n"
     ]
    }
   ],
   "source": [
    "# Counting number of tokens for train, valid, test\n",
    "counter = Counter( )\n",
    "for dataset_type in ['train', 'valid', 'test']:\n",
    "    for th_sent_toks in dataset[dataset_type]['th']['newmm']:\n",
    "        counter['th_{}_n_toks'.format(dataset_type)] += len(th_sent_toks)\n",
    "    for en_sent_toks in dataset[dataset_type]['en']['newmm']:\n",
    "        counter['en_{}_n_toks'.format(dataset_type)] += len(en_sent_toks)\n",
    "\n",
    "print(counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-sentencepiece/th-en\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-sentencepiece/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-sentencepiece/en-th\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-sentencepiece/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-newmm/th-en\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-newmm/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-newmm/en-th\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-newmm/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-sentencepiece/th-en\n",
      "dir: ../data/opensubtitles_bin/newmm-sentencepiece/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-sentencepiece/en-th\n",
      "dir: ../data/opensubtitles_bin/newmm-sentencepiece/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-newmm/th-en\n",
      "dir: ../data/opensubtitles_bin/newmm-newmm/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-newmm/en-th\n",
      "dir: ../data/opensubtitles_bin/newmm-newmm/en-th\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tok_type_src in ['sentencepiece', 'newmm']:\n",
    "    for tok_type_tgt in ['sentencepiece', 'newmm']:\n",
    "        langs = ['th', 'en']\n",
    "        for lang in langs:\n",
    "            src_lang = lang\n",
    "            tgt_lang = 'en' if lang =='th' else 'th'\n",
    "            FOLDER_NAME = \"opensubtitles_tok/{}-{}/{}-{}\".format(tok_type_src, tok_type_tgt, src_lang, tgt_lang )\n",
    "            FOLDER_NAME_BIN = \"opensubtitles_bin/{}-{}/{}-{}\".format(tok_type_src, tok_type_tgt, src_lang, tgt_lang)\n",
    "           \n",
    "            \n",
    "            # Create directories\n",
    "            print('create directories: ')\n",
    "            print('dir: ../data/{}'.format(FOLDER_NAME))\n",
    "            print('dir: ../data/{}'.format(FOLDER_NAME_BIN))\n",
    "\n",
    "            !mkdir -p ../data/{FOLDER_NAME}\n",
    "            !mkdir -p ../data/{FOLDER_NAME_BIN}\n",
    "\n",
    "            for split_name in ['train', 'valid', 'test']:\n",
    "                \n",
    "                write_spaced_tokens_to_file(dataset[split_name][src_lang][tok_type_src],\n",
    "                                            FOLDER_NAME, '{}.{}'.format(split_name, src_lang))\n",
    "                \n",
    "                write_spaced_tokens_to_file(dataset[split_name][tgt_lang][tok_type_tgt],\n",
    "                                            FOLDER_NAME, '{}.{}'.format(split_name, tgt_lang))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁bec ky , ▁um , ▁you ▁were ▁acting ▁particularly ▁strange ▁in ▁there ▁just ▁now .\n",
      "▁stay ▁with ▁her ▁so ▁anna ▁can ▁guide ▁you . ▁i ' m ▁going ▁back .\n",
      "▁look .\n",
      "▁oh , ▁no , ▁it ' s ▁the ▁other ▁way ▁around , ▁dr . ▁lewis .\n",
      "▁sort ▁of .\n",
      "▁bart ender , ▁something ▁really ▁strong , ▁please .\n",
      "▁yes , ▁obviously .\n",
      "▁la ' s ▁so ▁nice .\n",
      "▁i ' m ▁going ▁to ▁fix ▁it .\n",
      "▁i ▁get ▁b ored .\n"
     ]
    }
   ],
   "source": [
    "!head ../data/opensubtitles_tok/newmm-sentencepiece/th-en/train.en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "เบค กี้ เธอ ทำท่า แปลก ๆ เมื่อกี้ ใน ห้อง\n",
      "อยู่ กับ เธอ แอน นา จะ นำทาง คุณ ผม จะ กลับ ไป\n",
      "ฟัง นะ\n",
      "พอดี เลย ดร. ลี วิ ส\n",
      "แบบ ว่า\n",
      "เอ่อ บาร์ เท็น เด อร ์ ขอ อะไร ที่\n",
      "ก็ ใช่ ห น่ะ สิ\n",
      "แอลเอ สวย เนอะ\n",
      "ฉัน กำลังจะ แก้ ไขมัน\n",
      "ฉัน เบื่อ ละ\n"
     ]
    }
   ],
   "source": [
    "!head ../data/opensubtitles_tok/newmm-sentencepiece/th-en/train.th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
