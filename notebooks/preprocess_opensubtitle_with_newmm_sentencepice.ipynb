{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/haise/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/miniconda3/envs/haise/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/miniconda3/envs/haise/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/miniconda3/envs/haise/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/miniconda3/envs/haise/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/miniconda3/envs/haise/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/miniconda3/envs/haise/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/miniconda3/envs/haise/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/miniconda3/envs/haise/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/miniconda3/envs/haise/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/miniconda3/envs/haise/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/miniconda3/envs/haise/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import copy \n",
    "import re\n",
    "import html\n",
    "\n",
    "from time import time \n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import pythainlp\n",
    "from pythainlp.util import *\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.ulmfit import *\n",
    "\n",
    "# subword-nmt\n",
    "from subword_nmt import learn_bpe as learner\n",
    "from subword_nmt import apply_bpe as subword_tokenizer\n",
    "\n",
    "import fairseq \n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pythainlp.tokenize import DEFAULT_DICT_TRIE\n",
    "\n",
    "from pythainlp.corpus import thai_words\n",
    "\n",
    "print(pythainlp.__version__)\n",
    "# assert pythainlp.__version__ == '2.1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/Users/arta/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/Users/arta/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install BPEmb (BPE embeddings)\n",
    "\n",
    "!pip install --q bpemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USE\n",
    "\n",
    "# g = tf.Graph()\n",
    "# with g.as_default():\n",
    "#     text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "#     embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/1\")\n",
    "#     embedded_text = embed(text_input)\n",
    "#     init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "# g.finalize()\n",
    "\n",
    "# # def compute_similarity(src, tgt):\n",
    "# #     \"\"\"\n",
    "# #         Calculate sentence similarity based on Google Universal Sentence Encoder (Multilingual Large)\n",
    "# #     \"\"\"\n",
    "\n",
    "# # # Initialize session.\n",
    "# # session = tf.Session(graph=g)\n",
    "# # session.run(init_op)\n",
    "\n",
    "# # # Compute embeddings.\n",
    "# # en_result = session.run(embedded_text, feed_dict={text_input: english_sentences})\n",
    "# # it_result = session.run(embedded_text, feed_dict={text_input: italian_sentences})\n",
    "# # ja_result = session.run(embedded_text, feed_dict={text_input: japanese_sentences})\n",
    "\n",
    "# # # Compute similarity matrix. Higher score indicates greater similarity.\n",
    "# # similarity_matrix_it = np.inner(en_result, it_result)\n",
    "# # similarity_matrix_ja = np.inner(en_result, ja_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "\n",
    "bpemb_pretrained ={\n",
    "    'th': {\n",
    "        '25000': BPEmb(lang=\"th\", vs=25000)\n",
    "    },\n",
    "    'en': {\n",
    "        '25000': BPEmb(lang=\"en\", vs=25000)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3281534,\n",
       " ['Slave in the Magic Mirror, come from the farthest space.',\n",
       "  'Through wind and darkness, I summon thee.',\n",
       "  'Speak!'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/opensubtitle_v2018/OpenSubtitles.en-th.en','r', encoding='utf-8') as f:\n",
    "    en = f.read().split('\\n')\n",
    "len(en),en[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3281534,\n",
       " ['ทาสในกระจกวิเศษ, มาจากพื้นที่ที่ไกลที่สุด',\n",
       "  'ผ่านลมและความมืดฉันเรียกเจ้า',\n",
       "  'พูด!'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/opensubtitle_v2018/OpenSubtitles.en-th.th','r', encoding='utf-8') as f:\n",
    "    th = f.read().split('\\n')\n",
    "    \n",
    "len(th),th[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRule:    \n",
    "    def test(self, sentence, lang):\n",
    "        pass\n",
    "\n",
    "class ReplaceRule(BaseRule):   \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def replace(self, sentence, lang):\n",
    "        pass\n",
    "\n",
    "class UnescapeString(ReplaceRule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        return True\n",
    "\n",
    "    def replace(self, sentence, lang):\n",
    "        sentence = sentence.replace('\\\\\"', '\"')\n",
    "        return unescape_string(sentence)\n",
    "\n",
    "class ReplaceSymbolOccurenceRule(ReplaceRule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_list = ['', '', '​', '', '', '', '",
    "', '', 'โ', '​',\n",
    "                           '♪', '{\\ cHFFFFFF }', '\\cHFFFFFF', '§', 'font color = \"# 808080 \"',\n",
    "                          ' ##', '{\\\\cHFFFFFF}', '## ', ' ## ', '-# ', '# ', ' #', '\\ N', '\\ NI', ',8203;', '8203;', '#8203; ',\n",
    "                          '#8203;', '\\\\i1}', \"\\\\\\\\\\\\\\\\\", \"\\\\\\\\\\\\\", \"\\\\\\\\\", '\\\\ ', '\\\\', '{}']\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        for token in self.token_list:\n",
    "            if token in sentence:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def replace(self, sentence, lang):\n",
    "        for token in self.token_list:\n",
    "            sentence = sentence.replace(token, '')\n",
    "        return sentence\n",
    "\n",
    "class ReplaceHashtagInSentenceRule(ReplaceRule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if re.match(r\"[[^#]|[^##]]\", sentence):\n",
    "            return True\n",
    "        if re.search(r\"#$\", sentence):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def replace(self, sentence, lang):\n",
    "        sentence = re.sub(r\"^#+\", '', sentence).lstrip()\n",
    "        sentence = re.sub(r\"#$\", '', sentence).rstrip()\n",
    "\n",
    "        return sentence\n",
    "    \n",
    "\n",
    "class NormalizeThaiVowel(ReplaceRule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if 'เเ' in sentence and lang == 'th':\n",
    "            return True\n",
    "        if 'ใใ' in sentence and lang == 'th':\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def replace(self, sentence, lang):\n",
    "        sentence = re.sub(r\"เเ\", 'แ', sentence)\n",
    "        sentence = re.sub(r\"ใใ\", 'ใ', sentence)\n",
    "\n",
    "        return sentence\n",
    "    \n",
    "class ReplaceFullStopInThaiSentenceRule(ReplaceRule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if lang == 'th':\n",
    "            if re.search(r'\\.$', sentence):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def replace(self, sentence, lang):\n",
    "        sentence = re.sub(r\"\\.$\", '', sentence)\n",
    "        return sentence\n",
    "    \n",
    "    \n",
    "    \n",
    "class ReplaceDashInSentenceRule(ReplaceRule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if ' - ' in sentence:\n",
    "            return True\n",
    "        if '- ' in sentence:\n",
    "            return True\n",
    "        if ' -' in sentence:\n",
    "            return True\n",
    "        if re.search(r\"^[-]+\", sentence):\n",
    "            return True\n",
    "        if re.search(r\"[-]+$\", sentence):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def replace(self, sentence, lang):\n",
    "        sentence = re.sub(r\" - \", '', sentence)\n",
    "        sentence = re.sub(r\"- \", '', sentence)\n",
    "        sentence = re.sub(r\" -\", '', sentence)\n",
    "        sentence = re.sub(r\"^[-]+\", '', sentence) # start with space + \"-\" \n",
    "        sentence = re.sub(r\"[-]+$\", '', sentence) # end with space + \"-\" \n",
    "\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "กHello\n",
      "Hello\n",
      "Hello\n",
      "a-sad Hello\n",
      "กับใคร\n"
     ]
    }
   ],
   "source": [
    "def testReplaceDashInSentenceRule():\n",
    "    rule = ReplaceDashInSentenceRule()\n",
    "    assert rule.test('- ', lang=\"th\") == True\n",
    "    assert rule.test(' -', lang=\"th\") == True\n",
    "    assert rule.test(' - ', lang=\"th\") == True\n",
    "    assert rule.test('-กับใคร', lang=\"th\") == True\n",
    "\n",
    "    print(rule.replace('- Hello', lang=\"th\"))\n",
    "    print(rule.replace(' -กHello', lang=\"th\"))\n",
    "    print(rule.replace('Hello-', lang=\"th\"))\n",
    "    print(rule.replace('---- Hello- ', lang=\"th\"))\n",
    "    print(rule.replace('a-sad Hello--- ', lang=\"th\"))\n",
    "    print(rule.replace('-กับใคร', lang=\"th\"))\n",
    "\n",
    "testReplaceDashInSentenceRule()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "def testReplaceHashtagInSentenceRule():\n",
    "    rule = ReplaceHashtagInSentenceRule()\n",
    "    assert rule.test('#..', lang=\"th\") == True\n",
    "    assert rule.test('##..', lang=\"th\") == True\n",
    "    assert rule.test('.#', lang=\"th\") == True\n",
    "\n",
    "    print(rule.replace('#..', lang=\"th\"))\n",
    "    print(rule.replace('##..', lang=\"th\"))\n",
    "    print(rule.replace('#### ..', lang=\"th\"))\n",
    "    print(rule.replace('..', lang=\"th\"))\n",
    "    print(rule.replace('.#', lang=\"th\"))\n",
    "\n",
    "testReplaceHashtagInSentenceRule()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ฉัน\n"
     ]
    }
   ],
   "source": [
    "def testReplaceFullStopInThaiSentenceRule():\n",
    "    rule = ReplaceFullStopInThaiSentenceRule()\n",
    "    assert rule.test('ฉัน.', lang=\"th\") == True\n",
    "    print(rule.replace('ฉัน.', lang=\"th\"))\n",
    "\n",
    "        \n",
    "testReplaceFullStopInThaiSentenceRule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "แรกเกิด\n",
      "ให้\n"
     ]
    }
   ],
   "source": [
    "def testNormalizeThaiVowel():\n",
    "    rule = NormalizeThaiVowel()\n",
    "    assert rule.test('เเ', lang='th') == True\n",
    "    assert rule.test('ใใ', lang='th') == True\n",
    "\n",
    "    print(rule.replace('เเรกเกิด', lang='th'))\n",
    "    print(rule.replace('ใให้', lang='th'))\n",
    "\n",
    "testNormalizeThaiVowel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ContainsAdSymbol(BaseRule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if '@' in sentence:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "class NoThaiWordInThaiSentence(BaseRule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if lang == \"th\":\n",
    "            if countthai(sentence, ignore_chars='') == 0.0:\n",
    "                return True\n",
    "            if re.search(r'^โช [A-z]', sentence):\n",
    "                return True\n",
    "            if re.search(r'^โช\\s\\.', sentence):\n",
    "                return True\n",
    "            if re.search(r'^โช\\sโช', sentence):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "class ContainsTokensInThaiSentence(BaseRule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.list_of_tokens = ['โ#65533;', 'N#233;', '#/N#', 'ใใใ', '#']\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if lang == \"th\":\n",
    "            if re.search(r'\\d\\d; \\d\\d', sentence):\n",
    "                return True\n",
    "            for symbol in self.list_of_tokens:\n",
    "                if symbol in sentence:\n",
    "                    return True\n",
    "           \n",
    "        return False\n",
    "    \n",
    "class ContainsUnknownSymbols(BaseRule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.list_unknown_symbols = [ b'\\x98\\xc2', b'\\xae\\xc2', b'\\x99\\xc2',\n",
    "                                      b'\\xb1\\xc2', b'\\xc2\\xb7', b'\\xc2\\x8b',\n",
    "                                      b'\\xc3\\x83']\n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        for symbol in self.list_unknown_symbols:\n",
    "            if symbol in sentence.encode('utf-8'):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "class SentenceLengthLessThanOne(BaseRule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "    def test(self, sentence, lang):\n",
    "        if len(sentence) <= 1:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def testContainsAdSymbol():\n",
    "    rule = ContainsAdSymbol()\n",
    "    print(rule.test('@gmail', lang=\"th\"))\n",
    "    print(rule.test('gmasd', lang=\"th\"))\n",
    "testContainsAdSymbol()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc2\\x8b'\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def testContainsUnknownSymbols():\n",
    "    rule = ContainsUnknownSymbols()\n",
    "    print(''.encode('utf-8'))\n",
    "    print(rule.test('', lang=\"th\"))\n",
    "testContainsUnknownSymbols()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def testNoThaiWordInThaiSentence():\n",
    "    rule = NoThaiWordInThaiSentence()\n",
    "    print(rule.test('', lang=\"th\"))\n",
    "    print(rule.test('en', lang=\"th\"))\n",
    "    print(rule.test('โช a', lang=\"th\"))\n",
    "\n",
    "testNoThaiWordInThaiSentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_REPLACE_RULES = [ReplaceSymbolOccurenceRule,\n",
    "                         ReplaceDashInSentenceRule,\n",
    "                         ReplaceHashtagInSentenceRule,\n",
    "                         UnescapeString,\n",
    "                         NormalizeThaiVowel]\n",
    "\n",
    "DEFAULT_FILTER_OUT_RULES = [ContainsUnknownSymbols,\n",
    "                            ContainsTokensInThaiSentence,\n",
    "                            ContainsAdSymbol,\n",
    "                            NoThaiWordInThaiSentence,\n",
    "                            SentenceLengthLessThanOne]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text\n",
    "\n",
    "# LIST_OF_TOKENS_TO_REPLACE = ['', '', '​', '', '', '', '",
    "', '', 'โ', '​',\n",
    "#                    '♪', '{\\ cHFFFFFF }', '§', 'font color = \"# 808080 \"']\n",
    "def unescape_string(text):\n",
    "    return html.unescape(text)\n",
    "\n",
    "def sentences_filter(sentences, lang, rules=DEFAULT_FILTER_OUT_RULES):\n",
    "    indices = []\n",
    "    for index, sentence in tqdm_notebook(enumerate(sentences), total=len(sentences)):\n",
    "        \n",
    "        for rule in rules:\n",
    "            rule_obj = rule()\n",
    "            if rule_obj.test(sentence, lang=lang):\n",
    "                indices.append(index)\n",
    "    return indices\n",
    "\n",
    "def clean_sentence(sentence, lang, rules=DEFAULT_REPLACE_RULES):\n",
    "    for rule in rules:\n",
    "        \n",
    "        rule_obj = rule()\n",
    "        if rule_obj.test(sentence, lang=lang):\n",
    "            sentence = rule_obj.replace(sentence, lang=lang)\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_worker(sentence, lang, trie):\n",
    "    \n",
    "    _tokenizer_newmm = partial(pythainlp.tokenize.word_tokenize, engine='newmm',\n",
    "                               keep_whitespace=False,\n",
    "                              custom_dict=(trie if trie != None else DEFAULT_DICT_TRIE))\n",
    "    return ' '.join(_tokenizer_newmm(sentence))\n",
    "  \n",
    "def tokenize_handler(sentences, lang, trie=None):\n",
    "    toks = []\n",
    "    p = Pool(12)\n",
    "    t = time()\n",
    "    _tokenize_worker = partial(tokenize_worker, lang=lang, trie=trie)\n",
    "    toks = p.map(_tokenize_worker, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    print('{} s'.format(time() -t))\n",
    "\n",
    "    return toks\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_spaced_tokens_to_file(data, folder_name, filename):\n",
    "    with open('/root/mt-opus/data/{}/{}'.format(folder_name, filename),'w') as f:\n",
    "        for item in data:\n",
    "            f.write(item + '\\n')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence filtering (th)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef83021ce54a47478f80cc983baa3d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3281534), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "102619\n",
      "sentence filtering (en)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae4b4e1732d40ea8f810f42a00f8266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3281534), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4560\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "print('sentence filtering (th)')\n",
    "indices_to_filter_out_th = sentences_filter(th, lang='th')\n",
    "print(len(indices_to_filter_out_th))\n",
    "\n",
    "print('sentence filtering (en)')\n",
    "indices_to_filter_out_en = sentences_filter(en, lang='en')\n",
    "\n",
    "print(len(indices_to_filter_out_en))\n",
    "\n",
    "indices_to_filter_out = indices_to_filter_out_th + indices_to_filter_out_en\n",
    "indices_to_filter_out = set(indices_to_filter_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93721\n"
     ]
    }
   ],
   "source": [
    "print(len(indices_to_filter_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean sentence (th)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fce21425d0b465fa558fa59b4894726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3281534), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "clean sentence (en)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6adb6da2d6b4dcca76e57ce8d9c8ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3281534), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "228.31016874313354 seconds\n"
     ]
    }
   ],
   "source": [
    "# Clean Sentence\n",
    "\n",
    "print('clean sentence (th)')\n",
    "filtered_th = [clean_sentence(x, lang='th') for i, x in tqdm_notebook(enumerate(th), total=len(th)) if i not in indices_to_filter_out]\n",
    "print('clean sentence (en)')\n",
    "filtered_en = [clean_sentence(x, lang='en') for i, x in tqdm_notebook(enumerate(en), total=len(en)) if i not in indices_to_filter_out]\n",
    "\n",
    "print('{} seconds'.format(time() -t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3281534\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'filtered_th' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-60977eaca606>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_th\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'filtered_th' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(th))\n",
    "print(len(filtered_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last night, I dreamt I went to Manderley again.\n",
      "เมื่อคืนฉันฝันว่า ได้กลับไปที่เเมนเดอเลย์อีกครั้ง\n",
      "The drive wound away in front of me, twisting and turning as it had always done.\n",
      "ถนนเลื้อยคดเคี้ยวอยู่เบื้องหน้าของฉัน ยังคงคดโค้งเเละวกวนดั่งเช่นเคย\n",
      "Nature had come into her own again, and little by little had encroached upon the drive with long, tenacious fingers.\n",
      "ธรรมชาติได้กลับคืนสู่ตนเองอีกครั้ง ค่อยๆ ปกคลุมทางเข้าทีละน้อย ดั่งนิ้วมืออันเรียวยาวและเหนียวเเน่น\n",
      "On and on wound the poor thread that had once been our drive, and finally, there was Manderley.\n",
      "แผ่ปกคลุมทางเข้าอันซอมซ่อ ที่ครั้งหนึ่งเคยเป็นทางเดินรถของเรา ในที่สุดก็ถึงเเมนเดอเลย์\n",
      "Manderley, secretive and silent.\n",
      "เเมนเดอเลย์-- ลึกลับเเละเงียบงัน\n",
      "Time could not mar the perfect symmetry of those walls.\n",
      "กาลเวลามิอาจทําให้กําเเพง ที่ได้สมมาตรนี้ด่างพร้อยได้\n",
      "Moonlight can play odd tricks upon the fancy, and suddenly it seemed to me that light came from the windows.\n",
      "เเสงจันทร์อาจทําให้เกิดภาพลวงตา ทําให้ฉันเห็นว่ามีเเสงไฟ ลอดออกมาจากหน้าต่าง\n",
      "The illusion went with it.\n",
      "เเล้วภาพลวงตาก็มลายหายไป\n",
      "I looked upon a desolate shell with no whisper of the past about its staring walls.\n",
      "ฉันมองขึ้นไปยังโครงตึกอันรกร้าง ไร้ซึ่งเสียงกระซิบจากอดีต ขับขานถึงกําเเพงที่น่าขนลุกนั่น\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'th found': 11472})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter()\n",
    "\n",
    "for idx, sent in enumerate(th):\n",
    "    \n",
    "    if re.search('เเ',sent):\n",
    "        counter['th found'] += 1\n",
    "        if counter['th found'] < 10:\n",
    "            print(en[idx])\n",
    "            print(sent)\n",
    "   \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Tai-Sho. - Tai-Sho.\n",
      "ไทโช \\ไทโช\n",
      "\n",
      "♪ Get thee behind me, Satan\n",
      "โช Get thee behind me, Satan\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'th found': 2})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter()\n",
    "\n",
    "for idx, sent in enumerate(th):\n",
    "    \n",
    "    if re.search(r'โช [A-z]', sent):\n",
    "        counter['th found'] += 1\n",
    "        if counter['th found'] < 10:\n",
    "            print(en[idx])\n",
    "            print(sent)\n",
    "            print()\n",
    " \n",
    "\n",
    " \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter()\n",
    "\n",
    "for idx, sent in enumerate(filtered_th):\n",
    "    \n",
    "    if re.search('/ N',sent):\n",
    "        counter['found;'] += 1\n",
    "        if counter['found'] < 100:\n",
    "            print(filtered_en[idx])\n",
    "            print(sent)\n",
    "            print()\n",
    "\n",
    "  \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found b'\\xc2\\x8b'\n",
      "'Lf you know someone who has been bitten 'it is absolutely essential that you isolate them immediately.\n",
      "เธเธฑเนเธเธเนเธเธทเธญเธชเธดเนเธเธเธตเนเธเนเธณเธเธฒเธเธเธณเนเธเธเธญเธเนเธเธฒ เธเธกเนเธเธทเนเธญเธงเนเธฒเธเนเธงเธ 2 เธเธตเนเธฃเธ\n",
      "b'\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x91\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x87\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x84\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x97\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8a\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x94\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x87\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x97\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x95\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x8b\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x89\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x93\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x8b\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x92\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x93\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x80\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x82\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x87\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x80\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x82\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x92 \\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x9c\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x80\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x8a\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x97\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x87\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x92\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x8a\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x87\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x87 2 \\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x9b\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x95\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x83\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x81'\n",
      "--\n",
      "found b'\\xc2\\x8b'\n",
      "- Philip won't want to leave the house. - Philip isn't...\n",
      "เนเธเธฅเธตเธขเธเธเธดเธเนเธเนเธ\n",
      "b'\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x80\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x85\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x95\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x82\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x94\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x8a\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x94\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x9a\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x80\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x9b\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x8b\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x87'\n",
      "--\n",
      "found b'\\xc2\\x8b'\n",
      "You weren't easy to live with.\n",
      "เธฃเธญเนเธเธตเนเธขเธง\n",
      "b'\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x83\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x80\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x94\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x95\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x8b\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x82\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x87'\n",
      "--\n",
      "found b'\\xc2\\x8b'\n",
      "I'm telling you, it's a fuss over nothing.\n",
      "เธเธฑเธเนเธกเนเนเธเนเธกเธตเนเธเธญเธญเธขเธนเนเนเธเธเธฃเธฐเนเธเนเธฒเธเธตเน\n",
      "b'\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x89\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x91\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x84\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x84\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x94\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x89\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x95\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x80\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x98\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x82\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x83\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x83\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x90\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x80\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x9b\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x8b\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x92\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x95\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x88'\n",
      "--\n",
      "found b'\\xc2\\x8b'\n",
      "I guess we'll have to take the Jag.\n",
      "เนเธเธเธฐเนเธกเนเธฃเนเธญเธเธเธฐเธซเธเนเธญเธขเนเธซเธฃเธญ\n",
      "b'\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x90\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x84\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x83\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x89\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x87\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x8b\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x90\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8b\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x82\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x80\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8b\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x83\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d'\n",
      "--\n",
      "found b'\\xc2\\x8b'\n",
      "Christ!\n",
      "เธเธญเธเธเธตเนเธเธฑเธเธเธฐเธฃเธนเธเธเธดเธเธเธญเธเธเธฑเธ\n",
      "b'\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x95\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x95\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x89\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x89\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x91\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x90\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x83\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x94\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x8b\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x94\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x9b\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x82\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x87\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x89\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x91\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99'\n",
      "--\n",
      "found b'\\xc2\\x8b'\n",
      "This is about survival.\n",
      "เธเธงเธเธเธดเธชเนเธเธญเธฃเนเนเธกเนเธกเธฒเนเธเธฐเธเนเธญเธเนเธญเธเธเธตเนเธญเธตเธเนเธฅเธข\n",
      "b'\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x9e\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x87\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x8b\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x94\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8a\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x80\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x95\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x83\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x8c\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x84\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x92\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x95\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x90\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x95\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x89\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x87\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x94\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x95\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x89\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x95\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x80\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x85\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x82'\n",
      "--\n",
      "found b'\\xc2\\x8b'\n",
      "- We're with you. - I'm not going out there.\n",
      "เธเธญเธเธเนเธเธดเธเธญเธฒเธซเธฒเธฃเธเนเธฒเธเธซเธฅเธญเธเนเธเธเธเธเธฒเธข\n",
      "b'\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x9a\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x8b\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x8c\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x94\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x92\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8b\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x92\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x83\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x9c\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x92\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8b\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x85\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x94\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x84\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x9b\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x88\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x95\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x92\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x82'\n",
      "--\n",
      "found b'\\xc2\\x8b'\n",
      "- How do we get Philip out the car? - We haven't got time.\n",
      "เธเธญเธเธเธณเธเธฒเธเนเธเธซเนเธญเธเธเธฑเธเธเนเธฒเธกเธฑเนเธข\n",
      "b'\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x8a\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x9a\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x97\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x93\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x87\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x92\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x83\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x99\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8b\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x89\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x8d\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x87\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x8b\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x91\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xc2\\x9c\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x89\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x92\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x81\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x91\\xe0\\xb9\\x80\\xe0\\xb8\\x99\\xc2\\x89\\xe0\\xb9\\x80\\xe0\\xb8\\x98\\xe0\\xb8\\x82'\n",
      "--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'th found \\x0cn': 524})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter()\n",
    "# control_characters = [\n",
    "#     b'\\xc2\\x80',\n",
    "# b'\\xc2\\x81',\n",
    "# b'\\xc2\\x82',\n",
    "# b'\\xc2\\x83',\n",
    "# b'\\xc2\\x84',\n",
    "# b'\\xc2\\x85',\n",
    "# b'\\xc2\\x86',\n",
    "# b'\\xc2\\x87',\n",
    "# b'\\xc2\\x88',\n",
    "# b'\\xc2\\x89',\n",
    "# b'\\xc2\\x8a',\n",
    "# b'\\xc2\\x8b',\n",
    "# b'\\xc2\\x8c',\n",
    "# b'\\xc2\\x8d',\n",
    "# b'\\xc2\\x8e',\n",
    "# b'\\xc2\\x8f',\n",
    "# b'\\xc2\\x90',\n",
    "# b'\\xc2\\x91',\n",
    "# b'\\xc2\\x92',\n",
    "# b'\\xc2\\x93',\n",
    "# b'\\xc2\\x94',\n",
    "# b'\\xc2\\x95',\n",
    "# b'\\xc2\\x96',\n",
    "# b'\\xc2\\x97',\n",
    "# b'\\xc2\\x98',\n",
    "# b'\\xc2\\x99',\n",
    "# b'\\xc2\\x9a',\n",
    "# b'\\xc2\\x9b',\n",
    "# b'\\xc2\\x9c',\n",
    "# b'\\xc2\\x9d',\n",
    "# b'\\xc2\\x9e',\n",
    "# b'\\xc2\\x9f',\n",
    "# ]\n",
    "\n",
    "control_characters = [b'\\xc2\\x8b']\n",
    "for idx, sent in enumerate(th):\n",
    "    for token in control_characters:\n",
    "        if token in sent.encode('utf-8'):\n",
    "           \n",
    "            counter['th found \\fn'] += 1\n",
    "            if counter['th found \\fn'] < 10:\n",
    "                print('found', token)\n",
    "                print(en[idx])\n",
    "                print(sent)\n",
    "                print(sent.encode('utf-8'))\n",
    "                print('--')\n",
    "            break\n",
    " \n",
    " \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter()\n",
    "\n",
    "for idx, sent in enumerate(filtered_th):\n",
    "    \n",
    "    if re.search(\"เเ\", sent):\n",
    "        counter['th found ? '] += 1\n",
    "        if counter['th found ? '] < 30:\n",
    "            print(filtered_en[idx])\n",
    "            print(sent)\n",
    "            print()\n",
    " \n",
    " \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_counter = {\n",
    "    'th': Counter(),\n",
    "    'en': Counter()\n",
    "}\n",
    "for i in range(len(filtered_th)):\n",
    "    for lang in ['th', 'en']:\n",
    "        if lang == 'th':\n",
    "            sentence_counter[lang][filtered_th[i]] += 1\n",
    "        if lang == 'en':\n",
    "            sentence_counter[lang][filtered_en[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ใช่', 10609),\n",
       " ('ไม่', 6691),\n",
       " ('โอเค', 6606),\n",
       " ('ขอบคุณ', 4760),\n",
       " ('เฮ้', 3365),\n",
       " ('อะไรนะ?', 3246),\n",
       " ('ครับ', 2709),\n",
       " ('อะไรนะ', 2664),\n",
       " ('อะไร?', 2601),\n",
       " ('ไม่!', 2097)]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_counter['th'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What?', 13268),\n",
       " ('Yeah.', 12373),\n",
       " ('No.', 10795),\n",
       " ('Okay.', 7995),\n",
       " ('Yes.', 6653),\n",
       " ('Thank you.', 6488),\n",
       " ('Hey.', 4278),\n",
       " ('No!', 3772),\n",
       " (\"I don't know.\", 3650),\n",
       " ('Why?', 3565)]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_counter['en'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โซ | Zo\n",
      "โซ | Zoe\n",
      "โซ | Zo?\n",
      "โซ | Zo?\n",
      "โซ | Zo?\n",
      "โซ | Zo?\n"
     ]
    }
   ],
   "source": [
    "print_only = 100\n",
    "count = 0\n",
    "for i in range(len(filtered_th)):\n",
    "    if filtered_th[i] == 'โซ' and count < print_only:\n",
    "        print(filtered_th[i], '|', filtered_en[i])\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "กรุณารอ.\n",
      "Wait, please.\n",
      "\n",
      "สวัสดี.\n",
      "Hello there.\n",
      "\n",
      "นั่นดีกว่า.\n",
      "That's better.\n",
      "\n",
      "สมเด็จพระราชินี.\n",
      "The Queen.\n",
      "\n",
      "ไปได้.\n",
      "Now, go.\n",
      "\n",
      "ใช่.\n",
      "Yes.\n",
      "\n",
      "มาสิ ชายไก่ ติดตามฉัน.\n",
      "Come on, hen... Men. Follow me.\n",
      "\n",
      "เงียบ.\n",
      "Quiet.\n",
      "\n",
      "อย่าปล่อยให้เขา หยุดเขา.\n",
      "Don't let him.Stop him.\n",
      "\n",
      "แค่นั้นแหละ.\n",
      "That's it.\n",
      "\n",
      "count 168365\n"
     ]
    }
   ],
   "source": [
    "print_only = 10\n",
    "count = 0\n",
    "for i in range(len(filtered_th)):\n",
    "    if re.search(r'\\.$', filtered_th[i]):\n",
    "        if count < print_only:\n",
    "            print(filtered_th[i])\n",
    "            print(filtered_en[i])\n",
    "            print()\n",
    "        count += 1\n",
    "print('count', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = {\n",
    "    'th': {\n",
    "        'sentencepiece': [],\n",
    "        'newmm':[]\n",
    "    },\n",
    "    'en': {\n",
    "        'sentencepiece': [],\n",
    "        'newmm':[]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a Segment texts into tokens with `newmm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.81156301498413 s\n",
      "39.77449178695679 s\n"
     ]
    }
   ],
   "source": [
    "toks['th']['newmm'] = tokenize_handler(filtered_th, lang='th')\n",
    "toks['en']['newmm'] = tokenize_handler(filtered_en, lang='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ทาส ใน กระจก วิเศษ , มาจาก พื้นที่ ที่ ไกล ที่สุด',\n",
       "  'ผ่าน ลม และ ความมืด ฉัน เรียก เจ้า',\n",
       "  'พูด !',\n",
       "  'ให้ ฉัน เห็น พระพักตร์ ของ พระองค์',\n",
       "  'สิ่ง ที่ เจ้า จะ รู้ ว่า สมเด็จ พระราชินี ของ ฉัน ได้ อย่างไร',\n",
       "  'กระจก วิเศษ บน ผนัง ผู้ ที่ เป็น สังขาร หนึ่ง ทั้งหมด หรือไม่',\n",
       "  'ที่ มีชื่อเสียง เป็น ความงาม ของ เจ้า พระ บาท สมเด็จ พระเจ้าอยู่หัว',\n",
       "  'แต่ ถือเป็น แม่บ้าน ที่ น่ารัก ที่ ฉัน เห็น',\n",
       "  'ยาจก ไม่ สามารถ ซ่อน พระคุณ อ่อนโยน ของ เธอ',\n",
       "  'อนิจจา เธอ มี ความเป็นธรรม มากขึ้น กว่า เจ้า'],\n",
       " ['Slave in the Magic Mirror , come from the farthest space .',\n",
       "  'Through wind and darkness , I summon thee .',\n",
       "  'Speak !',\n",
       "  'Let me see thy face .',\n",
       "  'What wouldst thou know , my Queen ?',\n",
       "  'Magic Mirror on the wall , who is the fairest one of all ?',\n",
       "  'Famed is thy beauty , Majesty .',\n",
       "  'But hold , a lovely maid I see .',\n",
       "  'Rags cannot hide her gentle grace .',\n",
       "  'Alas , she is more fair than thee .'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks['th']['newmm'][0:10], toks['en']['newmm'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b Segment texts into BPE tokens with SentencePiece (BPEmb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bpe(sentences, lang, n_vocab=25000):\n",
    "    \"\"\"Return a list of bpe tokens give a list of sentences\"\"\"\n",
    "    segmented_sentences = []\n",
    "    for sentence in tqdm_notebook(sentences, total=len(sentences)):\n",
    "#         print(sentence)\n",
    "        bpe_tokens = bpemb_pretrained[lang]['{}'.format(n_vocab)].encode(sentence)\n",
    "        segmented_sentences.append(' '.join(bpe_tokens))\n",
    "        \n",
    "    return segmented_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Thai language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce4eb5e8cbe469886307235a01f1dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3202751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['▁ท าส ใน กระจก วิเศษ , ▁มาจาก พื้นที่ ที่ ไกล ที่สุด', '▁ผ่าน ลม และความ มืด ฉัน เรียก เจ้า', '▁พูด !', '▁ให้ ฉัน เห็น พระพักตร์ ของ ▁พระองค์', '▁สิ่งที่ เจ้า จะ รู้ว่า สมเด็จพระราชินี ▁ของ ฉัน ได้อย่างไร', '▁กระจ ก วิเศษ บน ผนัง ▁ผู้ ที่เป็น สัง ขาร หนึ่ง ทั้งหมด ▁หรือไม่', '▁ที่มีชื่อเสียง เป็น ความงาม ของ ▁เจ้า พระบาทสมเด็จพระ เจ้าอยู่หัว', '▁แต่ ถือเป็น แม่ บ้าน ที่น ่ารัก ที่ ฉัน ▁เห็น', '▁ยา จก ไม่สามารถ ซ่อน พระคุณ ▁อ่อน โยน ของเธอ', '▁อน ิจ จา เธอ มีความเป็น ธรรม ▁มาก ขึ้น กว่า เจ้า']\n"
     ]
    }
   ],
   "source": [
    "toks['th']['sentencepiece'] = encode_bpe(filtered_th, 'th', 25000)\n",
    "\n",
    "print(toks['th']['sentencepiece'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 English language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0ac2a9e16c4fe0814a82c713b07a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3202751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['▁slave ▁in ▁the ▁magic ▁mirror , ▁come ▁from ▁the ▁fart hest ▁space .', '▁through ▁wind ▁and ▁darkness , ▁i ▁summon ▁the e .', '▁speak !', '▁let ▁me ▁see ▁thy ▁face .', '▁what ▁would st ▁thou ▁know , ▁my ▁queen ?', '▁magic ▁mirror ▁on ▁the ▁wall , ▁who ▁is ▁the ▁fa ire st ▁one ▁of ▁all ?', '▁famed ▁is ▁thy ▁beauty , ▁majesty .', '▁but ▁hold , ▁a ▁lov ely ▁maid ▁i ▁see .', '▁ra gs ▁cannot ▁hide ▁her ▁gentle ▁grace .', '▁al as , ▁she ▁is ▁more ▁fair ▁than ▁the e .']\n"
     ]
    }
   ],
   "source": [
    "toks['en']['sentencepiece']  = encode_bpe(filtered_en, 'en', 25000)\n",
    "print(toks['en']['sentencepiece'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split train-valid-test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N =  3202751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2562200, 320275, 320276)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train-valid-test split 80/10/10\n",
    "\n",
    "n = len(toks['th']['newmm'])\n",
    "\n",
    "print('N = ',n)\n",
    "idx = list(range(n))\n",
    "\n",
    "random.seed(1234) # Set SEED\n",
    "random.shuffle(idx)\n",
    "\n",
    "train_idx, valid_idx, test_idx = idx[:int(n*0.8)], idx[int(n*0.8):int(n*0.9)], idx[int(n*0.9):]\n",
    "\n",
    "dataset_split = {}\n",
    "dataset_split['train'] = train_idx\n",
    "dataset_split['valid'] = valid_idx\n",
    "dataset_split['test'] = test_idx\n",
    "\n",
    "\n",
    "len(train_idx),len(valid_idx),len(test_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    },\n",
    "    'valid': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    },\n",
    "    'test': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    for lang in ['th', 'en']:\n",
    "        for tok_type in ['sentencepiece', 'newmm']:\n",
    "\n",
    "            dataset[split_name][lang][tok_type] = [toks[lang][tok_type][i] for i in dataset_split[split_name]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['เบค กี้ เธอ ทำท่า แปลก ๆ เมื่อกี้ ใน ห้อง', 'อยู่ กับ เธอ แอน นา จะ นำทาง คุณ ผม จะ กลับ ไป'] \n",
      "\n",
      "['Becky , um , you were acting particularly strange in there just now .', \"Stay with her so Anna can guide you . I ' m going back .\"] \n",
      "\n",
      "['▁เบ ค กี้ ▁เธอ ทํา ท่า แปลก ๆ ▁เมื่อ กี้ ▁ในห้อง', '▁ อยู่กับ เธอ ▁แอนนา จะนํา ทาง คุณ ▁ผม จะ กลับไป'] \n",
      "\n",
      "['▁bec ky , ▁um , ▁you ▁were ▁acting ▁particularly ▁strange ▁in ▁there ▁just ▁now .', \"▁stay ▁with ▁her ▁so ▁anna ▁can ▁guide ▁you . ▁i ' m ▁going ▁back .\"] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['th']['newmm'][0:2],'\\n')\n",
    "print(dataset['train']['en']['newmm'][0:2],'\\n')\n",
    "print(dataset['train']['th']['sentencepiece'][0:2],'\\n')\n",
    "print(dataset['train']['en']['sentencepiece'][0:2],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'en_train_n_toks': 92383739, 'th_train_n_toks': 86683223, 'en_valid_n_toks': 11536351, 'en_test_n_toks': 11535798, 'th_test_n_toks': 10833242, 'th_valid_n_toks': 10826042})\n"
     ]
    }
   ],
   "source": [
    "# Counting number of tokens for train, valid, test\n",
    "counter = Counter( )\n",
    "for dataset_type in ['train', 'valid', 'test']:\n",
    "    for th_sent_toks in dataset[dataset_type]['th']['newmm']:\n",
    "        counter['th_{}_n_toks'.format(dataset_type)] += len(th_sent_toks)\n",
    "    for en_sent_toks in dataset[dataset_type]['en']['newmm']:\n",
    "        counter['en_{}_n_toks'.format(dataset_type)] += len(en_sent_toks)\n",
    "\n",
    "print(counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-sentencepiece/th-en\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-sentencepiece/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-sentencepiece/en-th\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-sentencepiece/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-newmm/th-en\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-newmm/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-newmm/en-th\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-newmm/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-sentencepiece/th-en\n",
      "dir: ../data/opensubtitles_bin/newmm-sentencepiece/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-sentencepiece/en-th\n",
      "dir: ../data/opensubtitles_bin/newmm-sentencepiece/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-newmm/th-en\n",
      "dir: ../data/opensubtitles_bin/newmm-newmm/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-newmm/en-th\n",
      "dir: ../data/opensubtitles_bin/newmm-newmm/en-th\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tok_type_src in ['sentencepiece', 'newmm']:\n",
    "    for tok_type_tgt in ['sentencepiece', 'newmm']:\n",
    "        langs = ['th', 'en']\n",
    "        for lang in langs:\n",
    "            src_lang = lang\n",
    "            tgt_lang = 'en' if lang =='th' else 'th'\n",
    "            FOLDER_NAME = \"opensubtitles_tok/{}-{}/{}-{}\".format(tok_type_src, tok_type_tgt, src_lang, tgt_lang )\n",
    "            FOLDER_NAME_BIN = \"opensubtitles_bin/{}-{}/{}-{}\".format(tok_type_src, tok_type_tgt, src_lang, tgt_lang)\n",
    "           \n",
    "            \n",
    "            # Create directories\n",
    "            print('create directories: ')\n",
    "            print('dir: ../data/{}'.format(FOLDER_NAME))\n",
    "            print('dir: ../data/{}'.format(FOLDER_NAME_BIN))\n",
    "\n",
    "            !mkdir -p ../data/{FOLDER_NAME}\n",
    "            !mkdir -p ../data/{FOLDER_NAME_BIN}\n",
    "\n",
    "            for split_name in ['train', 'valid', 'test']:\n",
    "                \n",
    "                write_spaced_tokens_to_file(dataset[split_name][src_lang][tok_type_src],\n",
    "                                            FOLDER_NAME, '{}.{}'.format(split_name, src_lang))\n",
    "                \n",
    "                write_spaced_tokens_to_file(dataset[split_name][tgt_lang][tok_type_tgt],\n",
    "                                            FOLDER_NAME, '{}.{}'.format(split_name, tgt_lang))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁bec ky , ▁um , ▁you ▁were ▁acting ▁particularly ▁strange ▁in ▁there ▁just ▁now .\n",
      "▁stay ▁with ▁her ▁so ▁anna ▁can ▁guide ▁you . ▁i ' m ▁going ▁back .\n",
      "▁look .\n",
      "▁oh , ▁no , ▁it ' s ▁the ▁other ▁way ▁around , ▁dr . ▁lewis .\n",
      "▁sort ▁of .\n",
      "▁bart ender , ▁something ▁really ▁strong , ▁please .\n",
      "▁yes , ▁obviously .\n",
      "▁la ' s ▁so ▁nice .\n",
      "▁i ' m ▁going ▁to ▁fix ▁it .\n",
      "▁i ▁get ▁b ored .\n"
     ]
    }
   ],
   "source": [
    "!head ../data/opensubtitles_tok/newmm-sentencepiece/th-en/train.en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "เบค กี้ เธอ ทำท่า แปลก ๆ เมื่อกี้ ใน ห้อง\n",
      "อยู่ กับ เธอ แอน นา จะ นำทาง คุณ ผม จะ กลับ ไป\n",
      "ฟัง นะ\n",
      "พอดี เลย ดร. ลี วิ ส\n",
      "แบบ ว่า\n",
      "เอ่อ บาร์ เท็น เด อร ์ ขอ อะไร ที่\n",
      "ก็ ใช่ ห น่ะ สิ\n",
      "แอลเอ สวย เนอะ\n",
      "ฉัน กำลังจะ แก้ ไขมัน\n",
      "ฉัน เบื่อ ละ\n"
     ]
    }
   ],
   "source": [
    "!head ../data/opensubtitles_tok/newmm-sentencepiece/th-en/train.th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
