{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/saiko/miniconda3/envs/saiko/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# import sys\n",
    "# sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import copy \n",
    "import re\n",
    "import html\n",
    "import numpy as np\n",
    "\n",
    "from time import time \n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import pythainlp\n",
    "from pythainlp.util import *\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.ulmfit import *\n",
    "\n",
    "# subword-nmt\n",
    "from subword_nmt import learn_bpe as learner\n",
    "from subword_nmt import apply_bpe as subword_tokenizer\n",
    "\n",
    "import fairseq \n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pythainlp.tokenize import DEFAULT_DICT_TRIE\n",
    "\n",
    "from pythainlp.corpus import thai_words\n",
    "\n",
    "print(pythainlp.__version__)\n",
    "# assert pythainlp.__version__ == '2.1'\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import sentencepiece as spm\n",
    "import tf_sentencepiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install BPEmb (BPE embeddings)\n",
    "\n",
    "# !pip install --q bpemb emoji subword-nmt fairseq tensorflow_hub sentencepiece tf_sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# Universal Sentence Encoder\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/1\")\n",
    "    embedded_text = embed(text_input)\n",
    "    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "g.finalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_similarity(src_sent, tgt_sent):\n",
    "    \"\"\"\n",
    "        Calculate sentence similarity based on Google Universal Sentence Encoder (Multilingual Large)\n",
    "    \"\"\"\n",
    "    session = tf.Session(graph=g)\n",
    "    session.run(init_op)\n",
    "    \n",
    "    src_result = session.run(embedded_text, feed_dict={text_input: [src_sent]})\n",
    "    tgt_result = session.run(embedded_text, feed_dict={text_input: [tgt_sent]})\n",
    "    \n",
    "    return np.inner(src_result, tgt_result).reshape(1)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8449457\n",
      "0.64079666\n",
      "0.7397201\n",
      "0.7669394\n"
     ]
    }
   ],
   "source": [
    "print(compute_similarity(\"ฉันชื่อยีน\",\"My Name is Gene\"))\n",
    "print(compute_similarity(\"ฉันชื่อยีน\",\"My Name is Jane\"))\n",
    "print(compute_similarity(\"ฉันชื่อยีน\",\"My Name is Jeans\"))\n",
    "print(compute_similarity(\"ฉันชื่อยีน\",\"My Name is Jean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "\n",
    "bpemb_pretrained ={\n",
    "    'th': {\n",
    "        '25000': BPEmb(lang=\"th\", vs=25000)\n",
    "    },\n",
    "    'en': {\n",
    "        '25000': BPEmb(lang=\"en\", vs=25000)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3281534,\n",
       " ['Slave in the Magic Mirror, come from the farthest space.',\n",
       "  'Through wind and darkness, I summon thee.',\n",
       "  'Speak!'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/opensubtitle_v2018/OpenSubtitles.en-th.en','r', encoding='utf-8') as f:\n",
    "    en = f.read().split('\\n')\n",
    "len(en),en[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3281534,\n",
       " ['ทาสในกระจกวิเศษ, มาจากพื้นที่ที่ไกลที่สุด',\n",
       "  'ผ่านลมและความมืดฉันเรียกเจ้า',\n",
       "  'พูด!'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/opensubtitle_v2018/OpenSubtitles.en-th.th','r', encoding='utf-8') as f:\n",
    "    th = f.read().split('\\n')\n",
    "    \n",
    "len(th),th[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Opensubtitles_v2018\n",
    "\n",
    "\n",
    "## 1.1 Text cleaning, filtering out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt_opus.preprocess import (\n",
    "    SentenceLengthLessThanOrEqualToOne,\n",
    "    SentenceContainsUnknownSymbol,\n",
    "    SentenceContainsAdSymbol,\n",
    "    ThaiSentenceContainsNoThaiCharacters,\n",
    "    ThaiSentenceContainsNoThaiCharactersPattern,\n",
    "    SentenceContainsOnlyAsterisk,\n",
    "    \n",
    "    UnescapeString,\n",
    "    RemoveUnwantedSymbols,\n",
    "    RemoveUnwantedPattern,\n",
    "    ReplaceDashInSentence,\n",
    "    RemoveHashtagInSentence,\n",
    "    RemoveFullStopInThaiSentence,\n",
    "    NormalizeThaiVowel,\n",
    "    ReplaceAsteriskInSentence,\n",
    "    RemoveColonInSentence,\n",
    "    RemoveSemiColonInSentence,\n",
    "    FormatTime,\n",
    "    \n",
    "    SentencePairFoundRepeatedText,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to filter out a sentence pair\n",
    "filtering_rules = [\n",
    "    SentenceLengthLessThanOrEqualToOne,\n",
    "    SentenceContainsAdSymbol,\n",
    "    SentenceContainsUnknownSymbol,\n",
    "    ThaiSentenceContainsNoThaiCharacters,\n",
    "    ThaiSentenceContainsNoThaiCharactersPattern,\n",
    "    SentenceContainsOnlyAsterisk,\n",
    "]\n",
    "\n",
    "cleaning_rules = [\n",
    "    UnescapeString,\n",
    "    RemoveUnwantedSymbols,\n",
    "    RemoveUnwantedPattern,\n",
    "    ReplaceDashInSentence,\n",
    "    RemoveHashtagInSentence,\n",
    "    RemoveFullStopInThaiSentence,\n",
    "    NormalizeThaiVowel,\n",
    "    ReplaceDashInSentence,\n",
    "    ReplaceAsteriskInSentence,\n",
    "    RemoveColonInSentence,\n",
    "    RemoveSemiColonInSentence,\n",
    "    FormatTime,\n",
    "]\n",
    "\n",
    "filtering_sentence_pair_rules = [\n",
    "    SentencePairFoundRepeatedText\n",
    "]\n",
    "\n",
    "filtering_rules_post = [\n",
    "    SentenceLengthLessThanOrEqualToOne,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could be parallelized\n",
    "def filter_sentence(sentence, lang, rules=filtering_rules):\n",
    "    \"\"\"\n",
    "        Return True if a sentence match filtering pattern\n",
    "    \"\"\"\n",
    "    for rule in rules:\n",
    "        rule_obj = rule()\n",
    "        if rule_obj.test(sentence, lang=lang):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filter_sentences(sentences, lang, rules=filtering_rules):\n",
    "    \"\"\"\n",
    "        Returns a list of Boolean value, if such element is True, it means filter that sentence pair.\n",
    "        Otherwise, keep that sentnence pair.\n",
    "    \"\"\"\n",
    "    filtering_indices = []\n",
    "    p = Pool() # use all available cores\n",
    "    t = time()\n",
    "\n",
    "    _filter_sentence = partial(filter_sentence, lang=lang, rules=rules)\n",
    "    filtering_indices = p.map(_filter_sentence, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    filtering_indices_np = np.array(filtering_indices)\n",
    "    number_of_filtered_out = np.sum(filtering_indices_np)\n",
    "    \n",
    "    print('Time taken: {} s'.format(time() -t))\n",
    "    print('# sentences ({}) before filtered out'.format(lang), len(sentences))\n",
    "    print('# sentences ({}) filtered out'.format(lang), number_of_filtered_out)\n",
    "    print('# sentences ({}) after filtered out'.format(lang), len(sentences) - number_of_filtered_out)\n",
    "\n",
    "    return filtering_indices_np, number_of_filtered_out\n",
    "\n",
    "\n",
    "def filter_sentence_pair(sentence_pair, rules=filtering_sentence_pair_rules):\n",
    "    \"\"\"\n",
    "        Return True if a sentence match filtering sentence pair pattern\n",
    "    \"\"\"\n",
    "    for rule in rules:\n",
    "        rule_obj = rule()\n",
    "        if rule_obj.test(sentence_pair):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filter_sentence_pairs(sentence_pairs, rules=filtering_sentence_pair_rules):\n",
    "    \"\"\"\n",
    "        Returns a list of Boolean value, if such element is True, it means filter that sentence pair.\n",
    "        Otherwise, keep that sentnence pair.\n",
    "    \"\"\"\n",
    "    filtering_indices = []\n",
    "    p = Pool() # use all available cores\n",
    "    t = time()\n",
    "\n",
    "    _filter_sentence_pair = partial(filter_sentence_pair, rules=rules)\n",
    "    filtering_indices = p.map(_filter_sentence_pair, sentence_pairs)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    filtering_indices_np = np.array(filtering_indices)\n",
    "    number_of_filtered_out = np.sum(filtering_indices_np)\n",
    "    \n",
    "    print('Time taken: {} s'.format(time() -t))\n",
    "    print('# sentences before filtered out', len(sentence_pairs))\n",
    "    print('# sentences filtered out', number_of_filtered_out)\n",
    "    print('# sentences after filtered out', len(sentence_pairs) - number_of_filtered_out)\n",
    "\n",
    "    return filtering_indices_np, number_of_filtered_out\n",
    "\n",
    "\n",
    "def clean_sentence(sentence, lang, rules=cleaning_rules):\n",
    "    for rule in rules: \n",
    "        rule_obj = rule()\n",
    "        if rule_obj.test(sentence, lang=lang):\n",
    "            sentence = rule_obj.replace(sentence, lang=lang)\n",
    "    return sentence\n",
    "\n",
    "def clean_sentences(sentences, lang, rules=cleaning_rules):\n",
    "    \"\"\"\n",
    "        Clean the sentence with the specified text cleaning rules\n",
    "        Return a list of cleaned sentences\n",
    "    \"\"\"\n",
    "    p = Pool() # use all available cores\n",
    "    t = time()\n",
    "    \n",
    "    _clean_sentence = partial(clean_sentence, lang=lang, rules=rules)\n",
    "    cleaned_sentences = p.map(_clean_sentence, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    print('Time taken: {} s'.format(time() -t))\n",
    "    \n",
    "    return cleaned_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Filter by each language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 24.470014095306396 s\n",
      "# sentences (th) before filtered out 3281534\n",
      "# sentences (th) filtered out 77301\n",
      "# sentences (th) after filtered out 3204233\n"
     ]
    }
   ],
   "source": [
    "filtering_indices_th, _ = filter_sentences(th, lang='th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 9.293501853942871 s\n",
      "# sentences (en) before filtered out 3281534\n",
      "# sentences (en) filtered out 4577\n",
      "# sentences (en) after filtered out 3276957\n"
     ]
    }
   ],
   "source": [
    "filtering_indices_en, _ = filter_sentences(en, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# filtering_indices_th_en 79608\n"
     ]
    }
   ],
   "source": [
    "filtering_indices_th_en = filtering_indices_th | filtering_indices_en\n",
    "\n",
    "print('# filtering_indices_th_en', sum(filtering_indices_th_en))\n",
    "\n",
    "filtered_th = [th[i] for i, filtered_out in enumerate(filtering_indices_th_en) if not filtered_out]\n",
    "filtered_en = [en[i] for i, filtered_out in enumerate(filtering_indices_th_en) if not filtered_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# filtered_th 3201926\n",
      "# filtered_en 3201926\n"
     ]
    }
   ],
   "source": [
    "print('# filtered_th', len(filtered_th))\n",
    "print('# filtered_en', len(filtered_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3202604"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3281534 - 78930"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Filter by pair of source and target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 3.9197959899902344 s\n",
      "# sentences before filtered out 3201926\n",
      "# sentences filtered out 573\n",
      "# sentences after filtered out 3201353\n"
     ]
    }
   ],
   "source": [
    "\n",
    "th_en_tuples = [(filtered_th[i], filtered_en[i]) for i in range(0, len(filtered_th))]\n",
    "filtering_pairs_indices_th_en, _ = filter_sentence_pairs(th_en_tuples)\n",
    "\n",
    "filtered_pairs_th = [filtered_th[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if not filtered_out]\n",
    "filtered_pairs_en = [filtered_en[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if not filtered_out]\n",
    "sentence_paris_filted_out = [th_en_tuples[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if filtered_out]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# th_en_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# filtered_pairs_th 3201353\n",
      "# filtered_pairs_en 3201353\n"
     ]
    }
   ],
   "source": [
    "print('# filtered_pairs_th', len(filtered_pairs_th))\n",
    "print('# filtered_pairs_en', len(filtered_pairs_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in filtered_pairs_en:\n",
    "    if len(sent) <= 1:\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 110.19039797782898 s\n"
     ]
    }
   ],
   "source": [
    "cleaned_th = clean_sentences(filtered_pairs_th, lang=\"th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 76.6123218536377 s\n"
     ]
    }
   ],
   "source": [
    "cleaned_en = clean_sentences(filtered_pairs_en, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_th 3201353\n"
     ]
    }
   ],
   "source": [
    "print('cleaned_th', len(cleaned_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore cleaned sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ก <> A\n",
      "\n",
      "ข <> B\n",
      "\n",
      "ค <> C\n",
      "\n",
      "ง <> D\n",
      "\n",
      "ช <> I-i do know you.\n",
      "\n",
      "ม <> Look!\n",
      "\n",
      "ว <> Aw\n",
      "\n",
      "ม <> Brown!\n",
      "\n",
      "ค <> Brap.\n",
      "\n",
      "ส <> St\n",
      "\n",
      "ช <> Rich.\n",
      "\n",
      "ม <> No.\n",
      "\n",
      "บ <> VP Valet.\n",
      "\n",
      "ด <> W\n",
      "\n",
      "ฌ <> Jean.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'found': 17})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter()\n",
    "for i, sent in enumerate(cleaned_th):\n",
    "    if len(sent) <= 1:\n",
    "        c['found'] += 1\n",
    "        if c['found'] <= 15:\n",
    "            print(sent,'<>', cleaned_en[i])\n",
    "            print()\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Post-filtering out rule\n",
    "\n",
    "- redo the filtering rules for 1) each langauge individually and 2) a language pair again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 3.007598876953125 s\n",
      "# sentences (th) before filtered out 3201353\n",
      "# sentences (th) filtered out 17\n",
      "# sentences (th) after filtered out 3201336\n",
      "Time taken: 1.608112096786499 s\n",
      "# sentences (en) before filtered out 3201353\n",
      "# sentences (en) filtered out 239\n",
      "# sentences (en) after filtered out 3201114\n"
     ]
    }
   ],
   "source": [
    "filtering_post_indices_th, _ = filter_sentences(cleaned_th, lang='th', rules=filtering_rules_post)\n",
    "filtering_post_indices_en, _ = filter_sentences(cleaned_en, lang='en', rules=filtering_rules_post)\n",
    "\n",
    "filtering_post_indices_th_en = filtering_post_indices_th | filtering_post_indices_en\n",
    "\n",
    "\n",
    "cleaned_and_filtered_th = [cleaned_th[i] for i, filtered_out in enumerate(filtering_post_indices_th_en) if not filtered_out]\n",
    "cleaned_and_filtered_en = [cleaned_en[i] for i, filtered_out in enumerate(filtering_post_indices_th_en) if not filtered_out]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 4.035604000091553 s\n",
      "# sentences before filtered out 3201102\n",
      "# sentences filtered out 137\n",
      "# sentences after filtered out 3200965\n"
     ]
    }
   ],
   "source": [
    "th_en_tuples = [(cleaned_and_filtered_th[i], cleaned_and_filtered_en[i]) for i in range(0, len(cleaned_and_filtered_th))]\n",
    "filtering_pairs_indices_th_en, _ = filter_sentence_pairs(th_en_tuples)\n",
    "\n",
    "cleaned_and_filtered_th = [cleaned_and_filtered_th[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if not filtered_out]\n",
    "cleaned_and_filtered_en = [cleaned_and_filtered_en[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if not filtered_out]\n",
    "sentence_paris_filted_out = [th_en_tuples[i] for i, filtered_out in enumerate(filtering_pairs_indices_th_en) if filtered_out]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After 4 steps\n",
    "\n",
    "# number of sentence pairs is 3,201,120\n",
    "# number of sentence pairs is 3,200,973 (Add SentenceContainsAsterink rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threashold 0.001\n",
      "count: 0\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.006\n",
      "ภารกิจก็ดำเนินไปด้วยดี ระบบทั้งหมดก็ดูจะดีด้วย\n",
      "Well, the mission was going perfectly, like a textbook. They reached safe distance using conventional thrusters. All systems looked good.\n",
      "\n",
      "ฉันจะซื้อของหวานกับไวน์ติดมือไป นายจะบอกว่าฉันไม่น่าซื้อ และ\n",
      "I'll bring the dessert and a bottle of wine and you'll tell me I shouldn't have and while you're showing me around your house shaped like a ship your wife will be cooking a turkey\n",
      "\n",
      "ยังดี,รถของเขายังอยู่นี่. -เขาใช้รถรางไฟฟ้า. -อะไรน่ะ?\n",
      "After careful review of evidence retrieved at the scene, we have concluded this was a deliberate act of terrorism, and the President concurs with our assessment.\n",
      "\n",
      "พูดมาเถอะ ไม่ต้องห่วง ฉันไม่จับเขาเข้าคุกหรอก\n",
      "Although I may guess out a little, but still you have say out how your system works. I'm really interested in the underground society.\n",
      "\n",
      "คุณ... รู้ได้ยังไงว่าผม... ที่จริงนายไปที่บ่อน้ำพุขอพร\n",
      "how  how did you know that i was... you actually walked up to a wishing well, dropped a dime, and wished to be invisible so you could spy on women in the shower?\n",
      "\n",
      "count: 27\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.011\n",
      "นะ \"กำลังจะเกิดขึ้นกับเรา\n",
      "To hear that beetle talk, you'd think somethin' was going to happen to us.\n",
      "\n",
      "ใส่สิครับ ไม่ประมาทไว้ก่อน\n",
      "Yes, certainly, certainly, certainly. You can't be too careful with children.\n",
      "\n",
      "เอายังงี้ เธอให้สร้อยข้อมือมา..\n",
      "Tell you what... you give me the bracelet... and I'll show you the way out of the labyrinth.\n",
      "\n",
      "ระวังตัวกันด้วยล่ะ ไฟท์ติ้ง\n",
      "We at Channel 4, like you, have been stirred by the courage and the humanity....\n",
      "\n",
      "เราต้องหลบออกทางท่อระบาย\n",
      "We're getting out through the sewers. I have to check if they're clear.\n",
      "\n",
      "count: 290\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.016\n",
      "แขนของฉัน มันเจ็บ\n",
      "Lift my hand to the stick. I haven't the strength.\n",
      "\n",
      "และเป็นห้องเดียวที่มองลงไปเห็นทะเล\n",
      "It's the most beautiful room in the house, the only one that looks down across the lawns to the sea.\n",
      "\n",
      "พวกเขาทำไม่พวกเขา?\n",
      "He was left all through the war. They do, don't they?\n",
      "\n",
      "ผมจะส่งคนขี่ม้าไปตรวจ\n",
      "Okay, I'll send down a team of horses to check out the ground.\n",
      "\n",
      "เราจะเปลี่ยนโรงพยาบาลบ้า เป็นบ่อนพนันสำหรับคนบ้า\n",
      "This is the bill that will convert the State Hospital for the Insane into the William J. Le Petomane Memorial Gambling Casino for the Insane.\n",
      "\n",
      "count: 735\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.021\n",
      "การสวนสนามที่ยิ่งใหญ่ของกองทัพ\n",
      "Before half a million spectators the greatest ever display of arms marches by in review.\n",
      "\n",
      "เร็วเข้าทุกคน!\n",
      "Ship ashore. Come on! Come on, everybody.\n",
      "\n",
      "ดังนั้นนี่คือ บีทเทิล ที่มีชื่อเสียง? ดังนั้นนี่คือที่มีชื่อเสียงสกอตแลนด์ ยาร์ดใช่มั้ย?\n",
      "So this is the famous Beatles?\n",
      "\n",
      "มาสิ หนุ่ม เราจะได้รับน้ำ บางส่วนเมื่อ\n",
      "Come on, lad.\n",
      "\n",
      "ดูลี! ถึงฉัน!\n",
      "I'd never get it past the accountants.\n",
      "\n",
      "count: 727\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.026000000000000002\n",
      "ทำไมเอ่อเอ่อมันบอกว่าที่นี่เขา เอ่อ\n",
      "Where is he?\n",
      "\n",
      "ไคยีลี เราเปิดหัวใจของเราไป ไคยีลี,\n",
      "(All) Kaili!\n",
      "\n",
      "ในทางเล็ก ๆ ของตัวเองคุณจะรู้ว่า\n",
      "You know...\n",
      "\n",
      "อย่านั่งลง\n",
      "Move yourselves. Do sit down.\n",
      "\n",
      "แบรดเราเคยสะพานสะพาน\n",
      "\"Brad\" he called. \"Brad, we've gotten a bridge, a bridge.\"\n",
      "\n",
      "count: 1304\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.031\n",
      "ร้านของผม\n",
      "There goes the barbershop!\n",
      "\n",
      "นักวิทยาศาสตร์ของพวกเขามีการ ติดตั้งอย่างถูกต้องใช่มั้ย\n",
      "If I had a Luger...\n",
      "\n",
      "ที่จะทำหน้าที่สิบตรี บูทา\n",
      "Take this hastily scribbled note hastily to acting Lance Corporal Bhuta.\n",
      "\n",
      "บ้านเป็นที่ชื่นชอบของฉันคิดว่า คุณจะไม่ต้องการให้เรา\n",
      "Home is favourite.\n",
      "\n",
      "ดู ริงโก้\n",
      "...in time! Look at Ringo.\n",
      "\n",
      "count: 939\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.036000000000000004\n",
      "สาธิตเลย\n",
      "Demonstrate, professor.\n",
      "\n",
      "รถถัง 200 คัน รถหุ้มเกาะ 50 คัน\n",
      "The route is well guarded. Behind us are 200 tanks, 50 armored cars and 500 machine-guns.\n",
      "\n",
      "ดูนั่นสิ\n",
      "Look now, look at that.\n",
      "\n",
      "ที่ออกมาจากทะเลไปพบมัน\n",
      "\"The hawks, \" he thought, \"that come out to sea to meet them. \"\n",
      "\n",
      "เด็กๆ, คุณหึ่ง?\n",
      "(Indistinct chatter) Boys, are you buzzing?\n",
      "\n",
      "count: 1121\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.041\n",
      "เพราะมันสร้าง จากการฆ่าคนบริสุทธิ์\n",
      "Your cause is doomed to failure because it is built on the stupid persecution of innocent people.\n",
      "\n",
      "พลาด คุณเด็กผู้ชายซน\n",
      "Missed!\n",
      "\n",
      "มีข้อสงสัยเกี่ยวกับมันไม่เรากำลัง เสี่ยง\n",
      "No. And again.\n",
      "\n",
      "โอ้ครับ\n",
      "Would you like that?\n",
      "\n",
      "แต่คุณเป็นคนใจร้อนบิต\n",
      "You're a great man, a very great man, but you are impatient.\n",
      "\n",
      "count: 1376\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.046\n",
      "ลาก่อน\n",
      "Goodbye, old boy.\n",
      "\n",
      "ริงโก้ส์ ริงโก้ส์\n",
      "Ringo!\n",
      "\n",
      "ริงโก้ส์ ริงโก้ส์\n",
      "Ringo!\n",
      "\n",
      "ริงโก้ส์ ริงโก้ส์\n",
      "Ringo!\n",
      "\n",
      "ริงโก้ส์ ริงโก้ส์\n",
      "Ringo!\n",
      "\n",
      "count: 1534\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.051000000000000004\n",
      "เอ่อโอ้เอ่อ\n",
      "Uh, oh, uh, Cricket's the name.\n",
      "\n",
      "นอกกระท่อมของเขา\n",
      "The old man stood the mast outside his shack.\n",
      "\n",
      "เราไม่ได้ไปที่นั่น เราเพียงแค่ใส่มันรอบที่เรากำลังจะ ไปที่นั่น\n",
      "We're not going there.\n",
      "\n",
      "ในทางเชิงเปรียบเทียบที่ เข้มงวด\n",
      "I love you.\n",
      "\n",
      "แย่ ริงโก้ เด็กหนุ่มผู้น่าสงสาร\n",
      "Poor Ringo.\n",
      "\n",
      "count: 1016\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.056\n",
      "ครั้งต่อไปที่มี 6,000 คนที่เริ่มต้นจลาจล หรือหกคน โดยไม่ต้องสถานทูตนี้ การตระหนักถึงมัน\n",
      "The next time there are 6,000 people that begin a riot, or six people, without this embassy being aware of it, those responsible will be on the first plane out of here... with my personal recommendation they be dropped from the Foreign Service.\n",
      "\n",
      "อย่ามองไปทางขวาหรือซ้าย พวกเขาอาจจะเอาพาเราสำหรับผู้ยืน ดูผู้บริสุทธิ์\n",
      "Don't look right or left.\n",
      "\n",
      "ประตูกับดักนี้\n",
      "What?\n",
      "\n",
      "ไอ้ลูกหมา!\n",
      "! HiJo de puta que te pario!\n",
      "\n",
      "การพูดตามที่มันเป็น\n",
      "In a strict metaphorical way of speaking, as it were.\n",
      "\n",
      "count: 2058\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.061\n",
      "ในทะเลทรายไม่มีปลา\n",
      "They were as old as erosions in a fishless desert.\n",
      "\n",
      "ฉันถามคุณ\n",
      "Has a degree in woodwork.\n",
      "\n",
      "คุณบอกว่าสิ่งนั้นคืออะไร?\n",
      "Find out.\n",
      "\n",
      "กรุณาโปรด\n",
      "I am very busy... please!\n",
      "\n",
      "ฉันทั่วไป\n",
      "I came up from the ranks.\n",
      "\n",
      "count: 1240\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.066\n",
      "แค่เธอก็รู้ว่าราคาของการจับภาพ\n",
      "I regret losing a lieutenant as able as Ahme but she was told the price of capture.\n",
      "\n",
      "แน่นอนว่าทำไมคุณไม่ได้ คิดว่าของที่คุณตำหนิ? ราชา คือของขวัญจากสวนสัตว์ เบอร์ลิน\n",
      "Why didn't you think of that?\n",
      "\n",
      "มันเป็นผิดพลาดสมบูรณ์ในใจ ของฉัน 144,177 ทหารถือปืนคาบศิลา เตาอั้งโล่เอส\n",
      "It was a complete cock-up.\n",
      "\n",
      "คุณได้เป็นอย่างดีได้รับวิธีการที่ เป็น โอซีทียู\n",
      "What's your name?\n",
      "\n",
      "กริปวีด ครับ?\n",
      "Put me down for last stag. Gripweed.\n",
      "\n",
      "count: 771\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.07100000000000001\n",
      "ฉันคอแห้งมาก\n",
      "Water. Quick, I'm going to faint.\n",
      "\n",
      "กับเสื้อผ้าที่มา\n",
      "With clothes that come from the finest shops\n",
      "\n",
      "เฮ้!\n",
      "Whoa, whoa!\n",
      "\n",
      "ไม่ๆ\n",
      "No, no, no.\n",
      "\n",
      "เพราะขนาดที่ดีของเขา\n",
      "Mine I must improvise to his because of his great size.\n",
      "\n",
      "count: 2969\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.076\n",
      "ตอนนี้ไม่ได้ใช้แล้ว มันเป็นห้องที่สวยที่สุดในบ้าน\n",
      "It's not used now.\n",
      "\n",
      "จากเวลาที่ฉันออกไป\n",
      "And bring me the papers from the time I was away.\n",
      "\n",
      "สถานที่ที่คือที่รู้จักกันดีของการ พักผ่อน\n",
      "Old army place.\n",
      "\n",
      "โอ้ฉันรู้ครับเช่นคุณเซอร์\n",
      "There goes a very gallant gentleman. Oh, I know, sir, like you, sir.\n",
      "\n",
      "เราต้องรีบลงมือ\n",
      "The time has come to act and act quickly!\n",
      "\n",
      "count: 1062\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.081\n",
      "เต็มรูปแบบของคาถา\n",
      "The old Queen's a sly one, full of witchcraft.\n",
      "\n",
      "ใช่ครับ\n",
      "Yes. Yes, they are.\n",
      "\n",
      "แล้วกินมัน เพียงแค่ได้กลิ่นปลา ซาร์ดีน\n",
      "Then eat them.\n",
      "\n",
      "จ้ำ จ้ำ\n",
      "Hurry up. Hurry up.\n",
      "\n",
      "รับหลอดดูดน้ำมันจาก\n",
      "Filthy!\n",
      "\n",
      "count: 1602\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.08600000000000001\n",
      "แขวนบนเขา!\n",
      "Get him up! Hang on to him!\n",
      "\n",
      "จับ ชูลท์ซ\n",
      "Place Schultz under arrest.\n",
      "\n",
      "เราจะออกจากความมืดมิด เข้าสู่โลกใหม่ โลกที่เกื้อหนุน\n",
      "We are coming out of the darkness into a new world, a kindlier world, where men will rise above their hate, their greed and their brutality.\n",
      "\n",
      "เขารู้สึกแข็งแรงของปลาที่ดี ผ่านสายที่มีต่อการเคลื่อนไหว อย่างต่อเนื่อง สิ่งที่เขาได้รับการแต่งตั้งและ เขาคิดว่า\n",
      "He felt the strength of the great fish moving steadily toward what he had chosen... ... and he thought, \"When once through my treachery... ... it had been necessary for him to make a choice... ... his choice had been to stay in the deep water... ... far out beyond all snares and traps and treacheries.\n",
      "\n",
      "ตรงผ่านให้กับผิว\n",
      "My skin's soaked right through to the skin.\n",
      "\n",
      "count: 1623\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.091\n",
      "มา!\n",
      "Come on!\n",
      "\n",
      "ตรวจดูปืน\n",
      "Take charge of that gun!\n",
      "\n",
      "และ\n",
      "See? And\n",
      "\n",
      "หรือไม่?\n",
      "No?\n",
      "\n",
      "อาจมี!\n",
      "Maybe there are!\n",
      "\n",
      "count: 4499\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.096\n",
      "ใช่ผมว่าคุณมีสิทธิ์นั้น\n",
      "Yes. Unfortunately, I suppose you have the right to ask that.\n",
      "\n",
      "ฉันกล้าพูดว่ามันครองโลก\n",
      "With a ring like that I could, dare I say it, rule the world.\n",
      "\n",
      "โอ้ดีเพียงการคาดเดา เราไม่ได้เป็นซูเปอร์แมนที่ทุก ท่านทราบว่า\n",
      "Oh, well, just a guess.\n",
      "\n",
      "และประกาศชัยในพระนามท่าน เทพเจ้าผู้ยิ่งยง เราคือสาวกผู้ซึ่งท่านเลือกสรรมาเกิด\n",
      "And grant, in the name of Your prophet, our great Lawgiver, that we, Your chosen servants, created and born in Your divine image, may aspire the more perfectly to that spiritual godliness and bodily beauty\n",
      "\n",
      "ผมขอเวลานิดเดียวครับ\n",
      "One little moment, please. What I have to tell him is\n",
      "\n",
      "count: 453\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.101\n",
      "นี่คือจุดจบ\n",
      "I'm through! This is the end.\n",
      "\n",
      "รู้มั้ยค่ะฉันหวังว่าจะมีสิ่งประดิษฐ์.\n",
      "You know, I...\n",
      "\n",
      "ยา ยา ปาทานอาศัยอยู่ในอินเดีย\n",
      "Ya... Ya...\n",
      "\n",
      "ภรรยาของฉันเป็นหนุ่มสาว ถ้าคุณได้มีการพูดอะไร\n",
      "My wife is young.\n",
      "\n",
      "เพื่อให้คุณสามารถ ฉันจะไม่ว่า\n",
      "So you can.\n",
      "\n",
      "count: 1325\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.106\n",
      "นัดเขามา\n",
      "Ask him to come here.\n",
      "\n",
      "นี่เป็นอีกคนหนึ่ง! เพียงพอ สำหรับสัปดาห์ที่ผ่านมา!\n",
      "Here's another one!\n",
      "\n",
      "โอ้ฉันรู้ว่าคุณเป็น ดีและพยายามให้ออกจากทาง ของฉัน\n",
      "Oh, I know you are.\n",
      "\n",
      "ทำให้เขาอยู่ที่ไหนสักแห่งเขา อาจตำรวจขนาดใหญ่หนึ่ง\n",
      "Working his ticket.\n",
      "\n",
      "แทลี โฮ!\n",
      "Good luck to you all.\n",
      "\n",
      "count: 1500\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.111\n",
      "count: 2352\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.116\n",
      "จะ คะ ชมดู\n",
      "Be ca... Watch... Watch...\n",
      "\n",
      "ไอ้พวกหมูเน่า\n",
      "Pigs!\n",
      "\n",
      "มานี่\n",
      "Come in here.\n",
      "\n",
      "ก็ใช่\n",
      "Most amusing.\n",
      "\n",
      "คุมกล้องซิ\n",
      "See about the photography.\n",
      "\n",
      "count: 3136\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.121\n",
      "count: 5410\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.126\n",
      "ในป่าบาง?\n",
      "In the woods somewhere?\n",
      "\n",
      "ตะกร้าม้าแพะบิลลี่\n",
      "A pony cart and a billy goat Hi diddle dee dum\n",
      "\n",
      "ถึงคุณไม่ได้ชนแจกันล้ม ผมก็กะจะชวนคุณอยู่พอดี\n",
      "I wasn't being polite. I should have asked you to have lunch with me even if you hadn't upset the vase so clumsily.\n",
      "\n",
      "เธอไม่ได้สวมแหวนเสียสละ\n",
      "The ring.\n",
      "\n",
      "ลองจับเขา\n",
      "Quick! Let's catch him!\n",
      "\n",
      "count: 1694\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.131\n",
      "count: 3320\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.136\n",
      "เย็นและยากและน่ารัก\n",
      "Then there is the tuna cold and hard and lovely.\n",
      "\n",
      "โอ้ผับที่น่ารัก สองเบียร์หลืองและมะนาว\n",
      "Oh, lovely pub.\n",
      "\n",
      "ที่ลำคอกระทุ้ง!\n",
      "Shout it, Clapper. At the throat, jab!\n",
      "\n",
      "พึงระวัง... อย่าได้ปล่อยให้มนุษย์ มีความสามารถทัดเทียมวานร\n",
      "But our great Lawgiver tells us that never, never will the human have the ape's divine faculty for being able to distinguish between evil and good.\n",
      "\n",
      "อย่ามานอนอาบแดด\n",
      "Don't just lay there getting a suntan.\n",
      "\n",
      "count: 1350\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.14100000000000001\n",
      "count: 1780\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.146\n",
      "นอนตาย\n",
      "Sleeping Death.\n",
      "\n",
      "ลงมา\n",
      "Come down!\n",
      "\n",
      "ดีเกินไป\n",
      "Too good to be true.\n",
      "\n",
      "แด่ เฮอลิง\n",
      "To Field Marshal Herring!\n",
      "\n",
      "ดี\n",
      "Good.\n",
      "\n",
      "count: 9334\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.151\n",
      "count: 10068\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.156\n",
      "โอ๊ย, ด่าโอ๊ย\n",
      "She's pulling at it. (Yells) Oh!\n",
      "\n",
      "มันไม่ได้ลดลง\n",
      "It's sort of a long counterfoil.\n",
      "\n",
      "มันคือปลั๊กที่คุณเห็น สิ่งที่สำคัญคือ\n",
      "It's the plugs.\n",
      "\n",
      "คุณไม่ได้ใช้นิ้วมือว่าในอดีตที่ผ่านมา\n",
      "I'll raise you.\n",
      "\n",
      "พวกเขาจะไม่คว้านไส้พุงเราที่คุณ เห็น นั่นคือพูดเรื่องไม่มีสาระทั้งหมด คว้านไส้พุงออก\n",
      "They won't disembowel us, you see.\n",
      "\n",
      "count: 1811\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.161\n",
      "count: 3258\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.166\n",
      "มันเป็นทีของท่าน เราจะสังหาร พวกยิวอย่างลับๆ\n",
      "It's your destiny. We'll kill off the Jews, wipe out the brunettes, then will come forth a pure Aryan race.\n",
      "\n",
      "แน่นอนว่าผมไม่ทราบ\n",
      "Well, of course I can't think of any reason.\n",
      "\n",
      "เกิดอะไรขึ้นกับมีดหรือไม่ เขาอ้างว่ามันตกลงไปในหลุมในกระเป๋าของเขา\n",
      "What happened to the knife?\n",
      "\n",
      "คุณอาจขวา\n",
      "You're probably right.\n",
      "\n",
      "มาสิ และกิน คุณไม่สามารถตก ปลาและไม่กิน\n",
      "Come on and eat.\n",
      "\n",
      "count: 2029\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.171\n",
      "count: 5250\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.17600000000000002\n",
      "ไม่รู้ สิ่งที่เขารู้คือ ร้านตัดผมของเขา\n",
      "No. His one interest seems to be in his barbershop, which he believes he left a few weeks ago.\n",
      "\n",
      "คุณไม่ได้รักหล่อน\n",
      "You didn't love her. You didn't love her.\n",
      "\n",
      "พวกเขากำลังได้รับสิทธิ มันเป็นระบบ แต่ ..\n",
      "They're entitled.\n",
      "\n",
      "เมื่อคุณไปไกลเกินไปออก\n",
      "No, you violated your luck when you went too far out.\n",
      "\n",
      "มากเธอออกโดยคนจากพรุที่ มาก\n",
      "She's getting stuffed the jacksy off her by the man from the Pru.\n",
      "\n",
      "count: 831\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.181\n",
      "count: 5050\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.186\n",
      "ปิดวิทยุ\n",
      "Turn off the radio.\n",
      "\n",
      "ไปบอก ผบ\n",
      "Tell the Commander.\n",
      "\n",
      "มัน ปิโนคีโอ!\n",
      "Home again! It's me, Pinocchio!\n",
      "\n",
      "เฮย สิ่งที่เป็น เฮย ปล่อยให้ไป!\n",
      "Hey, what the\n",
      "\n",
      "อาฟิกาโร\n",
      "Figaro! Ah, Figaro.\n",
      "\n",
      "count: 2870\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.191\n",
      "count: 4624\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.196\n",
      "เคยไปเกาะความสุข?\n",
      "Pinocchio. Ever been to Pleasure Island?\n",
      "\n",
      "ทำไมฉันลาก่อน จิมืนี\n",
      "And besides, it's dangerous. Why, I Bye,Jiminy.\n",
      "\n",
      "และฉันได้พบค่อนข้างน้อย\n",
      "She's sweeter than all the girls and I met quite a few\n",
      "\n",
      "ไอ้คนทรยศ ไอ้ขี้โกงเหลี่ยมจัด\n",
      "You filthy double-crossing bastard! Of all the stinking dirty tricks\n",
      "\n",
      "พวกเขามาจากโรงงาน\n",
      "They're coming from the cities and towns\n",
      "\n",
      "count: 1834\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.201\n",
      "count: 10081\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.20600000000000002\n",
      "คุณเห็น? คุณมีความรู้สึกผิด ชอบชั่วของฉันได้อย่างไร\n",
      "Are you my conscience?\n",
      "\n",
      "อย่ากังวลไปเลย แม็กซิม ไม่มีใครโทษคุณหรอก\n",
      "I was perfectly well. Don't let it worry you, Maxim. Nobody can blame you for making a mistake.\n",
      "\n",
      "มันทำให้ฉันอยากร้องไห้\n",
      "When I think of things we did it makes me wanna cry\n",
      "\n",
      "เขาเป็นเพียงเหมือนคุณอยู่ที่ เขี้ยวลากดินที่ถูกสาป ผู้ก่อเหตุ ชื่ออะไรและรูปแบบ ของคุณหรือไม่\n",
      "He's just like you, a damn troublemaker.\n",
      "\n",
      "ถ้าคุณดูที่เขาฉันจะให้คุณกับเขา\n",
      "Don't look at him, look at me. If you look at him, I'll give you to him.\n",
      "\n",
      "count: 762\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.211\n",
      "count: 2911\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.216\n",
      "ทำดีกว่าพูด ชุดกันกระสุน\n",
      "Actions speak louder than words. A bulletproof uniform.\n",
      "\n",
      "ยู ออ เอ๊ะเอ๊ะ ปี ไอ เอ\n",
      "Eh, P-I Eh\n",
      "\n",
      "เราจะไม่จำเป็นต้องใช้มัน เรา ได้รับออก!\n",
      "We won't need it.\n",
      "\n",
      "ไม่ย่อท้อในที่สุด นักวิทยาศาสตร์ที่ ใกล้ที่สุด\n",
      "(Electronic beeping)\n",
      "\n",
      "สีแดงเขาแล้วฆ่าเขา จุ๊ ๆ\n",
      "You, you, you and you. Paint him red and then kill him.\n",
      "\n",
      "count: 2487\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.221\n",
      "count: 5772\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.226\n",
      "ดื่มน้ำโปรด\n",
      "A drink of water, please.\n",
      "\n",
      "ที่อยู่ใกล้ บางทีเราควรที่จะไป นอน\n",
      "That was close. Maybe we'd better go to bed... before something else happens.\n",
      "\n",
      "เธอไม่รู้เลยสักนิดเรื่องการเป็นสุภาพสตรี\n",
      "You haven't the experience, you haven't the faintest idea what it means to be a great lady.\n",
      "\n",
      "อย่ามาแซว นี่ขนสัตว์แท้ๆ นะไอ้น้อง\n",
      "Don't be silly.\n",
      "\n",
      "เป็นหมอที่ยิ่งใหญ่ที่สุดที่มี\n",
      "The dark water of the gulf is the greatest healer that there is. \"\n",
      "\n",
      "count: 2744\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.231\n",
      "count: 9496\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.23600000000000002\n",
      "เพราะคุณสูญเสียความมั่นใจ\n",
      "I know you did not leave me because you lost confidence.\n",
      "\n",
      "ระหว่างเสียงเป่าชายทำ และระเบิดถอนหายใจของหญิง\n",
      "He could tell the difference between the noise the male made... ... and the sighing blow of the female.\n",
      "\n",
      "เขาเป็นคนขี้ขลาดเหม็น\n",
      "He's a private soldier. He's a stinking coward.\n",
      "\n",
      "เขตต้องห้ามถูกปิดนานนับศตวรรษ\n",
      "The forbidden zone has been closed for centuries, and rightly so.\n",
      "\n",
      "แล้วผ่านแคนยอนนั่น ทางเรียบดี\n",
      "Then maybe down that canyon. I think it's pretty level off there.\n",
      "\n",
      "count: 869\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.241\n",
      "count: 3136\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.246\n",
      "หัวใจเพียงหนึ่งเดียว\n",
      "One heart\n",
      "\n",
      "แกจะไปไหน\n",
      "Where are you going?\n",
      "\n",
      "ไม่กัดวัน\n",
      "Not a bite for days.\n",
      "\n",
      "บางทีผมอาจจะเคย บางทีผมอาจจะบ้า\n",
      "Perhaps I was.\n",
      "\n",
      "ไม่มีหรอก\n",
      "Oh, no, there isn't.\n",
      "\n",
      "count: 3916\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.251\n",
      "count: 9937\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.256\n",
      "เขาจะต้องลำบากแน่ๆ เวลาเดิน\n",
      "He will have the embarrassment of walking the entire floor.\n",
      "\n",
      "สทรอมโบลี นักแสดงโทที่ฉัน และโดยการอนุญาตเป็นพิเศษ ในการบริหารจัดการ ที่ฉันด้วย\n",
      "Stromboli the Master Showman that's a-me and by special permission of the management that's a-me too is presenting to you something... you will absolutely refuse to believe.\n",
      "\n",
      "ยังเลยค่ะ ฉันกลัวว่าจะมีอะไรเกิดขึ้นกับเขา\n",
      "No, he hasn't been in the house at all, and I'm afraid something might have happened to him.\n",
      "\n",
      "นำมันออกมาจากใต้เตียง\n",
      "The old man brought it out from under the bed.\n",
      "\n",
      "ดโรก บันทึกสำหรับนิตยสาร กองร้อย\n",
      "Get this half-track back. Drogue, records for the regimental magazine.\n",
      "\n",
      "count: 1834\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.261\n",
      "count: 4755\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.266\n",
      "ในป่า!\n",
      "In the woods!\n",
      "\n",
      "รีบรีบ\n",
      "Hurry, hurry.\n",
      "\n",
      "ตายก็ตายครั้งเดียว\n",
      "We might as well die as live like this.\n",
      "\n",
      "บอลลูน\n",
      "The ballroom.\n",
      "\n",
      "เราทุกคนต่างอยากมีความสุข\n",
      "We want to live by each other's happiness, not misery.\n",
      "\n",
      "count: 5049\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.271\n",
      "count: 7769\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.276\n",
      "สวยมากๆ เลยค่ะ\n",
      "It's very beautiful, isn't it?\n",
      "\n",
      "ลาก่อน แม็กซิม\n",
      "Well, goodbye, Maxim, old boy.\n",
      "\n",
      "ผมเป็นคนธรรมดาสามัญไร้พิษภัย\n",
      "I'm not, you know. I'm a perfectly ordinary, harmless bloke.\n",
      "\n",
      "อย่าโง่\n",
      "Don't be silly.\n",
      "\n",
      "ความคิดคือความคิดที่ฉันเห็นมัน\n",
      "The idea is...\n",
      "\n",
      "count: 4421\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.281\n",
      "count: 9086\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.28600000000000003\n",
      "ให้ตายนอนซึมผ่าน!\n",
      "Let the Sleeping Death seep through!\n",
      "\n",
      "ทั้งบ๊องๆ\n",
      "Both absent-minded.\n",
      "\n",
      "ทำอะไรโง่ๆ ขึ้นไปบนหลังคา\n",
      "Don't be a fool, you'll be murdered. Get on the roof.\n",
      "\n",
      "ใช่หัวหน้า ฉันจะจ่ายเงินให้คุณ อย่างดี\n",
      "I'll pay you well.\n",
      "\n",
      "ขอบคุณค่ะ\n",
      "Oh, yes, thank you.\n",
      "\n",
      "count: 4403\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.291\n",
      "count: 7702\n",
      "\n",
      "----------------\n",
      "\n",
      "threashold 0.296\n",
      "ใช่ค่ะ เราเข้ากันได้ดีเหลือเกิน\n",
      "Yes, it was rather, because, you see, we got on so well together.\n",
      "\n",
      "ผมคงใส่มันผิดอีกแล้วน่ะ\n",
      "I've threaded it up wrong as usual or something.\n",
      "\n",
      "มีอะไรคะ ฉันทําอะไรลงไป\n",
      "What is it?\n",
      "\n",
      "ผมคงตบหล่อน\n",
      "I must have struck her.\n",
      "\n",
      "ส่วนที่เหลือสำหรับนาที แต่แล้วคุณจะต้องไปในและ\n",
      "You rest for a minute.\n",
      "\n",
      "count: 3310\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'counter_0.006': 27,\n",
       "         'counter_0.011': 290,\n",
       "         'counter_0.016': 735,\n",
       "         'counter_0.021': 727,\n",
       "         'counter_0.026': 1304,\n",
       "         'counter_0.031': 939,\n",
       "         'counter_0.036': 1121,\n",
       "         'counter_0.041': 1376,\n",
       "         'counter_0.046': 1534,\n",
       "         'counter_0.051': 1016,\n",
       "         'counter_0.056': 2058,\n",
       "         'counter_0.061': 1240,\n",
       "         'counter_0.066': 771,\n",
       "         'counter_0.071': 2969,\n",
       "         'counter_0.076': 1062,\n",
       "         'counter_0.081': 1602,\n",
       "         'counter_0.086': 1623,\n",
       "         'counter_0.091': 4499,\n",
       "         'counter_0.096': 453,\n",
       "         'counter_0.1': 1325,\n",
       "         'counter_0.11': 2352,\n",
       "         'counter_0.12': 5410,\n",
       "         'counter_0.13': 3320,\n",
       "         'counter_0.14': 1780,\n",
       "         'counter_0.15': 10068,\n",
       "         'counter_0.16': 3258,\n",
       "         'counter_0.17': 5250,\n",
       "         'counter_0.18': 5050,\n",
       "         'counter_0.19': 4624,\n",
       "         'counter_0.2': 10081,\n",
       "         'counter_0.21': 2911,\n",
       "         'counter_0.22': 5772,\n",
       "         'counter_0.23': 9496,\n",
       "         'counter_0.24': 3136,\n",
       "         'counter_0.25': 9937,\n",
       "         'counter_0.26': 4755,\n",
       "         'counter_0.27': 7769,\n",
       "         'counter_0.28': 9086,\n",
       "         'counter_0.29': 7702,\n",
       "         'counter_0.3': 3310})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter()\n",
    "prev_threashold = 0.001\n",
    "for threashold in np.arange(0.001, 0.3, 0.005):\n",
    "    print('threashold', threashold)\n",
    "    for i, sent in enumerate(cleaned_and_filtered_th):\n",
    "        diff = abs(len(cleaned_and_filtered_th[i]) - len(cleaned_and_filtered_en[i]))\n",
    "        diff_ratio = 1 - (diff / ((len(cleaned_and_filtered_th[i]) + len(cleaned_and_filtered_en[i])) / 2) )\n",
    "        \n",
    "        if  diff_ratio > prev_threashold and diff_ratio <= threashold:\n",
    "            c['counter_{:.2}'.format(threashold)] += 1\n",
    "            \n",
    "            if c['counter_{:.2}'.format(threashold)] <= 5:\n",
    "             \n",
    "                print(sent)\n",
    "                print(cleaned_and_filtered_en[i])\n",
    "                print()\n",
    "    print('count:', c['counter_{:.2}'.format(threashold)])\n",
    "    prev_threashold = threashold\n",
    "    print('\\n----------------\\n')\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/opensubtitle_v2018/sent.cleaned.v1.th', 'w') as f:\n",
    "    for sent in cleaned_and_filtered_th:\n",
    "        f.write(sent + '\\n')\n",
    "        \n",
    "with open('../data/opensubtitle_v2018/sent.cleaned.v1.en', 'w') as f:\n",
    "    for sent in cleaned_and_filtered_en:\n",
    "        f.write(sent + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Text Segmentation (word, subword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for newmm tokenization\n",
    "\n",
    "def tokenize_worker(sentence, lang, trie):\n",
    "    \n",
    "    _tokenizer_newmm = partial(pythainlp.tokenize.word_tokenize, engine='newmm',\n",
    "                               keep_whitespace=False,\n",
    "                              custom_dict=(trie if trie != None else DEFAULT_DICT_TRIE))\n",
    "    return ' '.join(_tokenizer_newmm(sentence))\n",
    "  \n",
    "def tokenize_handler(sentences, lang, trie=None):\n",
    "    toks = []\n",
    "    p = Pool(6)\n",
    "    t = time()\n",
    "    _tokenize_worker = partial(tokenize_worker, lang=lang, trie=trie)\n",
    "    toks = p.map(_tokenize_worker, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    print('{} s'.format(time() -t))\n",
    "\n",
    "    return toks\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_spaced_tokens_to_file(data, folder_name, filename):\n",
    "    with open('/root/mt-opus/data/{}/{}'.format(folder_name, filename),'w') as f:\n",
    "        for item in data:\n",
    "            f.write(item + '\\n')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = {\n",
    "    'th': {\n",
    "        'sentencepiece': [],\n",
    "        'newmm':[]\n",
    "    },\n",
    "    'en': {\n",
    "        'sentencepiece': [],\n",
    "        'newmm':[]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Segment text into words with a dictionary-based tokenizer (`newmm`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.81156301498413 s\n",
      "39.77449178695679 s\n"
     ]
    }
   ],
   "source": [
    "# Thai\n",
    "toks['th']['newmm'] = tokenize_handler(filtered_th, lang='th')\n",
    "# English\n",
    "toks['en']['newmm'] = tokenize_handler(filtered_en, lang='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ทาส ใน กระจก วิเศษ , มาจาก พื้นที่ ที่ ไกล ที่สุด',\n",
       "  'ผ่าน ลม และ ความมืด ฉัน เรียก เจ้า',\n",
       "  'พูด !',\n",
       "  'ให้ ฉัน เห็น พระพักตร์ ของ พระองค์',\n",
       "  'สิ่ง ที่ เจ้า จะ รู้ ว่า สมเด็จ พระราชินี ของ ฉัน ได้ อย่างไร',\n",
       "  'กระจก วิเศษ บน ผนัง ผู้ ที่ เป็น สังขาร หนึ่ง ทั้งหมด หรือไม่',\n",
       "  'ที่ มีชื่อเสียง เป็น ความงาม ของ เจ้า พระ บาท สมเด็จ พระเจ้าอยู่หัว',\n",
       "  'แต่ ถือเป็น แม่บ้าน ที่ น่ารัก ที่ ฉัน เห็น',\n",
       "  'ยาจก ไม่ สามารถ ซ่อน พระคุณ อ่อนโยน ของ เธอ',\n",
       "  'อนิจจา เธอ มี ความเป็นธรรม มากขึ้น กว่า เจ้า'],\n",
       " ['Slave in the Magic Mirror , come from the farthest space .',\n",
       "  'Through wind and darkness , I summon thee .',\n",
       "  'Speak !',\n",
       "  'Let me see thy face .',\n",
       "  'What wouldst thou know , my Queen ?',\n",
       "  'Magic Mirror on the wall , who is the fairest one of all ?',\n",
       "  'Famed is thy beauty , Majesty .',\n",
       "  'But hold , a lovely maid I see .',\n",
       "  'Rags cannot hide her gentle grace .',\n",
       "  'Alas , she is more fair than thee .'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks['th']['newmm'][0:10], toks['en']['newmm'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Segment text into subword (ie. BPE tokens) with Pretrained SentencePiece (`BPEmb`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bpe(sentences, lang, n_vocab=25000):\n",
    "    \"\"\"Return a list of bpe tokens give a list of sentences\"\"\"\n",
    "    segmented_sentences = []\n",
    "    for sentence in tqdm_notebook(sentences, total=len(sentences)):\n",
    "#         print(sentence)\n",
    "        bpe_tokens = bpemb_pretrained[lang]['{}'.format(n_vocab)].encode(sentence)\n",
    "        segmented_sentences.append(' '.join(bpe_tokens))\n",
    "        \n",
    "    return segmented_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Thai language__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce4eb5e8cbe469886307235a01f1dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3202751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['▁ท าส ใน กระจก วิเศษ , ▁มาจาก พื้นที่ ที่ ไกล ที่สุด', '▁ผ่าน ลม และความ มืด ฉัน เรียก เจ้า', '▁พูด !', '▁ให้ ฉัน เห็น พระพักตร์ ของ ▁พระองค์', '▁สิ่งที่ เจ้า จะ รู้ว่า สมเด็จพระราชินี ▁ของ ฉัน ได้อย่างไร', '▁กระจ ก วิเศษ บน ผนัง ▁ผู้ ที่เป็น สัง ขาร หนึ่ง ทั้งหมด ▁หรือไม่', '▁ที่มีชื่อเสียง เป็น ความงาม ของ ▁เจ้า พระบาทสมเด็จพระ เจ้าอยู่หัว', '▁แต่ ถือเป็น แม่ บ้าน ที่น ่ารัก ที่ ฉัน ▁เห็น', '▁ยา จก ไม่สามารถ ซ่อน พระคุณ ▁อ่อน โยน ของเธอ', '▁อน ิจ จา เธอ มีความเป็น ธรรม ▁มาก ขึ้น กว่า เจ้า']\n"
     ]
    }
   ],
   "source": [
    "toks['th']['sentencepiece'] = encode_bpe(filtered_th, 'th', 25000)\n",
    "\n",
    "print(toks['th']['sentencepiece'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__English language__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0ac2a9e16c4fe0814a82c713b07a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3202751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['▁slave ▁in ▁the ▁magic ▁mirror , ▁come ▁from ▁the ▁fart hest ▁space .', '▁through ▁wind ▁and ▁darkness , ▁i ▁summon ▁the e .', '▁speak !', '▁let ▁me ▁see ▁thy ▁face .', '▁what ▁would st ▁thou ▁know , ▁my ▁queen ?', '▁magic ▁mirror ▁on ▁the ▁wall , ▁who ▁is ▁the ▁fa ire st ▁one ▁of ▁all ?', '▁famed ▁is ▁thy ▁beauty , ▁majesty .', '▁but ▁hold , ▁a ▁lov ely ▁maid ▁i ▁see .', '▁ra gs ▁cannot ▁hide ▁her ▁gentle ▁grace .', '▁al as , ▁she ▁is ▁more ▁fair ▁than ▁the e .']\n"
     ]
    }
   ],
   "source": [
    "toks['en']['sentencepiece']  = encode_bpe(filtered_en, 'en', 25000)\n",
    "print(toks['en']['sentencepiece'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) Segment text into subword (ie. BPE tokens) with SentencePiece model (unigram) trained from Opensubtitles_v2018 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: --input must not be empty\n",
      "\n",
      "sentencepiece\n",
      "\n",
      "Usage: sentencepiece [options] files\n",
      "\n",
      "   --accept_language (comma-separated list of languages this model can accept)  type: string  default: \n",
      "   --add_dummy_prefix (Add dummy whitespace at the beginning of text)  type: bool  default: true\n",
      "   --bos_id (Override BOS (<s>) id. Set -1 to disable BOS.)  type: int32  default: 1\n",
      "   --bos_piece (Override BOS (<s>) piece.)  type: string  default: <s>\n",
      "   --character_coverage (character coverage to determine the minimum symbols)  type: double  default: 0.9995\n",
      "   --control_symbols (comma separated list of control symbols)  type: string  default: \n",
      "   --eos_id (Override EOS (</s>) id. Set -1 to disable EOS.)  type: int32  default: 2\n",
      "   --eos_piece (Override EOS (</s>) piece.)  type: string  default: </s>\n",
      "   --hard_vocab_limit (If set to false, --vocab_size is considered as a soft limit.)  type: bool  default: true\n",
      "   --input (comma separated list of input sentences)  type: string  default: \n",
      "   --input_format (Input format. Supported format is `text` or `tsv`.)  type: string  default: \n",
      "   --input_sentence_size (maximum size of sentences the trainer loads)  type: int32  default: 0\n",
      "   --max_sentence_length (maximum length of sentence in byte)  type: int32  default: 4192\n",
      "   --max_sentencepiece_length (maximum length of sentence piece)  type: int32  default: 16\n",
      "   --model_prefix (output model prefix)  type: string  default: \n",
      "   --model_type (model algorithm: unigram, bpe, word or char)  type: string  default: unigram\n",
      "   --normalization_rule_name (Normalization rule name. Choose from nfkc or identity)  type: string  default: nmt_nfkc\n",
      "   --normalization_rule_tsv (Normalization rule TSV file. )  type: string  default: \n",
      "   --num_sub_iterations (number of EM sub-iterations)  type: int32  default: 2\n",
      "   --num_threads (number of threads for training)  type: int32  default: 16\n",
      "   --pad_id (Override PAD (<pad>) id. Set -1 to disable PAD.)  type: int32  default: -1\n",
      "   --pad_piece (Override PAD (<pad>) piece.)  type: string  default: <pad>\n",
      "   --remove_extra_whitespaces (Removes leading, trailing, and duplicate internal whitespace)  type: bool  default: true\n",
      "   --seed_sentencepiece_size (the size of seed sentencepieces)  type: int32  default: 1000000\n",
      "   --self_test_sample_size (the size of self test samples)  type: int32  default: 0\n",
      "   --shrinking_factor (Keeps top shrinking_factor pieces with respect to the loss)  type: double  default: 0.75\n",
      "   --shuffle_input_sentence (Randomly sample input sentences in advance. Valid when --input_sentence_size > 0)  type: bool  default: true\n",
      "   --split_by_number (split tokens by numbers (0-9))  type: bool  default: true\n",
      "   --split_by_unicode_script (use Unicode script to split sentence pieces)  type: bool  default: true\n",
      "   --split_by_whitespace (use a white space to split sentence pieces)  type: bool  default: true\n",
      "   --treat_whitespace_as_suffix (treat whitespace marker as suffix instead of prefix.)  type: bool  default: false\n",
      "   --unk_id (Override UNK (<unk>) id.)  type: int32  default: 0\n",
      "   --unk_piece (Override UNK (<unk>) piece.)  type: string  default: <unk>\n",
      "   --unk_surface (Dummy surface string for <unk>. In decoding <unk> is decoded to `unk_surface`.)  type: string  default:  ⁇ \n",
      "   --use_all_vocab (If set to true, use all tokens as vocab. Valid for word/char models.)  type: bool  default: false\n",
      "   --user_defined_symbols (comma separated list of user defined symbols)  type: string  default: \n",
      "   --vocab_size (vocabulary size)  type: int32  default: 8000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Split train-valid-test \n",
    "\n",
    "- split: train/valid/test (80/10/10)\n",
    "\n",
    "- seed for rando,.shuffle: 1234\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N =  3202751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2562200, 320275, 320276)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train-valid-test split 80/10/10\n",
    "\n",
    "n = len(toks['th']['newmm'])\n",
    "\n",
    "print('N = ',n)\n",
    "idx = list(range(n))\n",
    "\n",
    "random.seed(1234) # Set SEED\n",
    "random.shuffle(idx)\n",
    "\n",
    "train_idx, valid_idx, test_idx = idx[:int(n*0.8)], idx[int(n*0.8):int(n*0.9)], idx[int(n*0.9):]\n",
    "\n",
    "dataset_split = {}\n",
    "dataset_split['train'] = train_idx\n",
    "dataset_split['valid'] = valid_idx\n",
    "dataset_split['test'] = test_idx\n",
    "\n",
    "\n",
    "len(train_idx),len(valid_idx),len(test_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    },\n",
    "    'valid': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    },\n",
    "    'test': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    for lang in ['th', 'en']:\n",
    "        for tok_type in ['sentencepiece', 'newmm']:\n",
    "\n",
    "            dataset[split_name][lang][tok_type] = [toks[lang][tok_type][i] for i in dataset_split[split_name]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['เบค กี้ เธอ ทำท่า แปลก ๆ เมื่อกี้ ใน ห้อง', 'อยู่ กับ เธอ แอน นา จะ นำทาง คุณ ผม จะ กลับ ไป'] \n",
      "\n",
      "['Becky , um , you were acting particularly strange in there just now .', \"Stay with her so Anna can guide you . I ' m going back .\"] \n",
      "\n",
      "['▁เบ ค กี้ ▁เธอ ทํา ท่า แปลก ๆ ▁เมื่อ กี้ ▁ในห้อง', '▁ อยู่กับ เธอ ▁แอนนา จะนํา ทาง คุณ ▁ผม จะ กลับไป'] \n",
      "\n",
      "['▁bec ky , ▁um , ▁you ▁were ▁acting ▁particularly ▁strange ▁in ▁there ▁just ▁now .', \"▁stay ▁with ▁her ▁so ▁anna ▁can ▁guide ▁you . ▁i ' m ▁going ▁back .\"] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['th']['newmm'][0:2],'\\n')\n",
    "print(dataset['train']['en']['newmm'][0:2],'\\n')\n",
    "print(dataset['train']['th']['sentencepiece'][0:2],'\\n')\n",
    "print(dataset['train']['en']['sentencepiece'][0:2],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'en_train_n_toks': 92383739, 'th_train_n_toks': 86683223, 'en_valid_n_toks': 11536351, 'en_test_n_toks': 11535798, 'th_test_n_toks': 10833242, 'th_valid_n_toks': 10826042})\n"
     ]
    }
   ],
   "source": [
    "# Counting number of tokens for train, valid, test\n",
    "counter = Counter( )\n",
    "for dataset_type in ['train', 'valid', 'test']:\n",
    "    for th_sent_toks in dataset[dataset_type]['th']['newmm']:\n",
    "        counter['th_{}_n_toks'.format(dataset_type)] += len(th_sent_toks)\n",
    "    for en_sent_toks in dataset[dataset_type]['en']['newmm']:\n",
    "        counter['en_{}_n_toks'.format(dataset_type)] += len(en_sent_toks)\n",
    "\n",
    "print(counter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write segmented text to files__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-sentencepiece/th-en\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-sentencepiece/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-sentencepiece/en-th\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-sentencepiece/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-newmm/th-en\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-newmm/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-newmm/en-th\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-newmm/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-sentencepiece/th-en\n",
      "dir: ../data/opensubtitles_bin/newmm-sentencepiece/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-sentencepiece/en-th\n",
      "dir: ../data/opensubtitles_bin/newmm-sentencepiece/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-newmm/th-en\n",
      "dir: ../data/opensubtitles_bin/newmm-newmm/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-newmm/en-th\n",
      "dir: ../data/opensubtitles_bin/newmm-newmm/en-th\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tok_type_src in ['sentencepiece', 'newmm']:\n",
    "    for tok_type_tgt in ['sentencepiece', 'newmm']:\n",
    "        langs = ['th', 'en']\n",
    "        for lang in langs:\n",
    "            src_lang = lang\n",
    "            tgt_lang = 'en' if lang =='th' else 'th'\n",
    "            FOLDER_NAME = \"opensubtitles_tok/{}-{}/{}-{}\".format(tok_type_src, tok_type_tgt, src_lang, tgt_lang )\n",
    "            FOLDER_NAME_BIN = \"opensubtitles_bin/{}-{}/{}-{}\".format(tok_type_src, tok_type_tgt, src_lang, tgt_lang)\n",
    "           \n",
    "            \n",
    "            # Create directories\n",
    "            print('create directories: ')\n",
    "            print('dir: ../data/{}'.format(FOLDER_NAME))\n",
    "            print('dir: ../data/{}'.format(FOLDER_NAME_BIN))\n",
    "\n",
    "            !mkdir -p ../data/{FOLDER_NAME}\n",
    "            !mkdir -p ../data/{FOLDER_NAME_BIN}\n",
    "\n",
    "            for split_name in ['train', 'valid', 'test']:\n",
    "                \n",
    "                write_spaced_tokens_to_file(dataset[split_name][src_lang][tok_type_src],\n",
    "                                            FOLDER_NAME, '{}.{}'.format(split_name, src_lang))\n",
    "                \n",
    "                write_spaced_tokens_to_file(dataset[split_name][tgt_lang][tok_type_tgt],\n",
    "                                            FOLDER_NAME, '{}.{}'.format(split_name, tgt_lang))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁bec ky , ▁um , ▁you ▁were ▁acting ▁particularly ▁strange ▁in ▁there ▁just ▁now .\n",
      "▁stay ▁with ▁her ▁so ▁anna ▁can ▁guide ▁you . ▁i ' m ▁going ▁back .\n",
      "▁look .\n",
      "▁oh , ▁no , ▁it ' s ▁the ▁other ▁way ▁around , ▁dr . ▁lewis .\n",
      "▁sort ▁of .\n",
      "▁bart ender , ▁something ▁really ▁strong , ▁please .\n",
      "▁yes , ▁obviously .\n",
      "▁la ' s ▁so ▁nice .\n",
      "▁i ' m ▁going ▁to ▁fix ▁it .\n",
      "▁i ▁get ▁b ored .\n"
     ]
    }
   ],
   "source": [
    "!head ../data/opensubtitles_tok/newmm-sentencepiece/th-en/train.en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "เบค กี้ เธอ ทำท่า แปลก ๆ เมื่อกี้ ใน ห้อง\n",
      "อยู่ กับ เธอ แอน นา จะ นำทาง คุณ ผม จะ กลับ ไป\n",
      "ฟัง นะ\n",
      "พอดี เลย ดร. ลี วิ ส\n",
      "แบบ ว่า\n",
      "เอ่อ บาร์ เท็น เด อร ์ ขอ อะไร ที่\n",
      "ก็ ใช่ ห น่ะ สิ\n",
      "แอลเอ สวย เนอะ\n",
      "ฉัน กำลังจะ แก้ ไขมัน\n",
      "ฉัน เบื่อ ละ\n"
     ]
    }
   ],
   "source": [
    "!head ../data/opensubtitles_tok/newmm-sentencepiece/th-en/train.th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
