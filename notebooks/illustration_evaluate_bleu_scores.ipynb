{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from mt_opus.bleu_score import compute_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BPE\n",
    "\n",
    "from bpemb import BPEmb\n",
    "\n",
    "bpemb_pretrained ={\n",
    "    'th': {\n",
    "        '25000': BPEmb(lang=\"th\", vs=25000)\n",
    "    },\n",
    "    'en': {\n",
    "        '25000': BPEmb(lang=\"en\", vs=25000)\n",
    "    }\n",
    "}\n",
    "\n",
    "def encode_bpe(sentences: List[str], lang, n_vocab=25000):\n",
    "    \"\"\"Return a list of bpe tokens give a list of sentences\"\"\"\n",
    "    segmented_sentences = []\n",
    "    for sentence in sentences:\n",
    "#         print(sentence)\n",
    "        bpe_tokens = bpemb_pretrained[lang]['{}'.format(n_vocab)].encode(sentence)\n",
    "        segmented_sentences.append(' '.join(bpe_tokens))\n",
    "        \n",
    "    return segmented_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(src, tgt, src_type, tgt_type):\n",
    "    \n",
    "    if src == \"th\" and tgt == \"en\":\n",
    "        if tgt_type == \"newmm\":\n",
    "            return [\"I\", \"call\", \"to\", \"the\", \"Bullet\", \"chicken\", \"shop\", \"today\", \".\"]\n",
    "        if tgt_type == \"sentencepiece\":\n",
    "            return [\"_I\", \"_call\", \"_to\", \"_the\", \"_Bullet\", \"_chicken\", \"_shop\", \"_today.\"]\n",
    " \n",
    "    if src == \"en\" and tgt == \"th\":\n",
    "        if tgt_type == \"newmm\":\n",
    "            return [\"วันนี้\", \"ฉัน\", \"โทร\", \"ไป\", \"ที่\", \"ร้าน\", \"ไก่\", \"กระสุน\", ]       \n",
    "        if tgt_type == \"sentencepiece\":\n",
    "            return [\"_วันนี้\", \"_ฉัน\", \"โทร\", \"ไป\", \"ที่\", \"_ร้าน\", \"ไก่\", \"กระ\", \"_สุน\"]        \n",
    "        \n",
    "        \n",
    "def nmt(sentence, src, tgt, src_type, tgt_type):\n",
    "    \n",
    "    if src_type == \"newmm\":\n",
    "        src_toks = word_tokenize(sentence, keep_whitespace=False)\n",
    "    if src_type == \"sentencepiece\":\n",
    "        src_toks = encode_bpe([sentence], lang=src)\n",
    "    \n",
    "    print(\"\\t- src sentence tokenized: `{}`\\n\".format(src_toks))\n",
    "    \n",
    "    predicted_tokens = model_inference(src,tgt, src_type, tgt_type)\n",
    "    \n",
    "    print(\"\\t- predicted tokens (before retokenize): `{}`\".format(predicted_tokens))\n",
    "    if tgt_type == \"newmm\":\n",
    "        predicted_tokens = ' '.join(predicted_tokens)\n",
    "        print(\"\\t- predicted sentence (aftrer concatenation): `{}`\".format(predicted_tokens))\n",
    "\n",
    "    if tgt_type == \"sentencepiece\":\n",
    "        predicted_tokens = ''.join(predicted_tokens).replace(\"_\", \" \")\n",
    "        print(\"\\t- predicted sentence (aftrer remobe bpe): `{}`\".format(predicted_tokens))\n",
    "\n",
    "    return word_tokenize(predicted_tokens, keep_whitespace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_tokens = {\n",
    "    \"th\": [[[\"ฉัน\", \"โทร\", \"ไป\", \"ที่\", \"ร้าน\", \"ไก่\", \"กระสุน\", \"วันนี้\"]]],\n",
    "    \"en\": [[[\"Today\", \",\", \"I\", \"call\", \"to\", \"the\", \"Bullet\", \"Chicken\", \"shop\", \".\"]]],\n",
    "}\n",
    "\n",
    "refs_text = {\n",
    "    \"th\": \"ฉันโทรไปที่ร้านไก่กระสุน วันนี้\",\n",
    "    \"en\": \"Today, I call to the Bullet Chicken shop.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "th → en\n",
      "\n",
      "    - newmm → newmm\n",
      "\n",
      "\t- src sentence: `ฉันโทรไปที่ร้านไก่กระสุน วันนี้`\n",
      "\n",
      "\t- src sentence tokenized: `['ฉัน', 'โทร', 'ไป', 'ที่', 'ร้าน', 'ไก่', 'กระสุน', 'วันนี้']`\n",
      "\n",
      "\t- predicted tokens (before retokenize): `['I', 'call', 'to', 'the', 'Bullet', 'chicken', 'shop', 'today', '.']`\n",
      "\t- predicted sentence (aftrer concatenation): `I call to the Bullet chicken shop today .`\n",
      "\n",
      "\t- predicted_tokens (after retokenize): `['I', 'call', 'to', 'the', 'Bullet', 'chicken', 'shop', 'today', '.']`\n",
      "\n",
      "\t- score = (bleu, precisions, bp, ratio, translation_length, reference_length) \n",
      "\t         = (0.43443712531357925, [0.7777777777777778, 0.5, 0.42857142857142855, 0.3333333333333333], 0.8948393168143697, 0.9, 9, 10)\n",
      "\n",
      "    - newmm → sentencepiece\n",
      "\n",
      "\t- src sentence: `ฉันโทรไปที่ร้านไก่กระสุน วันนี้`\n",
      "\n",
      "\t- src sentence tokenized: `['ฉัน', 'โทร', 'ไป', 'ที่', 'ร้าน', 'ไก่', 'กระสุน', 'วันนี้']`\n",
      "\n",
      "\t- predicted tokens (before retokenize): `['_I', '_call', '_to', '_the', '_Bullet', '_chicken', '_shop', '_today.']`\n",
      "\t- predicted sentence (aftrer remobe bpe): ` I call to the Bullet chicken shop today.`\n",
      "\n",
      "\t- predicted_tokens (after retokenize): `['I', 'call', 'to', 'the', 'Bullet', 'chicken', 'shop', 'today', '.']`\n",
      "\n",
      "\t- score = (bleu, precisions, bp, ratio, translation_length, reference_length) \n",
      "\t         = (0.43443712531357925, [0.7777777777777778, 0.5, 0.42857142857142855, 0.3333333333333333], 0.8948393168143697, 0.9, 9, 10)\n",
      "\n",
      "    - sentencepiece → newmm\n",
      "\n",
      "\t- src sentence: `ฉันโทรไปที่ร้านไก่กระสุน วันนี้`\n",
      "\n",
      "\t- src sentence tokenized: `['▁ฉัน โทร ไปที่ ร้าน ไก่ กระสุน ▁วัน นี้']`\n",
      "\n",
      "\t- predicted tokens (before retokenize): `['I', 'call', 'to', 'the', 'Bullet', 'chicken', 'shop', 'today', '.']`\n",
      "\t- predicted sentence (aftrer concatenation): `I call to the Bullet chicken shop today .`\n",
      "\n",
      "\t- predicted_tokens (after retokenize): `['I', 'call', 'to', 'the', 'Bullet', 'chicken', 'shop', 'today', '.']`\n",
      "\n",
      "\t- score = (bleu, precisions, bp, ratio, translation_length, reference_length) \n",
      "\t         = (0.43443712531357925, [0.7777777777777778, 0.5, 0.42857142857142855, 0.3333333333333333], 0.8948393168143697, 0.9, 9, 10)\n",
      "\n",
      "    - sentencepiece → sentencepiece\n",
      "\n",
      "\t- src sentence: `ฉันโทรไปที่ร้านไก่กระสุน วันนี้`\n",
      "\n",
      "\t- src sentence tokenized: `['▁ฉัน โทร ไปที่ ร้าน ไก่ กระสุน ▁วัน นี้']`\n",
      "\n",
      "\t- predicted tokens (before retokenize): `['_I', '_call', '_to', '_the', '_Bullet', '_chicken', '_shop', '_today.']`\n",
      "\t- predicted sentence (aftrer remobe bpe): ` I call to the Bullet chicken shop today.`\n",
      "\n",
      "\t- predicted_tokens (after retokenize): `['I', 'call', 'to', 'the', 'Bullet', 'chicken', 'shop', 'today', '.']`\n",
      "\n",
      "\t- score = (bleu, precisions, bp, ratio, translation_length, reference_length) \n",
      "\t         = (0.43443712531357925, [0.7777777777777778, 0.5, 0.42857142857142855, 0.3333333333333333], 0.8948393168143697, 0.9, 9, 10)\n",
      "\n",
      "\n",
      "en → th\n",
      "\n",
      "    - newmm → newmm\n",
      "\n",
      "\t- src sentence: `Today, I call to the Bullet Chicken shop.`\n",
      "\n",
      "\t- src sentence tokenized: `['Today', ',', 'I', 'call', 'to', 'the', 'Bullet', 'Chicken', 'shop', '.']`\n",
      "\n",
      "\t- predicted tokens (before retokenize): `['วันนี้', 'ฉัน', 'โทร', 'ไป', 'ที่', 'ร้าน', 'ไก่', 'กระสุน']`\n",
      "\t- predicted sentence (aftrer concatenation): `วันนี้ ฉัน โทร ไป ที่ ร้าน ไก่ กระสุน`\n",
      "\n",
      "\t- predicted_tokens (after retokenize): `['วันนี้', 'ฉัน', 'โทร', 'ไป', 'ที่', 'ร้าน', 'ไก่', 'กระสุน']`\n",
      "\n",
      "\t- score = (bleu, precisions, bp, ratio, translation_length, reference_length) \n",
      "\t         = (0.8694417438899829, [1.0, 0.8571428571428571, 0.8333333333333334, 0.8], 1.0, 1.0, 8, 8)\n",
      "\n",
      "    - newmm → sentencepiece\n",
      "\n",
      "\t- src sentence: `Today, I call to the Bullet Chicken shop.`\n",
      "\n",
      "\t- src sentence tokenized: `['Today', ',', 'I', 'call', 'to', 'the', 'Bullet', 'Chicken', 'shop', '.']`\n",
      "\n",
      "\t- predicted tokens (before retokenize): `['_วันนี้', '_ฉัน', 'โทร', 'ไป', 'ที่', '_ร้าน', 'ไก่', 'กระ', '_สุน']`\n",
      "\t- predicted sentence (aftrer remobe bpe): ` วันนี้ ฉันโทรไปที่ ร้านไก่กระ สุน`\n",
      "\n",
      "\t- predicted_tokens (after retokenize): `['วันนี้', 'ฉัน', 'โทร', 'ไป', 'ที่', 'ร้าน', 'ไก่', 'กระ', 'สุ', 'น']`\n",
      "\n",
      "\t- score = (bleu, precisions, bp, ratio, translation_length, reference_length) \n",
      "\t         = (0.537284965911771, [0.7, 0.5555555555555556, 0.5, 0.42857142857142855], 1.0, 1.25, 10, 8)\n",
      "\n",
      "    - sentencepiece → newmm\n",
      "\n",
      "\t- src sentence: `Today, I call to the Bullet Chicken shop.`\n",
      "\n",
      "\t- src sentence tokenized: `['▁today , ▁i ▁call ▁to ▁the ▁bullet ▁chicken ▁shop .']`\n",
      "\n",
      "\t- predicted tokens (before retokenize): `['วันนี้', 'ฉัน', 'โทร', 'ไป', 'ที่', 'ร้าน', 'ไก่', 'กระสุน']`\n",
      "\t- predicted sentence (aftrer concatenation): `วันนี้ ฉัน โทร ไป ที่ ร้าน ไก่ กระสุน`\n",
      "\n",
      "\t- predicted_tokens (after retokenize): `['วันนี้', 'ฉัน', 'โทร', 'ไป', 'ที่', 'ร้าน', 'ไก่', 'กระสุน']`\n",
      "\n",
      "\t- score = (bleu, precisions, bp, ratio, translation_length, reference_length) \n",
      "\t         = (0.8694417438899829, [1.0, 0.8571428571428571, 0.8333333333333334, 0.8], 1.0, 1.0, 8, 8)\n",
      "\n",
      "    - sentencepiece → sentencepiece\n",
      "\n",
      "\t- src sentence: `Today, I call to the Bullet Chicken shop.`\n",
      "\n",
      "\t- src sentence tokenized: `['▁today , ▁i ▁call ▁to ▁the ▁bullet ▁chicken ▁shop .']`\n",
      "\n",
      "\t- predicted tokens (before retokenize): `['_วันนี้', '_ฉัน', 'โทร', 'ไป', 'ที่', '_ร้าน', 'ไก่', 'กระ', '_สุน']`\n",
      "\t- predicted sentence (aftrer remobe bpe): ` วันนี้ ฉันโทรไปที่ ร้านไก่กระ สุน`\n",
      "\n",
      "\t- predicted_tokens (after retokenize): `['วันนี้', 'ฉัน', 'โทร', 'ไป', 'ที่', 'ร้าน', 'ไก่', 'กระ', 'สุ', 'น']`\n",
      "\n",
      "\t- score = (bleu, precisions, bp, ratio, translation_length, reference_length) \n",
      "\t         = (0.537284965911771, [0.7, 0.5555555555555556, 0.5, 0.42857142857142855], 1.0, 1.25, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "for lang in [\"th\", \"en\"]:\n",
    "      \n",
    "    src_lang = lang\n",
    "    tgt_lang = \"en\" if src_lang == \"th\" else \"th\"\n",
    "\n",
    "    print(\"\\n\\n{} → {}\".format(src_lang, tgt_lang))\n",
    "\n",
    "    for src_type in [\"newmm\", \"sentencepiece\"]:\n",
    "        for tgt_type in [\"newmm\", \"sentencepiece\"]:\n",
    "          \n",
    "          \n",
    "            print(\"\\n    - {} → {}\\n\".format(src_type, tgt_type))\n",
    "\n",
    "            sentence = refs_text[src_lang]\n",
    "            \n",
    "            print(\"\\t- src sentence: `{}`\\n\".format(sentence))\n",
    "            predicted_tokens = nmt(sentence, src_lang, tgt_lang, src_type, tgt_type)\n",
    "            print(\"\\n\\t- predicted_tokens (after retokenize): `{}`\".format(predicted_tokens))\n",
    "            score = compute_bleu(refs_tokens[tgt_lang], [predicted_tokens])            \n",
    "            \n",
    "            \n",
    "            print(\"\\n\\t- score = (bleu, precisions, bp, ratio, translation_length, reference_length) \\n\\t         = {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
