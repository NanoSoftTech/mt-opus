{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "import random\n",
    "from time import time\n",
    "import io\n",
    "import copy \n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.ulmfit import *\n",
    "\n",
    "from subword_nmt import learn_bpe as learner\n",
    "from subword_nmt import apply_bpe as subword_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/opensubtitles/OpenSubtitles.en-th.en','r', encoding='utf-8') as f:\n",
    "    en = f.read().split('\\n')\n",
    "len(en),en[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/opensubtitles/OpenSubtitles.en-th.th','r', encoding='utf-8') as f:\n",
    "    th = f.read().split('\\n')\n",
    "len(th),th[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. subword-nmt\n",
    "\n",
    "-----\n",
    "\n",
    "- pre-tokenize the input text (both source and target) with `newmm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "# list_of_unk_toks = [b'\\xc2\\x99', b'\\xc2\\x9a', b'\\xc2\\x95',  b'\\xc2\\x97', b'\\xc2\\x98',\n",
    "#                     b'\\xc2\\x80', b'\\xc2\\x81', b'\\xc2\\x82', b'\\xc2\\x83', b'\\xc2\\x84',\n",
    "#                     b'\\xc2\\x86',\n",
    "#                     b'\\xc2\\x87', b'\\xc2\\x88', \n",
    "#                     b'\\xc2\\x89', b'\\xc2\\x8a']\n",
    "  \n",
    "list_of_unk_toks = [b'\\x98\\xc2', b'\\xae\\xc2', b'\\x99\\xc2', b'\\xb1\\xc2' , b'\\xc2\\xb7']\n",
    "for index, text in enumerate(th):\n",
    "    for token in list_of_unk_toks:\n",
    "        if token in text.encode('utf-8'):\n",
    "            print('found', token)\n",
    "            print('index [{}]: {}'.format(index, text))\n",
    "            print('English:', en[index])\n",
    "            print('---')\n",
    "            counter += 1\n",
    "            break\n",
    "            \n",
    "            \n",
    "#     break\n",
    "\n",
    "print('total number of lines that contain \"เธ...\" =', counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substring_lists = ['โช', 'รขโขยช', '', '? ?', '​', '', '', '', '",
    "', '', 'โ', '​', '']\n",
    "for item in substring_lists:\n",
    "    print(item.encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer_newmm = partial(word_tokenize, engine='newmm', keep_whitespace=False)\n",
    "def tokenize_worker(sentence):\n",
    "    \n",
    "    for substring in substring_lists:\n",
    "        sentence = sentence.replace(substring, '')\n",
    "\n",
    "    return ' '.join(_tokenizer_newmm(sentence))\n",
    "\n",
    "    \n",
    "def tokenize_handler(sentences):\n",
    "    toks = []\n",
    "    p = Pool(12)\n",
    "    t = time()\n",
    "\n",
    "    toks = p.map(tokenize_worker, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    print('{} s'.format(time() -t))\n",
    "\n",
    "    return toks\n",
    "\n",
    "\n",
    "\n",
    "def sentences_filter(sentences):\n",
    "    indices = []\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        for token in list_of_unk_toks:\n",
    "            if token in sentence.encode('utf-8'):\n",
    "                indices.append(index)\n",
    "                break\n",
    "        if len(sentence) == 1:\n",
    "            indices.append(index)\n",
    "            \n",
    "    return indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices_to_filter_out_th = sentences_filter(th)\n",
    "indices_to_filter_out_en = sentences_filter(en)\n",
    "\n",
    "print(len(indices_to_filter_out_th))\n",
    "print(len(indices_to_filter_out_en))\n",
    "\n",
    "indices_to_filter_out = indices_to_filter_out_th + indices_to_filter_out_en\n",
    "indices_to_filter_out = set(indices_to_filter_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_th = [x for i, x in enumerate(th) if i not in indices_to_filter_out]\n",
    "filtered_en = [x for i, x in enumerate(en) if i not in indices_to_filter_out]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(th), len(en))\n",
    "print(len(filtered_th), len(filtered_en))\n",
    "print('diff: {} sentences'.format((len(filtered_th) - len(th))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tok = tokenize_handler(filtered_en[:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(en_tok), en_tok[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_tok = tokenize_handler(filtered_th[:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(th_tok), th_tok[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-valid-test split 80/10/10\n",
    "\n",
    "n = len(th_tok)\n",
    "print('n=',n)\n",
    "idx = list(range(n))\n",
    "\n",
    "random.seed(1234) # Set SEED\n",
    "random.shuffle(idx)\n",
    "\n",
    "train_idx, valid_idx, test_idx = idx[:int(n*0.8)], idx[int(n*0.8):int(n*0.9)], idx[int(n*0.9):]\n",
    "\n",
    "len(train_idx),len(valid_idx),len(test_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train = [en_tok[i] for i in train_idx]\n",
    "print(len(en_train))\n",
    "\n",
    "en_valid = [en_tok[i] for i in valid_idx]\n",
    "print(len(en_valid))\n",
    "\n",
    "en_test = [en_tok[i] for i in test_idx]\n",
    "print(len(en_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_train = [th_tok[i] for i in train_idx]\n",
    "print(len(th_train))\n",
    "\n",
    "th_valid = [th_tok[i] for i in valid_idx]\n",
    "print(len(th_valid))\n",
    "\n",
    "th_test = [th_tok[i] for i in test_idx]\n",
    "print(len(th_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder name is now `opensubtitles_tok_bpe` for this sanbox_bpe.ipynb\n",
    "\n",
    "FOLDER_NAME = \"opensubtitles_tok_bpe\"\n",
    "FOLDER_NAME_BIN = \"opensubtitles_bin_bpe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_spaced_tokens_to_file(data, filename):\n",
    "    with open('./data/{}/{}'.format(FOLDER_NAME, filename),'w') as f:\n",
    "        for item in data:\n",
    "            f.write(item + '\\n')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_spaced_tokens_to_file(en_train, 'train.en')\n",
    "write_spaced_tokens_to_file(th_train, 'train.th')\n",
    "write_spaced_tokens_to_file(en_valid, 'valid.en')\n",
    "write_spaced_tokens_to_file(th_valid, 'valid.th')\n",
    "write_spaced_tokens_to_file(en_test, 'test.en')\n",
    "write_spaced_tokens_to_file(th_test, 'test.th')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train[:5], th_train[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_valid[:5], th_valid[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "for i in range(len(th_train)):\n",
    "    if b'\\xc2' in th_train[i].encode('utf-8'):\n",
    "#     if '¶' in en_train[i]:\n",
    "        print('en:', en_train[i])\n",
    "        print('th:', th_train[i])\n",
    "        print('th:', th_train[i].encode('utf-8'))\n",
    "        print('------------')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn BPE from train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 bpe-to-bpe (en -> th) (separated vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE_TOKENS = 25000\n",
    "\n",
    "# BPE_CODE = './data/{}/code'.format(FOLDER_NAME)\n",
    "BPE_CODE_JOINT = './data/{}/code.joint'.format(FOLDER_NAME)\n",
    "\n",
    "TRAIN_EN = './data/{}/train.en'.format(FOLDER_NAME)\n",
    "TRAIN_TH = './data/{}/train.th'.format(FOLDER_NAME)\n",
    "VOCAB_PREFIX= './data/{}/vocab'.format(FOLDER_NAME)\n",
    "\n",
    "\n",
    "print(\"learn_bpe.py on ${TRAIN}...\")\n",
    "print(BPE_TOKENS, BPE_CODE, TRAIN_EN, TRAIN_TH, VOCAB_PREFIX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run subword-nmt learn-bpe for Joint TH-EN\n",
    "\n",
    "!subword-nmt learn-joint-bpe-and-vocab --input {TRAIN_TH} {TRAIN_EN} -s {BPE_TOKENS} -o {BPE_CODE_JOINT} --verbose --write-vocabulary {VOCAB_PREFIX}.th {VOCAB_PREFIX}.en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in ['en', 'th']:\n",
    "    for file_prefix in ['train', 'valid', 'test']:\n",
    "        # <train/valid/test>.<en/th>\n",
    "        file_name = './data/{}/{}.{}'.format(FOLDER_NAME, file_prefix, lang)\n",
    "        \n",
    "        # <train/valid/test>.bpe.<en/th>\n",
    "        file_name_bpe = './data/{}/{}.bpe.{}'.format(FOLDER_NAME, file_prefix, lang)\n",
    "       \n",
    "        print('apply BPE to', file_name)\n",
    "\n",
    "        !subword-nmt apply-bpe -c {BPE_CODE_JOINT} --vocabulary {VOCAB_PREFIX}.{lang} < {file_name} > {file_name_bpe}\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-preprocess --source-lang en --target-lang th \\\n",
    "    --trainpref data/$FOLDER_NAME/train.bpe \\\n",
    "    --validpref data/$FOLDER_NAME/valid.bpe \\\n",
    "    --testpref data/$FOLDER_NAME/test.bpe \\\n",
    "    --destdir data/$FOLDER_NAME_BIN \\\n",
    "    --bpe subword_nmt \\\n",
    "    --joined-dictionary \\\n",
    "    --workers 12\n",
    "    \n",
    "# shared dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namespace(alignfile=None, bpe='subword_nmt', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data/opensubtitles_bin_bpe', fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=8, seed=1, source_lang='en', srcdict=None, target_lang='th', task='translation', tbmf_wrapper=False, tensorboard_logdir='', testpref='data/opensubtitles_tok_bpe/test.bpe', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, trainpref='data/opensubtitles_tok_bpe/train.bpe', user_dir=None, validpref='data/opensubtitles_tok_bpe/valid.bpe', workers=12)\n",
    "# | [en] Dictionary: 25311 types\n",
    "# | [en] data/opensubtitles_tok_bpe/train.bpe.en: 2608023 sents, 27457327 tokens, 0.0% replaced by <unk>\n",
    "# | [en] Dictionary: 25311 types\n",
    "# | [en] data/opensubtitles_tok_bpe/valid.bpe.en: 326003 sents, 3437093 tokens, 0.000378% replaced by <unk>\n",
    "# | [en] Dictionary: 25311 types\n",
    "# | [en] data/opensubtitles_tok_bpe/test.bpe.en: 326003 sents, 3433729 tokens, 0.000146% replaced by <unk>\n",
    "# | [th] Dictionary: 25311 types\n",
    "# | [th] data/opensubtitles_tok_bpe/train.bpe.th: 2608023 sents, 22482753 tokens, 0.0% replaced by <unk>\n",
    "# | [th] Dictionary: 25311 types\n",
    "# | [th] data/opensubtitles_tok_bpe/valid.bpe.th: 326003 sents, 2811450 tokens, 0.0% replaced by <unk>\n",
    "# | [th] Dictionary: 25311 types\n",
    "# | [th] data/opensubtitles_tok_bpe/test.bpe.th: 326003 sents, 2812530 tokens, 0.0% replaced by <unk>\n",
    "# | Wrote preprocessed data to data/opensubtitles_bin_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-train \\\n",
    "    data/opensubtitles_bin_bpe \\\n",
    "    --arch transformer_iwslt_de_en --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 2048 \\\n",
    "    --bpe subword_nmt \\\n",
    "    --save-dir data/opensubtitles_model/transformers_bpe \\\n",
    "    --tensorboard-logdir data/opensubtitles_model/transformers_bpe/tensorboard_log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairseq-generate data/opensubtitles_bin_bpe \\\n",
    "#     --path data/opensubtitles_model/transformers_bpe/checkpoint_best.pt \\\n",
    "#     --beam 5 --remove-bpe\n",
    "\n",
    "\n",
    "# for transformer-word2word-en2th\n",
    "fairseq-generate data/opensubtitles_bin \\\n",
    "    --path checkpoints/checkpoint_best.pt \\\n",
    "    --beam 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
