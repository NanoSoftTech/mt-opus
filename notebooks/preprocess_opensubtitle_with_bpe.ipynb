{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import copy \n",
    "import re\n",
    "import html\n",
    "\n",
    "from time import time \n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import pythainlp\n",
    "from pythainlp.util import *\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.ulmfit import *\n",
    "\n",
    "# subword-nmt\n",
    "from subword_nmt import learn_bpe as learner\n",
    "from subword_nmt import apply_bpe as subword_tokenizer\n",
    "\n",
    "import fairseq \n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pythainlp.tokenize import DEFAULT_DICT_TRIE\n",
    "\n",
    "from pythainlp.corpus import thai_words\n",
    "\n",
    "print(pythainlp.__version__)\n",
    "assert pythainlp.__version__ == '2.1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# เขียนใหม่ เอาแบบ initializer รับ wordlist มา\n",
    "class Trie:\n",
    "  class Node(object):\n",
    "    __slots__ = 'end', 'children'\n",
    "    def __init__(self):\n",
    "      self.end = False \n",
    "      self.children = {}\n",
    "      \n",
    "  def __init__(self, words):\n",
    "    self.words = words\n",
    "    self.root = Trie.Node()\n",
    "    for word in words:\n",
    "      cur = self.root\n",
    "      for ch in word: \n",
    "        node = cur.children.get(ch)\n",
    "        if not node: \n",
    "          node = Trie.Node() \n",
    "          cur.children[ch] = node \n",
    "        cur = node\n",
    "      cur.end = True \n",
    "  def prefixes(self, text):\n",
    "    res = []\n",
    "    cur = self.root\n",
    "    for i, ch in enumerate(text):\n",
    "      node = cur.children.get(ch)\n",
    "      if not node: break\n",
    "      if node.end:\n",
    "        res.append(text[:i+1])\n",
    "      cur = node\n",
    "    return res\n",
    "    \n",
    "  def __contains__(self, key):\n",
    "    return key in self.words\n",
    "  def __iter__(self):\n",
    "    yield from self.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323 ms ± 802 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pt = Trie(thai_words())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กา', 'กาก', 'กากี']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.prefixes('กากี่')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 19.0.3, however version 19.2.3 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# install BPEmb (BPE embeddings)\n",
    "\n",
    "!pip install --q bpemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "\n",
    "bpemb_pretrained ={\n",
    "    'th': {\n",
    "        '25000': BPEmb(lang=\"th\", vs=25000)\n",
    "    },\n",
    "    'en': {\n",
    "        '25000': BPEmb(lang=\"en\", vs=25000)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3281534,\n",
       " ['Slave in the Magic Mirror, come from the farthest space.',\n",
       "  'Through wind and darkness, I summon thee.',\n",
       "  'Speak!'])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/opensubtitles/OpenSubtitles.en-th.en','r', encoding='utf-8') as f:\n",
    "    en = f.read().split('\\n')\n",
    "len(en),en[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3281534,\n",
       " ['ทาสในกระจกวิเศษ, มาจากพื้นที่ที่ไกลที่สุด',\n",
       "  'ผ่านลมและความมืดฉันเรียกเจ้า',\n",
       "  'พูด!'])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/opensubtitles/OpenSubtitles.en-th.th','r', encoding='utf-8') as f:\n",
    "    th = f.read().split('\\n')\n",
    "    \n",
    "len(th),th[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text\n",
    "\n",
    "LIST_OF_UNKNOWN_TOKENS = [b'\\x98\\xc2', b'\\xae\\xc2', b'\\x99\\xc2', b'\\xb1\\xc2' , b'\\xc2\\xb7', b'\\xc3\\x83']\n",
    "LISY_OF_TOKENS_TO_REPLACE = ['', '', '​', '', '', '', '",
    "', '', 'โ', '​',\n",
    "                   '♪', '{\\ cHFFFFFF }', '§', 'font color = \"# 808080 \"']\n",
    "def unescape_string(text):\n",
    "    return html.unescape(text)\n",
    "\n",
    "def sentences_filter(sentences, lang):\n",
    "    indices = []\n",
    "    for index, sentence in tqdm_notebook(enumerate(sentences), total=len(sentences)):\n",
    "        for token in LIST_OF_UNKNOWN_TOKENS:\n",
    "            if token in sentence.encode('utf-8'):\n",
    "                indices.append(index)\n",
    "                break\n",
    "        if len(sentence) <= 1:\n",
    "            indices.append(index)\n",
    "            continue\n",
    "        if lang == 'th' and countthai(sentence, ignore_chars='') == 0.0:\n",
    "            indices.append(index)\n",
    "            continue\n",
    "    return indices\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    for token in LISY_OF_TOKENS_TO_REPLACE:\n",
    "        sentence = sentence.replace(token, '')\n",
    "        sentence = unescape_string(sentence)\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_worker(sentence, lang, trie):\n",
    "    \n",
    "    _tokenizer_newmm = partial(pythainlp.tokenize.word_tokenize, engine='newmm',\n",
    "                               keep_whitespace=False,\n",
    "                              custom_dict=(trie if trie != None else DEFAULT_DICT_TRIE))\n",
    "    return ' '.join(_tokenizer_newmm(sentence))\n",
    "  \n",
    "def tokenize_handler(sentences, lang, trie=None):\n",
    "    toks = []\n",
    "    p = Pool(12)\n",
    "    t = time()\n",
    "    _tokenize_worker = partial(tokenize_worker, lang=lang, trie=trie)\n",
    "    toks = p.map(_tokenize_worker, sentences)\n",
    "    \n",
    "    p.close()\n",
    "    p.join() # call Pool.join() to wait for the worker processes to terminate.\n",
    "\n",
    "    print('{} s'.format(time() -t))\n",
    "\n",
    "    return toks\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_spaced_tokens_to_file(data, folder_name, filename):\n",
    "    with open('/root/mt-opus/data/{}/{}'.format(folder_name, filename),'w') as f:\n",
    "        for item in data:\n",
    "            f.write(item + '\\n')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a71371634346f08d65cb6cacf94b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3281534), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425025bfbc3e46e193943c9ff6f05568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3281534), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "76544\n",
      "4392\n",
      "111.4701018333435 seconds\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "indices_to_filter_out_th = sentences_filter(th, lang='th')\n",
    "indices_to_filter_out_en = sentences_filter(en, lang='en')\n",
    "\n",
    "print(len(indices_to_filter_out_th))\n",
    "print(len(indices_to_filter_out_en))\n",
    "\n",
    "indices_to_filter_out = indices_to_filter_out_th + indices_to_filter_out_en\n",
    "indices_to_filter_out = set(indices_to_filter_out)\n",
    "\n",
    "\n",
    "filtered_th = [clean_sentence(x) for i, x in enumerate(th) if i not in indices_to_filter_out]\n",
    "filtered_en = [clean_sentence(x) for i, x in enumerate(en) if i not in indices_to_filter_out]\n",
    "\n",
    "print('{} seconds'.format(time() -t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ทาสในกระจกวิเศษ, มาจากพื้นที่ที่ไกลที่สุด'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_th[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = {\n",
    "    'th': {\n",
    "        'sentencepiece': [],\n",
    "        'newmm':[]\n",
    "    },\n",
    "    'en': {\n",
    "        'sentencepiece': [],\n",
    "        'newmm':[]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a Segment texts into tokens with `newmm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toks['th']['newmm'] = tokenize_handler(filtered_th[:10000], lang='th')\n",
    "\n",
    "# toks['en']['newmm'] = tokenize_handler(filtered_en[:10000], lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test with Pyhon Trie\n",
    "# t = time()\n",
    "# for sent in filtered_th[:100000]:\n",
    "#      toks = pythainlp.tokenize.word_tokenize(text=sent,\n",
    "#                                              engine='newmm',\n",
    "#                                              keep_whitespace=False,\n",
    "#                                              custom_dict=pt)\n",
    "        \n",
    "# print('{} s'.format(time() -t))\n",
    "# # toks['th']['newmm'] = tokenize_handler(filtered_th[:10000], lang='th', trie=pt)\n",
    "\n",
    "# # toks['en']['newmm'] = tokenize_handler(filtered_en[:10000], lang='en', trie=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test with Marisa Trie\n",
    "# t = time()\n",
    "# for sent in filtered_th[:100000]:\n",
    "#      toks = pythainlp.tokenize.word_tokenize(text=sent,\n",
    "#                                              engine='newmm',\n",
    "#                                              keep_whitespace=False)\n",
    "# print('{} s'.format(time() -t))\n",
    "# # toks['th']['newmm'] = tokenize_handler(filtered_th[:10000], lang='th', trie=pt)\n",
    "\n",
    "# # toks['en']['newmm'] = tokenize_handler(filtered_en[:10000], lang='en', trie=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.81156301498413 s\n",
      "39.77449178695679 s\n"
     ]
    }
   ],
   "source": [
    "toks['th']['newmm'] = tokenize_handler(filtered_th, lang='th')\n",
    "toks['en']['newmm'] = tokenize_handler(filtered_en, lang='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ทาส ใน กระจก วิเศษ , มาจาก พื้นที่ ที่ ไกล ที่สุด',\n",
       "  'ผ่าน ลม และ ความมืด ฉัน เรียก เจ้า',\n",
       "  'พูด !',\n",
       "  'ให้ ฉัน เห็น พระพักตร์ ของ พระองค์',\n",
       "  'สิ่ง ที่ เจ้า จะ รู้ ว่า สมเด็จ พระราชินี ของ ฉัน ได้ อย่างไร',\n",
       "  'กระจก วิเศษ บน ผนัง ผู้ ที่ เป็น สังขาร หนึ่ง ทั้งหมด หรือไม่',\n",
       "  'ที่ มีชื่อเสียง เป็น ความงาม ของ เจ้า พระ บาท สมเด็จ พระเจ้าอยู่หัว',\n",
       "  'แต่ ถือเป็น แม่บ้าน ที่ น่ารัก ที่ ฉัน เห็น',\n",
       "  'ยาจก ไม่ สามารถ ซ่อน พระคุณ อ่อนโยน ของ เธอ',\n",
       "  'อนิจจา เธอ มี ความเป็นธรรม มากขึ้น กว่า เจ้า'],\n",
       " ['Slave in the Magic Mirror , come from the farthest space .',\n",
       "  'Through wind and darkness , I summon thee .',\n",
       "  'Speak !',\n",
       "  'Let me see thy face .',\n",
       "  'What wouldst thou know , my Queen ?',\n",
       "  'Magic Mirror on the wall , who is the fairest one of all ?',\n",
       "  'Famed is thy beauty , Majesty .',\n",
       "  'But hold , a lovely maid I see .',\n",
       "  'Rags cannot hide her gentle grace .',\n",
       "  'Alas , she is more fair than thee .'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks['th']['newmm'][0:10], toks['en']['newmm'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b Segment texts into BPE tokens with SentencePiece (BPEmb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bpe(sentences, lang, n_vocab=25000):\n",
    "    \"\"\"Return a list of bpe tokens give a list of sentences\"\"\"\n",
    "    segmented_sentences = []\n",
    "    for sentence in tqdm_notebook(sentences, total=len(sentences)):\n",
    "#         print(sentence)\n",
    "        bpe_tokens = bpemb_pretrained[lang]['{}'.format(n_vocab)].encode(sentence)\n",
    "        segmented_sentences.append(' '.join(bpe_tokens))\n",
    "        \n",
    "    return segmented_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Thai language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce4eb5e8cbe469886307235a01f1dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3202751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['▁ท าส ใน กระจก วิเศษ , ▁มาจาก พื้นที่ ที่ ไกล ที่สุด', '▁ผ่าน ลม และความ มืด ฉัน เรียก เจ้า', '▁พูด !', '▁ให้ ฉัน เห็น พระพักตร์ ของ ▁พระองค์', '▁สิ่งที่ เจ้า จะ รู้ว่า สมเด็จพระราชินี ▁ของ ฉัน ได้อย่างไร', '▁กระจ ก วิเศษ บน ผนัง ▁ผู้ ที่เป็น สัง ขาร หนึ่ง ทั้งหมด ▁หรือไม่', '▁ที่มีชื่อเสียง เป็น ความงาม ของ ▁เจ้า พระบาทสมเด็จพระ เจ้าอยู่หัว', '▁แต่ ถือเป็น แม่ บ้าน ที่น ่ารัก ที่ ฉัน ▁เห็น', '▁ยา จก ไม่สามารถ ซ่อน พระคุณ ▁อ่อน โยน ของเธอ', '▁อน ิจ จา เธอ มีความเป็น ธรรม ▁มาก ขึ้น กว่า เจ้า']\n"
     ]
    }
   ],
   "source": [
    "toks['th']['sentencepiece'] = encode_bpe(filtered_th, 'th', 25000)\n",
    "\n",
    "print(toks['th']['sentencepiece'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 English language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0ac2a9e16c4fe0814a82c713b07a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3202751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['▁slave ▁in ▁the ▁magic ▁mirror , ▁come ▁from ▁the ▁fart hest ▁space .', '▁through ▁wind ▁and ▁darkness , ▁i ▁summon ▁the e .', '▁speak !', '▁let ▁me ▁see ▁thy ▁face .', '▁what ▁would st ▁thou ▁know , ▁my ▁queen ?', '▁magic ▁mirror ▁on ▁the ▁wall , ▁who ▁is ▁the ▁fa ire st ▁one ▁of ▁all ?', '▁famed ▁is ▁thy ▁beauty , ▁majesty .', '▁but ▁hold , ▁a ▁lov ely ▁maid ▁i ▁see .', '▁ra gs ▁cannot ▁hide ▁her ▁gentle ▁grace .', '▁al as , ▁she ▁is ▁more ▁fair ▁than ▁the e .']\n"
     ]
    }
   ],
   "source": [
    "toks['en']['sentencepiece']  = encode_bpe(filtered_en, 'en', 25000)\n",
    "print(toks['en']['sentencepiece'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split train-valid-test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N =  3202751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2562200, 320275, 320276)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train-valid-test split 80/10/10\n",
    "\n",
    "n = len(toks['th']['newmm'])\n",
    "\n",
    "print('N = ',n)\n",
    "idx = list(range(n))\n",
    "\n",
    "random.seed(1234) # Set SEED\n",
    "random.shuffle(idx)\n",
    "\n",
    "train_idx, valid_idx, test_idx = idx[:int(n*0.8)], idx[int(n*0.8):int(n*0.9)], idx[int(n*0.9):]\n",
    "\n",
    "dataset_split = {}\n",
    "dataset_split['train'] = train_idx\n",
    "dataset_split['valid'] = valid_idx\n",
    "dataset_split['test'] = test_idx\n",
    "\n",
    "\n",
    "len(train_idx),len(valid_idx),len(test_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    },\n",
    "    'valid': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    },\n",
    "    'test': {\n",
    "        'en': {\n",
    "            'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        },\n",
    "        'th': {\n",
    "             'sentencepiece': [],\n",
    "            'newmm':[]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    for lang in ['th', 'en']:\n",
    "        for tok_type in ['sentencepiece', 'newmm']:\n",
    "\n",
    "            dataset[split_name][lang][tok_type] = [toks[lang][tok_type][i] for i in dataset_split[split_name]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['เบค กี้ เธอ ทำท่า แปลก ๆ เมื่อกี้ ใน ห้อง', 'อยู่ กับ เธอ แอน นา จะ นำทาง คุณ ผม จะ กลับ ไป'] \n",
      "\n",
      "['Becky , um , you were acting particularly strange in there just now .', \"Stay with her so Anna can guide you . I ' m going back .\"] \n",
      "\n",
      "['▁เบ ค กี้ ▁เธอ ทํา ท่า แปลก ๆ ▁เมื่อ กี้ ▁ในห้อง', '▁ อยู่กับ เธอ ▁แอนนา จะนํา ทาง คุณ ▁ผม จะ กลับไป'] \n",
      "\n",
      "['▁bec ky , ▁um , ▁you ▁were ▁acting ▁particularly ▁strange ▁in ▁there ▁just ▁now .', \"▁stay ▁with ▁her ▁so ▁anna ▁can ▁guide ▁you . ▁i ' m ▁going ▁back .\"] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['th']['newmm'][0:2],'\\n')\n",
    "print(dataset['train']['en']['newmm'][0:2],'\\n')\n",
    "print(dataset['train']['th']['sentencepiece'][0:2],'\\n')\n",
    "print(dataset['train']['en']['sentencepiece'][0:2],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'en_train_n_toks': 92383739, 'th_train_n_toks': 86683223, 'en_valid_n_toks': 11536351, 'en_test_n_toks': 11535798, 'th_test_n_toks': 10833242, 'th_valid_n_toks': 10826042})\n"
     ]
    }
   ],
   "source": [
    "# Counting number of tokens for train, valid, test\n",
    "counter = Counter( )\n",
    "for dataset_type in ['train', 'valid', 'test']:\n",
    "    for th_sent_toks in dataset[dataset_type]['th']['newmm']:\n",
    "        counter['th_{}_n_toks'.format(dataset_type)] += len(th_sent_toks)\n",
    "    for en_sent_toks in dataset[dataset_type]['en']['newmm']:\n",
    "        counter['en_{}_n_toks'.format(dataset_type)] += len(en_sent_toks)\n",
    "\n",
    "print(counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-sentencepiece/th-en\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-sentencepiece/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-sentencepiece/en-th\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-sentencepiece/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-newmm/th-en\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-newmm/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/sentencepiece-newmm/en-th\n",
      "dir: ../data/opensubtitles_bin/sentencepiece-newmm/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-sentencepiece/th-en\n",
      "dir: ../data/opensubtitles_bin/newmm-sentencepiece/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-sentencepiece/en-th\n",
      "dir: ../data/opensubtitles_bin/newmm-sentencepiece/en-th\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-newmm/th-en\n",
      "dir: ../data/opensubtitles_bin/newmm-newmm/th-en\n",
      "create directories: \n",
      "dir: ../data/opensubtitles_tok/newmm-newmm/en-th\n",
      "dir: ../data/opensubtitles_bin/newmm-newmm/en-th\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tok_type_src in ['sentencepiece', 'newmm']:\n",
    "    for tok_type_tgt in ['sentencepiece', 'newmm']:\n",
    "        langs = ['th', 'en']\n",
    "        for lang in langs:\n",
    "            src_lang = lang\n",
    "            tgt_lang = 'en' if lang =='th' else 'th'\n",
    "            FOLDER_NAME = \"opensubtitles_tok/{}-{}/{}-{}\".format(tok_type_src, tok_type_tgt, src_lang, tgt_lang )\n",
    "            FOLDER_NAME_BIN = \"opensubtitles_bin/{}-{}/{}-{}\".format(tok_type_src, tok_type_tgt, src_lang, tgt_lang)\n",
    "           \n",
    "            \n",
    "            # Create directories\n",
    "            print('create directories: ')\n",
    "            print('dir: ../data/{}'.format(FOLDER_NAME))\n",
    "            print('dir: ../data/{}'.format(FOLDER_NAME_BIN))\n",
    "\n",
    "            !mkdir -p ../data/{FOLDER_NAME}\n",
    "            !mkdir -p ../data/{FOLDER_NAME_BIN}\n",
    "\n",
    "            for split_name in ['train', 'valid', 'test']:\n",
    "                \n",
    "                write_spaced_tokens_to_file(dataset[split_name][src_lang][tok_type_src],\n",
    "                                            FOLDER_NAME, '{}.{}'.format(split_name, src_lang))\n",
    "                \n",
    "                write_spaced_tokens_to_file(dataset[split_name][tgt_lang][tok_type_tgt],\n",
    "                                            FOLDER_NAME, '{}.{}'.format(split_name, tgt_lang))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁bec ky , ▁um , ▁you ▁were ▁acting ▁particularly ▁strange ▁in ▁there ▁just ▁now .\r\n",
      "▁stay ▁with ▁her ▁so ▁anna ▁can ▁guide ▁you . ▁i ' m ▁going ▁back .\r\n",
      "▁look .\r\n",
      "▁oh , ▁no , ▁it ' s ▁the ▁other ▁way ▁around , ▁dr . ▁lewis .\r\n",
      "▁sort ▁of .\r\n",
      "▁bart ender , ▁something ▁really ▁strong , ▁please .\r\n",
      "▁yes , ▁obviously .\r\n",
      "▁la ' s ▁so ▁nice .\r\n",
      "▁i ' m ▁going ▁to ▁fix ▁it .\r\n",
      "▁i ▁get ▁b ored .\r\n"
     ]
    }
   ],
   "source": [
    "!head ../data/opensubtitles_tok/newmm-sentencepiece/th-en/train.en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "เบค กี้ เธอ ทำท่า แปลก ๆ เมื่อกี้ ใน ห้อง\r\n",
      "อยู่ กับ เธอ แอน นา จะ นำทาง คุณ ผม จะ กลับ ไป\r\n",
      "ฟัง นะ\r\n",
      "พอดี เลย ดร. ลี วิ ส\r\n",
      "แบบ ว่า\r\n",
      "เอ่อ บาร์ เท็น เด อร ์ ขอ อะไร ที่\r\n",
      "ก็ ใช่ ห น่ะ สิ\r\n",
      "แอลเอ สวย เนอะ\r\n",
      "ฉัน กำลังจะ แก้ ไขมัน\r\n",
      "ฉัน เบื่อ ละ\r\n"
     ]
    }
   ],
   "source": [
    "!head ../data/opensubtitles_tok/newmm-sentencepiece/th-en/train.th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
